% Chapter 9, Section 5

\section{Core CNN Algorithms \difficultyInline{intermediate}}
\label{sec:cnn-algorithms}

We introduce algorithms progressively, starting from basic cross-correlation to residual learning.

\subsection{Cross-Correlation and Convolution}
Given input $I\in\mathbb{R}^{H\times W\times C_{\text{in}}}$ and kernel $K\in\mathbb{R}^{k\times k\times C_{\text{in}}\times C_{\text{out}}}$, the output feature map $S\in\mathbb{R}^{H'\times W'\times C_{\text{out}}}$ under stride $s$ and padding $p$ is computed by cross-correlation:
\begin{equation}
S(i,j,c_{\text{out}}) = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \sum_{c_{\text{in}}=0}^{C_{\text{in}}-1} I(i+m, j+n, c_{\text{in}}) \cdot K(m, n, c_{\text{in}}, c_{\text{out}}) + b_{c_{\text{out}}}
\end{equation}
where $b_{c_{\text{out}}}$ is the bias term for output channel $c_{\text{out}}$. Libraries often refer to this as "convolution" \cite{GoodfellowEtAl2016}.

\subsection{Backpropagation Through Convolution}
Backpropagation through convolution follows the chain rule, where gradients flow backward through the cross-correlation operation to update both input and kernel parameters. For loss $\mathcal{L}$ and pre-activation output $S = I * K$, the gradients are computed as $\frac{\partial \mathcal{L}}{\partial K} = I \star \frac{\partial \mathcal{L}}{\partial S}$ and $\frac{\partial \mathcal{L}}{\partial I} = \frac{\partial \mathcal{L}}{\partial S} * K^\text{rot}$, where $\star$ denotes cross-correlation, $*$ denotes convolution, and $K^\text{rot}$ is the kernel rotated by $180^{\circ}$. The mathematical derivation shows that gradient computation mirrors the forward pass but with rotated kernels, enabling efficient parameter updates through cross-correlation operations.\cite{GoodfellowEtAl2016}

\begin{examplebox}{Backpropagation Through Convolution - Shape-aware Example}
For $I\in\mathbb{R}^{32\times32\times 64}$ and $K\in\mathbb{R}^{3\times3\times64\times128}$ with stride 1 and same padding, $S\in\mathbb{R}^{32\times32\times128}$. The gradient w.r.t. $K$ accumulates over spatial locations and batch.
\end{examplebox}

\subsection{Pooling Backpropagation}
Pooling is a downsampling operation that reduces spatial dimensions by aggregating local regions, typically using max or average operations to preserve important features while reducing computational complexity.

Pooling backpropagation requires careful handling of gradient routing to maintain the mathematical properties of the forward pass. For max pooling, the upstream gradient is routed exclusively to the maximal input in each pooling region, since only the maximum value contributed to the output. For average pooling, the gradient is evenly divided among all elements in each region, reflecting the equal contribution of each input to the average. This gradient routing ensures that the backward pass accurately reflects the forward computation, enabling proper parameter updates during training.

\subsection{Residual Connections}
A residual connection is a skip connection that adds the input directly to the output of a layer, allowing the network to learn residual functions rather than direct mappings, which helps with gradient flow and enables training of very deep networks.

Residual connections introduce identity skip paths that fundamentally change gradient flow in deep networks. In a residual block with input $\vect{x}$ and residual mapping $\mathcal{F}$, the output becomes $\vect{y}=\mathcal{F}(\vect{x})+\vect{x}$, where the identity connection provides a direct path for information and gradients. The mathematical derivation of backpropagation yields $\frac{\partial \mathcal{L}}{\partial \vect{x}}=\frac{\partial \mathcal{L}}{\partial \vect{y}}\left(\frac{\partial \mathcal{F}}{\partial \vect{x}}+\mat{I}\right)$, where the identity matrix $\mat{I}$ ensures that gradients can flow directly through the skip connection, stabilizing training and enabling very deep networks.\cite{He2016}

\subsection{Progressive Complexity: Depthwise Separable Convolutions}
Depthwise separable convolution factors standard convolution into depthwise (per-channel) and pointwise ($1\times1$) operations, reducing FLOPs and parameters (used by MobileNet). This keeps representational power while improving efficiency.

\begin{remark}[Parameter Comparison]
For $C_{\text{in}}=C_{\text{out}}=c$ and kernel $k\times k$:
\begin{align}
\text{standard} &= k^2 c^2,\\
\text{depthwise separable} &= k^2 c + c^2,\quad \text{saving} \approx 1 - \frac{k^2 c + c^2}{k^2 c^2}.
\end{align}
This shows that depthwise separable convolutions achieve significant parameter reduction while maintaining representational power, making them particularly effective for mobile and edge computing applications.
\end{remark}

\subsection{Normalization and Activation}
Batch normalization and ReLU-family activations play crucial roles in modern CNN training by improving optimization dynamics and generalization performance. Batch normalization addresses covariate shift by normalizing activations across the batch dimension, smoothing the loss landscape and enabling faster convergence with higher learning rates. ReLU and its variants provide sparse activations that reduce computational complexity while maintaining gradient flow, with techniques like Leaky ReLU and ELU addressing the "dying ReLU" problem. These components work together to create more stable training dynamics, allowing networks to learn more effectively from complex visual data.\cite{Ioffe2015}

\subsection{Other Useful Variants}
Modern CNN architectures incorporate several additional techniques that address specific challenges in deep learning. Group and Layer Normalization provide alternatives to batch normalization when batch sizes are small or when batch statistics are unreliable, offering different approaches to activation normalization that can improve training stability. Dilated convolutions expand the receptive field without requiring pooling operations, making them particularly effective in segmentation tasks where spatial resolution must be preserved. Anti-aliased downsampling techniques, including blur pooling and low-pass filtering before strided operations, reduce aliasing artifacts in feature maps, leading to more robust representations that better preserve spatial information during downsampling operations.

% \subsection*{Key Takeaways}
% \begin{itemize}
%     \item Cross-correlation is the practical "convolution" in deep learning; parameter sharing and locality are core.
%     \item Residual connections preserve gradient flow, enabling very deep networks.
%     \item Efficient convolutions (depthwise separable) trade compute for accuracy with strong Pareto gains.
%     \item Normalization and careful downsampling are critical for stable optimization and preserving information.
% \end{itemize}


