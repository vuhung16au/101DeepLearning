% Chapter 2, Section 6: Eigendecomposition

\section{Eigendecomposition \difficultyInline{beginner}}
\label{sec:eigendecomposition}

Eigendecomposition is a powerful tool for understanding and analyzing linear transformations, with important applications in deep learning.

\subsection{Eigenvalues and Eigenvectors}

\begin{definition}[Eigenvector and Eigenvalue]
An \emph{eigenvector} of a square matrix $\mat{A} \in \mathbb{R}^{n \times n}$ is a non-zero vector $\vect{v}$ such that:
\begin{equation}
    \mat{A}\vect{v} = \lambda\vect{v}
\end{equation}
where $\lambda \in \mathbb{R}$ (or $\mathbb{C}$) is the corresponding \emph{eigenvalue}.
\end{definition}

The eigenvector's direction is preserved under the transformation $\mat{A}$, with only its magnitude scaled by $\lambda$.

\begin{example}
Consider $\mat{A} = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}$. We can verify that $\vect{v}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ is an eigenvector:
\begin{equation}
    \mat{A}\vect{v}_1 = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 3 \\ 0 \end{bmatrix} = 3\vect{v}_1
\end{equation}
So $\lambda_1 = 3$ is an eigenvalue.
\end{example}

\subsection{Finding Eigenvalues}

To find eigenvalues, we solve the \emph{characteristic equation}:
\begin{equation}
    \det(\mat{A} - \lambda\mat{I}) = 0
\end{equation}

This gives a polynomial of degree $n$ called the characteristic polynomial, which has $n$ roots (counting multiplicities) in $\mathbb{C}$.

\begin{example}
For $\mat{A} = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$:
\begin{equation}
    \det\begin{bmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{bmatrix} = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = 0
\end{equation}
Solving gives $\lambda_1 = 3$ and $\lambda_2 = 1$.
\end{example}

\subsection{Eigendecomposition}

If a matrix $\mat{A} \in \mathbb{R}^{n \times n}$ has $n$ linearly independent eigenvectors, it can be decomposed as:

\begin{equation}
    \mat{A} = \mat{V}\boldsymbol{\Lambda}\mat{V}^{-1}
\end{equation}

where $\mat{V}$ is the matrix whose columns are eigenvectors $\mat{V} = [\vect{v}_1, \vect{v}_2, \ldots, \vect{v}_n]$ and $\boldsymbol{\Lambda}$ is a diagonal matrix of eigenvalues $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$.

This is called the \emph{eigendecomposition} or \emph{spectral decomposition}.

\subsection{Symmetric Matrices}

Symmetric matrices have particularly nice properties.

\begin{theorem}[Spectral Theorem for Symmetric Matrices]
If $\mat{A}$ is a real symmetric matrix ($\mat{A} = \mat{A}\transpose$), then:
\begin{enumerate}
    \item All eigenvalues are real
    \item Eigenvectors corresponding to different eigenvalues are orthogonal
    \item $\mat{A}$ can be decomposed as:
    \begin{equation}
        \mat{A} = \mat{Q}\boldsymbol{\Lambda}\mat{Q}\transpose
    \end{equation}
    where $\mat{Q}$ is an orthogonal matrix ($\mat{Q}\transpose\mat{Q} = \mat{I}$) of eigenvectors.
\end{enumerate}
\end{theorem}

This decomposition is fundamental in many algorithms, including Principal Component Analysis (PCA).

\subsection{Properties of Eigenvalues}

For a matrix $\mat{A} \in \mathbb{R}^{n \times n}$, several important properties connect eigenvalues to other matrix characteristics. The trace of a matrix equals the sum of its eigenvalues $\text{trace}(\mat{A}) = \sum_{i=1}^{n} A_{ii} = \sum_{i=1}^{n} \lambda_i$, providing a direct relationship between the diagonal elements and the eigenvalues that is useful for understanding the overall behavior of the matrix. The determinant of a matrix equals the product of its eigenvalues $\det(\mat{A}) = \prod_{i=1}^{n} \lambda_i$, which connects the volume scaling factor of the linear transformation to the eigenvalues and provides insight into whether the transformation preserves or changes the orientation of the space. If $\mat{A}$ is invertible, the eigenvalues of $\mat{A}^{-1}$ are the reciprocals of the original eigenvalues $1/\lambda_i$, which means that the inverse transformation scales vectors by the inverse of the original scaling factors. The eigenvalues of $\mat{A}^k$ are the $k$-th powers of the original eigenvalues $\lambda_i^k$, which is particularly useful for understanding the long-term behavior of iterative processes and the stability of dynamical systems.

\subsection{Positive Definite Matrices}

\begin{definition}[Positive Definite]
A symmetric matrix $\mat{A}$ is \emph{positive definite} if for all non-zero $\vect{x} \in \mathbb{R}^n$:
\begin{equation}
    \vect{x}\transpose\mat{A}\vect{x} > 0
\end{equation}
Equivalently, all eigenvalues of $\mat{A}$ are positive.
\end{definition}

\begin{definition}[Positive Semi-definite]
$\mat{A}$ is \emph{positive semi-definite} if $\vect{x}\transpose\mat{A}\vect{x} \geq 0$ for all $\vect{x}$, i.e., all eigenvalues are non-negative.
\end{definition}

Positive definite matrices are crucial in optimization, as they ensure that local minima are global minima for quadratic functions.

\subsection{Applications in Deep Learning}

Eigendecomposition has several important applications in deep learning that leverage the geometric and algebraic properties of eigenvalues and eigenvectors. Principal Component Analysis (PCA) finds directions of maximum variance by computing eigenvectors of the covariance matrix, which is fundamental for dimensionality reduction and data preprocessing in many machine learning pipelines. In optimization, the Hessian matrix's eigenvalues determine the curvature of the loss surface, with positive definite Hessians indicating convexity and providing insights into the convergence properties of optimization algorithms. Spectral normalization constrains the largest eigenvalue of weight matrices to stabilize training of generative adversarial networks (GANs), preventing the discriminator from becoming too powerful and maintaining training stability. Graph Neural Networks use graph Laplacian eigendecomposition to define spectral graph convolutions, which enable the application of convolutional neural network concepts to irregular graph structures. Understanding the dynamics of recurrent neural networks involves analyzing the eigenvalues of recurrent weight matrices, as these eigenvalues affect gradient flow and stability during training, with eigenvalues greater than 1 leading to exploding gradients and eigenvalues less than 1 leading to vanishing gradients.

\subsection{Computational Considerations}

Computing eigendecomposition involves several computational considerations that are important for practical applications in deep learning. Full eigendecomposition requires $O(n^3)$ operations for dense matrices, making it computationally expensive for large matrices, which is why approximate methods are often preferred in practice. Power iteration for finding the dominant eigenvector requires $O(kn^2)$ operations for $k$ iterations, providing a more efficient way to compute the most important eigenvalue and eigenvector when only the dominant direction is needed. Iterative methods such as Lanczos algorithm are particularly useful for sparse matrices, as they can exploit the sparsity structure to reduce computational complexity and memory requirements. For large-scale deep learning applications, we often use approximations such as power iteration when only the top eigenvalues are needed, focus on top-$k$ eigenvalues and eigenvectors when the full spectrum is not required, and employ specialized algorithms for specific matrix structures such as symmetric or sparse matrices to achieve better performance and numerical stability.

Understanding eigendecomposition provides insight into the geometric properties of linear transformations and is essential for many advanced deep learning techniques.
