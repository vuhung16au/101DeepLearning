% Exercises (Hands-On Exercises) for Chapter 10: Sequence Modeling

\section*{Exercises}
\addcontentsline{toc}{section}{Exercises}

\subsection*{Easy}

\begin{problem}[RNN vs Feedforward]
Explain why standard feedforward networks are not suitable for sequence modeling tasks. What key capability do RNNs provide?

\textbf{Hint:} Consider variable-length inputs and the need to maintain temporal context.
\end{problem}

\begin{problem}[LSTM Gates]
Name the three gates in an LSTM cell and briefly describe the role of each.

\textbf{Hint:} Think about what information needs to be forgotten, what new information to store, and what to output.
\end{problem}

\begin{problem}[Vanishing Gradients]
Explain why vanilla RNNs suffer from the vanishing gradient problem when processing long sequences.

\textbf{Hint:} Consider repeated matrix multiplication during backpropagation through time.
\end{problem}

\begin{problem}[Sequence-to-Sequence Tasks]
Give three examples of sequence-to-sequence tasks and explain what makes them challenging.

\textbf{Hint:} Consider machine translation, speech recognition, and video captioning.
\end{problem}

\subsection*{Medium}

\begin{problem}[BPTT Implementation]
Describe how truncated backpropagation through time (BPTT) works. What are the trade-offs compared to full BPTT?

\textbf{Hint:} Consider memory requirements, gradient approximation quality, and the effective temporal window.
\end{problem}

\begin{problem}[Attention Mechanism]
Explain the intuition behind attention mechanisms in sequence-to-sequence models. How does attention address the bottleneck of fixed-size context vectors?

\textbf{Hint:} Consider how different parts of the input sequence should influence different parts of the output.
\end{problem}

\subsection*{Hard}

\begin{problem}[GRU vs LSTM]
Compare GRU (Gated Recurrent Unit) and LSTM architectures mathematically. Derive their update equations and analyse computational complexity.

\textbf{Hint:} Count the number of parameters and operations per cell. GRU has fewer gates.
\end{problem}

\begin{problem}[Bidirectional RNN Gradient]
Derive the gradient flow in a bidirectional RNN. Explain why bidirectional RNNs cannot be used for online prediction tasks.

\textbf{Hint:} Consider that backward pass requires seeing the entire future sequence.
\end{problem}


\begin{problem}[Advanced Topic 1]
Explain a key concept from this chapter and its practical applications.

\textbf{Hint:} Consider the theoretical foundations and real-world implications.
\end{problem}

\begin{problem}[Advanced Topic 2]
Analyse the relationship between different techniques covered in this chapter.

\textbf{Hint:} Look for connections and trade-offs between methods.
\end{problem}

\begin{problem}[Advanced Topic 3]
Design an experiment to test a hypothesis related to this chapter's content.

\textbf{Hint:} Consider experimental design, metrics, and potential confounding factors.
\end{problem}

\begin{problem}[Advanced Topic 4]
Compare different approaches to solving a problem from this chapter.

\textbf{Hint:} Consider computational complexity, accuracy, and practical considerations.
\end{problem}

\begin{problem}[Advanced Topic 5]
Derive a mathematical relationship or prove a theorem from this chapter.

\textbf{Hint:} Start with the definitions and work through the logical steps.
\end{problem}

\begin{problem}[Advanced Topic 6]
Implement a practical solution to a problem discussed in this chapter.

\textbf{Hint:} Consider the implementation details and potential challenges.
\end{problem}

\begin{problem}[Advanced Topic 7]
Evaluate the limitations and potential improvements of techniques from this chapter.

\textbf{Hint:} Consider both theoretical limitations and practical constraints.
\end{problem}
