% Chapter 7, Section 5

\section{Batch Normalization \difficultyInline{intermediate}}
\label{sec:batch-normalization}

\textbf{Batch normalization} normalizes layer inputs across the batch dimension.

\subsection{Intuition: Keeping Scales Stable}

Training can become unstable if the distribution of activations shifts as earlier layers update (internal covariate shift). Batch normalization re-centers and re-scales activations, keeping them in a predictable range so downstream layers see a more stable input distribution. This allows larger learning rates and speeds up training.

\subsection{Algorithm}

For mini-batch $\mathcal{B}$ with activations $\vect{x}$:

\begin{align}
\mu_{\mathcal{B}} &= \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} x_i \\
\sigma^2_{\mathcal{B}} &= \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} (x_i - \mu_{\mathcal{B}})^2 \\
\hat{x}_i &= \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{align}

where $\gamma$ and $\beta$ are learnable parameters.

Implementation details: maintain running averages $\mu_{\text{running}}, \sigma^2_{\text{running}}$ updated with momentum $\rho$ per iteration; apply per-feature normalization for fully connected layers and per-channel per-spatial-location statistics for CNNs \cite{Ioffe2015,GoodfellowEtAl2016}.

\subsection{Benefits}

Batch normalization stabilizes distributions by mitigating internal covariate shift and keeping activations in a stable range, enabling larger learning rates through better-conditioned optimization that allows faster training. It provides less sensitive initialization with a wider set of workable initializations, while the mini-batch noise in statistics acts as stochastic regularization that improves generalization. Batch normalization supports deeper networks by facilitating training of very deep architectures like ResNets, and improves gradient flow where normalized scales yield healthier signal-to-noise ratios in backpropagation, making it an essential component for modern deep learning architectures.

\subsection{Inference}

At test time, use running averages computed during training:
\begin{equation}
y = \gamma \frac{x - \mu_{\text{running}}}{\sqrt{\sigma^2_{\text{running}} + \epsilon}} + \beta
\end{equation}

Be careful when batch sizes are small at inference or differ from training: do not recompute batch statistics at test time; use the stored running averages. For distribution shift, consider recalibrating $\mu_{\text{running}}, \sigma^2_{\text{running}}$ with a small unlabeled buffer.

\subsection{Variants}

Layer Normalization normalizes across features for each sample and is effective in RNNs and Transformers where batch statistics are less stable. Group Normalization normalizes within groups of channels and is robust to small batch sizes common in detection and segmentation tasks. Instance Normalization normalizes each sample independently and is prominent in style transfer where contrast and style per instance varies. Batch Renormalization and Ghost BatchNorm adjust for mismatch between batch and population statistics or simulate small batches inside large ones for regularization, providing alternatives when standard batch normalization is not suitable.

\begin{example}
\textbf{Example (vision):} In a CNN with batch size 128, use per-channel BN after each convolution with momentum $\rho=0.9$ and $\epsilon=10^{-5}$. During inference, freeze $\gamma,\beta$ and use stored running statistics.
\end{example}

% Index entries
\index{batch normalization}
\index{normalization layers}
\index{internal covariate shift}

