% Chapter 7, Section 5

\section{Batch Normalization \difficultyInline{intermediate}}
\label{sec:batch-normalization}

\textbf{Batch normalization} normalizes layer inputs across the batch dimension.

\subsection{Intuition: Keeping Scales Stable}

Training can become unstable if the distribution of activations shifts as earlier layers update (internal covariate shift). Batch normalization re-centers and re-scales activations, keeping them in a predictable range so downstream layers see a more stable input distribution. This allows larger learning rates and speeds up training.

\subsection{Algorithm}

For mini-batch $\mathcal{B}$ with activations $\vect{x}$:

\begin{align}
\mu_{\mathcal{B}} &= \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} x_i \\
\sigma^2_{\mathcal{B}} &= \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} (x_i - \mu_{\mathcal{B}})^2 \\
\hat{x}_i &= \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} \\
y_i &= \gamma \hat{x}_i + \beta
\end{align}

where $\gamma$ and $\beta$ are learnable parameters.

Implementation details: maintain running averages $\mu_{\text{running}}, \sigma^2_{\text{running}}$ updated with momentum $\rho$ per iteration; apply per-feature normalization for fully connected layers and per-channel per-spatial-location statistics for CNNs \cite{Ioffe2015,GoodfellowEtAl2016}.

\subsection{Benefits}

\begin{itemize}
    \item \textbf{Stabilizes distributions:} Mitigates internal covariate shift, keeping activations in a stable range \cite{Ioffe2015}.
    \item \textbf{Enables larger learning rates:} Better-conditioned optimization allows faster training.
    \item \textbf{Less sensitive initialization:} Wider set of workable initializations \cite{GoodfellowEtAl2016}.
    \item \textbf{Regularization effect:} Mini-batch noise in statistics acts as stochastic regularization, improving generalization.
    \item \textbf{Supports deeper nets:} Facilitates training very deep architectures (e.g., ResNets) \cite{He2016}.
    \item \textbf{Improves gradient flow:} Normalized scales yield healthier signal-to-noise ratios in backprop.
\end{itemize}

\subsection{Inference}

At test time, use running averages computed during training:
\begin{equation}
y = \gamma \frac{x - \mu_{\text{running}}}{\sqrt{\sigma^2_{\text{running}} + \epsilon}} + \beta
\end{equation}

Be careful when batch sizes are small at inference or differ from training: do not recompute batch statistics at test time; use the stored running averages. For distribution shift, consider recalibrating $\mu_{\text{running}}, \sigma^2_{\text{running}}$ with a small unlabeled buffer.

\subsection{Variants}

\textbf{Layer Normalization:} Normalize across features for each sample; effective in RNNs and Transformers where batch statistics are less stable.

\textbf{Group Normalization:} Normalize within groups of channels; robust to small batch sizes common in detection/segmentation.

\textbf{Instance Normalization:} Normalize each sample independently; prominent in style transfer where contrast/style per instance varies.

\textbf{Batch Renormalization / Ghost BatchNorm:} Adjust for mismatch between batch and population statistics or simulate small batches inside large ones for regularization.

\begin{example}
\textbf{Example (vision):} In a CNN with batch size 128, use per-channel BN after each convolution with momentum $\rho=0.9$ and $\epsilon=10^{-5}$. During inference, freeze $\gamma,\beta$ and use stored running statistics.
\end{example}

% Index entries
\index{batch normalization}
\index{normalization layers}
\index{internal covariate shift}

