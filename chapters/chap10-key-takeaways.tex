% Key Takeaways for Chapter 10

\section*{Key Takeaways}
\addcontentsline{toc}{section}{Key Takeaways}

\begin{keytakeaways}
\begin{itemize}[leftmargin=2em]
    \item \textbf{RNNs process sequential data} by maintaining hidden states that capture temporal dependencies.
    \item \textbf{LSTMs and GRUs} mitigate vanishing gradients via gating mechanisms that control information flow.
    \item \textbf{Backpropagation through time} (BPTT) computes gradients by unrolling the recurrent computation graph.
    \item \textbf{Attention mechanisms} allow models to focus on relevant parts of the input sequence, improving alignment in seq2seq tasks.
    \item \textbf{Practical challenges} include gradient clipping, teacher forcing, and exposure bias in autoregressive generation.
\end{itemize}
\end{keytakeaways}


