% Problems (Hands-On Exercises) for Chapter 1: Introduction

\section*{Problems}
\addcontentsline{toc}{section}{Problems}

\subsection*{Easy}

\begin{problem}[Historical Milestones]
List three key breakthroughs that enabled the rise of deep learning in the 21st century and explain their significance.

\textbf{Hint:} Consider computational advances, data availability, and algorithmic innovations.
\end{problem}

\begin{problem}[Deep Learning vs Traditional ML]
Explain the main difference between deep learning and traditional machine learning approaches in terms of feature engineering.

\textbf{Hint:} Think about automatic feature learning versus manual feature extraction.
\end{problem}

\begin{problem}[Application Domains]
Name three real-world domains where deep learning has achieved significant success and briefly describe one application in each domain.

\textbf{Hint:} Consider computer vision, natural language processing, and speech recognition.
\end{problem}

\begin{problem}[Neural Network Basics]
What is the fundamental building block of a neural network, and how does it process information?

\textbf{Hint:} Think about the basic computational unit that receives inputs, applies weights, and produces an output.
\end{problem}

\begin{problem}[Learning Process]
Explain in simple terms what happens during the training of a neural network.

\textbf{Hint:} Consider how the network adjusts its parameters based on examples and feedback.
\end{problem}

\begin{problem}[Data Requirements]
Why do deep learning models typically require large amounts of data to perform well?

\textbf{Hint:} Think about the relationship between model complexity and the need for diverse examples.
\end{problem}

\begin{problem}[Computational Power]
What type of hardware is most commonly used to train deep learning models, and why?

\textbf{Hint:} Consider the parallel processing capabilities needed for matrix operations.
\end{problem}

\begin{problem}[Feature Learning]
How does deep learning differ from traditional programming in terms of how features are identified?

\textbf{Hint:} Compare manual feature engineering with automatic feature discovery.
\end{problem}

\begin{problem}[Model Layers]
What does the term "deep" refer to in deep learning, and why is depth important?

\textbf{Hint:} Think about the hierarchical representation of information in multiple layers.
\end{problem}

\begin{problem}[Training vs Inference]
Distinguish between the training phase and the inference phase of a deep learning model.

\textbf{Hint:} Consider when the model learns versus when it makes predictions.
\end{problem}

\begin{problem}[Success Stories]
Name one famous deep learning achievement (such as AlphaGo, ImageNet, or GPT) and explain why it was significant.

\textbf{Hint:} Consider breakthroughs that demonstrated the potential of deep learning to the general public.
\end{problem}

\begin{problem}[Problem Types]
What types of problems are best suited for deep learning approaches?

\textbf{Hint:} Think about problems involving pattern recognition, complex relationships, or high-dimensional data.
\end{problem}

\subsection*{Medium}

\begin{problem}[Enabling Factors]
Analyse how the availability of large datasets and computational resources (GPUs) together enabled the practical success of deep learning. Why wasn't one factor alone sufficient?

\textbf{Hint:} Consider the computational requirements of training deep networks and the need for diverse training examples.
\end{problem}

\begin{problem}[Challenges and Limitations]
Identify two major challenges or limitations of current deep learning approaches and propose potential research directions to address them.

\textbf{Hint:} Think about interpretability, data efficiency, generalisation, or robustness to adversarial examples.
\end{problem}

\begin{problem}[Historical Context]
Compare the "AI winter" periods with the current deep learning boom. What factors made the difference between failure and success?

\textbf{Hint:} Consider the role of computational resources, data availability, and algorithmic improvements.
\end{problem}

\begin{problem}[Architecture Evolution]
Trace the evolution from perceptrons to modern deep neural networks. What were the key architectural innovations that enabled deeper networks?

\textbf{Hint:} Think about activation functions, weight initialisation, and training algorithms.
\end{problem}

\begin{problem}[Data and Performance]
Explain the relationship between dataset size, model complexity, and performance in deep learning. Why does this relationship exist?

\textbf{Hint:} Consider the bias-variance trade-off and the curse of dimensionality.
\end{problem}

\begin{problem}[Computational Scaling]
Analyse how the computational requirements of deep learning have evolved and what this means for accessibility and democratisation of AI.

\textbf{Hint:} Consider the costs of training large models and the implications for research and industry.
\end{problem}

\begin{problem}[Generalisation Challenges]
Why do deep learning models sometimes fail to generalise well to new data, and what strategies can help improve generalisation?

\textbf{Hint:} Think about overfitting, domain shift, and regularisation techniques.
\end{problem}

\subsection*{Hard}

\begin{problem}[Theoretical Foundations]
Critically evaluate the universal approximation theorem and its implications for deep learning. What are the practical limitations of this theoretical guarantee?

\textbf{Hint:} Consider the difference between theoretical possibility and practical feasibility, including training dynamics and optimisation challenges.
\end{problem}

\begin{problem}[Emergent Properties]
Investigate how emergent properties arise in deep neural networks and their implications for AI safety and interpretability. Provide specific examples.

\textbf{Hint:} Consider phenomena like in-context learning, few-shot learning, and unexpected capabilities that emerge in large language models.
\end{problem}

\begin{problem}[Scaling Laws and Limits]
Analyse the scaling laws in deep learning and discuss the fundamental limits they might reveal about the approach. What alternatives might be necessary?

\textbf{Hint:} Consider the relationship between model size, data size, and performance, and think about potential bottlenecks or diminishing returns.
\end{problem}

\begin{problem}[Biological Inspiration]
Compare and contrast biological neural networks with artificial neural networks. What insights from neuroscience could improve deep learning architectures?

\textbf{Hint:} Consider sparsity, plasticity, energy efficiency, and the role of different types of neurons and connections in biological systems.
\end{problem}

\begin{problem}[Ethical and Societal Implications]
Evaluate the broader societal implications of deep learning advances, including issues of bias, fairness, privacy, and the concentration of AI capabilities.

\textbf{Hint:} Consider both technical solutions and policy frameworks needed to address these challenges.
\end{problem}

\begin{problem}[Future Research Directions]
Propose three novel research directions that could address current limitations of deep learning. Justify why these directions are promising and what challenges they might face.

\textbf{Hint:} Consider areas like neurosymbolic AI, quantum machine learning, or biologically-inspired architectures.
\end{problem}

\begin{problem}[Mathematical Rigour]
Critically assess the mathematical foundations of deep learning. What are the key theoretical gaps that need to be addressed for a more rigorous understanding?

\textbf{Hint:} Consider optimisation theory, generalisation bounds, and the theoretical understanding of why deep networks work so well in practice.
\end{problem}

