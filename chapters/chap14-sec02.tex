% Chapter 14, Section 2

\section{Regularized Autoencoders \difficultyInline{intermediate}}
\label{sec:regularized-ae}

Regularized autoencoders extend basic autoencoders by adding various forms of regularization to encourage learning of more useful and robust representations through constraints on the latent space or training process.

\subsection{Sparse Autoencoders}

Sparse autoencoders add a sparsity penalty on hidden activations through the loss function $L = \|\vect{x} - \hat{\vect{x}}\|^2 + \lambda \sum_j |h_j|$, where the L1 penalty encourages learning of sparse, interpretable features by forcing most hidden units to be inactive for any given input, promoting the discovery of meaningful and independent features.

\subsection{Denoising Autoencoders (DAE)}

Denoising autoencoders train to reconstruct clean input from corrupted versions by first corrupting the input $\tilde{\vect{x}} \sim q(\tilde{\vect{x}}|\vect{x})$, then encoding the corrupted input $\vect{h} = f(\tilde{\vect{x}})$, decoding and reconstructing $\hat{\vect{x}} = g(\vect{h})$, and minimizing the loss $L = \|\vect{x} - \hat{\vect{x}}\|^2$. This approach learns robust representations by forcing the model to recover the original signal from noisy inputs, where corruption types include additive Gaussian noise, masking that randomly sets inputs to zero, and salt-and-pepper noise, with the model learning to identify and remove these corruptions while preserving the essential structure of the data.

\subsection{Contractive Autoencoders (CAE)}

Contractive autoencoders add a penalty on the Jacobian of the encoder through the loss function $L = \|\vect{x} - \hat{\vect{x}}\|^2 + \lambda \left\|\frac{\partial f(\vect{x})}{\partial \vect{x}}\right\|_F^2$, where the Frobenius norm penalty encourages locally contractive mappings that are robust to small perturbations by penalizing large gradients in the encoder function.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (regularized AEs)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Noise level}, ylabel={Reconstruction error}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(0.0,0.05) (0.1,0.06) (0.2,0.08) (0.3,0.12) (0.4,0.20)};
%       \addplot[bookred,very thick,dashed] coordinates{(0.0,0.05) (0.1,0.07) (0.2,0.11) (0.3,0.20) (0.4,0.35)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{DAE (solid) vs. plain AE (dashed) under increasing input noise (illustrative).}
%   \label{fig:dae-robust}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={$\Vert \nabla f(x) \Vert_F^2$ penalty $\lambda$}, ylabel={Val. error}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(0.0,0.15) (0.1,0.12) (0.2,0.11) (0.4,0.12) (0.8,0.16)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Contractive penalty tuning vs. validation error (illustrative).}
%   \label{fig:cae-penalty}
% \end{figure}
