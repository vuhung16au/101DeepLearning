% Chapter 11, Section 2

\section{Baseline Models and Debugging \difficultyInline{intermediate}}
\label{sec:baselines-debugging}

Baseline models and systematic debugging strategies are essential for establishing performance floors, identifying implementation issues, and ensuring that complex models provide genuine improvements over simple approaches.

\subsection{Establishing Baselines}

A \textbf{baseline model} is a simple, well-understood reference system that provides a performance floor for comparison against more complex approaches. Strong baselines de-risk projects by validating data quality, metrics, and feasibility \textcite{GoodfellowEtAl2016,Prince2023}. They serve as sanity checks to ensure that sophisticated models actually improve upon simple solutions rather than introducing unnecessary complexity. Baselines help identify whether poor performance stems from model limitations or fundamental issues with data quality, preprocessing, or evaluation methodology. By establishing multiple baselines across different complexity levels, practitioners can quantify the marginal value of each architectural choice and avoid over-engineering solutions. These reference points become immutable benchmarks that prevent performance regression and provide confidence that improvements are genuine rather than artifacts of experimental variance. \index{baseline}

Start with simple baselines:
\begin{enumerate}
    \item \textbf{Random baseline:} Random predictions
    \item \textbf{Simple heuristics:} Rule-based systems
    \item \textbf{Classical ML:} Logistic regression, random forests
    \item \textbf{Simple neural networks:} Small architectures
\end{enumerate}

Compare deep learning improvements against these baselines. Use \emph{data leakage} \index{data leakage} checks (e.g., time-based splits, patient-level splits) and ensure identical preprocessing across baselines.

\subsection{Debugging Strategy}

Deep learning models often fail silently or produce unexpected results due to the complexity of neural architectures and the non-convex optimization landscape. Systematic debugging is essential because model failures can stem from multiple sources: implementation bugs, data quality issues, hyperparameter choices, or fundamental limitations of the approach. Without proper debugging methodology, practitioners may waste significant time pursuing ineffective solutions or miss critical insights about their data and model behavior.

\index{silent failures}

\textbf{Step 1: Overfit a small dataset}
\begin{itemize}
    \item Take 10-100 examples
    \item Turn off regularization
    \item If can't overfit, suspect implementation, data, or optimization bugs
\end{itemize}

\textbf{Step 2: Check intermediate outputs}
\begin{itemize}
    \item Visualize activations
    \item Check gradient magnitudes
    \item Verify loss decreases on training set
    \item Plot learning-rate vs. loss; test different seeds
\end{itemize}

\textbf{Step 3: Diagnose underfitting vs. overfitting}
\begin{itemize}
    \item \textbf{Underfitting:} Poor train performance $\to$ increase capacity
    \item \textbf{Overfitting:} Good train, poor validation $\to$ add regularization
\end{itemize}

\subsection{Common Issues}

Vanishing and exploding gradients represent fundamental challenges in deep network training that can prevent effective learning. Batch normalization stabilizes training by normalizing inputs to each layer, preventing gradients from becoming too small or large during backpropagation—for example, in a 50-layer network without batch norm, gradients might vanish to near-zero values by layer 20, but with batch norm they remain stable throughout the entire network. Gradient clipping prevents exploding gradients by capping gradient magnitudes at a threshold (e.g., 1.0 or 5.0), which is particularly important in RNNs where gradients can grow exponentially over long sequences, causing training instability and preventing convergence. Proper weight initialization (Xavier/He initialization) ensures gradients start at reasonable magnitudes rather than vanishing or exploding from the first forward pass, with He initialization for ReLU networks setting weights to $\sqrt{2/n}$ where $n$ is the input size, preventing the "dead neuron" problem where all activations become zero. Residual connections provide direct paths for gradient flow, allowing information to bypass layers where gradients might vanish, as seen in ResNet architectures where the skip connection $y = F(x) + x$ ensures that even if $F(x)$ becomes zero, the gradient can still flow through the identity connection.

Dead ReLUs occur when neurons never activate because their weights become too negative, often due to aggressive learning rates, where reducing the learning rate from 0.01 to 0.001 can prevent neurons from being "killed" during early training, allowing them to recover and contribute to learning. Unlike standard ReLU which outputs zero for negative inputs, Leaky ReLU allows small negative values (e.g., 0.01x) and ELU provides smooth negative outputs, preventing the "dying ReLU" problem where neurons become permanently inactive, as seen in networks where 30-50\% of neurons might never fire after initialization.

Loss not decreasing can stem from multiple sources that require systematic investigation. Learning rates that are too high cause the optimizer to overshoot the minimum and oscillate around it, while rates that are too low make training painfully slow—a learning rate of 0.1 might cause loss to bounce between 0.5 and 0.7, while 0.0001 might show no improvement for 100 epochs. Gradient computation bugs can cause the optimizer to move in wrong directions or not move at all, with common issues including incorrect backpropagation implementations, wrong loss function derivatives, or gradient accumulation errors that result in gradients being zero or pointing away from the minimum. Incorrect data preprocessing can make learning impossible by normalizing inputs to the wrong scale or introducing data leakage, such as normalizing images to [0,1] when the model expects [-1,1], or accidentally including future information in time series data that prevents the model from learning meaningful patterns. Misaligned labels or incorrect class indexing can cause the model to learn the wrong mappings, with a common mistake being using 1-based indexing for labels when the model expects 0-based indexing, causing the model to predict class 0 when it should predict class 1, resulting in consistently wrong predictions.

\index{batch normalization}\index{gradient clipping}

\subsection{Ablation and sanity checks}

Perform \emph{ablation studies} \index{ablation study} to quantify the contribution of each component (augmentation, architecture blocks, regularizers). Use \emph{label shuffling} to verify the pipeline cannot learn when labels are randomized. Train with \emph{frozen features} to isolate head capacity.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (debugging)}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      xlabel={Epoch}, ylabel={Loss}, grid=both, legend pos=north east]
      \addplot[bookpurple,very thick] coordinates{(0,1.0) (1,0.8) (2,0.65) (3,0.55) (4,0.50) (5,0.48)};\addlegendentry{Train}
      \addplot[bookred,very thick,dashed] coordinates{(0,1.1) (1,0.95) (2,0.90) (3,0.92) (4,1.00) (5,1.10)};\addlegendentry{Val}
    \end{axis}
  \end{tikzpicture}
  \caption{Typical overfitting: training loss decreases while validation loss bottoms out and rises.}
  \label{fig:overfit-curve}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      ymode=log,
      xlabel={Layer depth}, ylabel={$\lVert g \rVert_2$}, grid=both]
      \addplot[bookpurple,very thick] coordinates{(1,1e-1) (2,8e-2) (3,5e-2) (4,2e-2) (5,8e-3) (6,3e-3)};
    \end{axis}
  \end{tikzpicture}
  \caption{Gradient norms vanishing with depth; motivates normalization and residual connections.}
  \label{fig:vanishing-grad}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      xlabel={Learning rate}, ylabel={Final loss}, grid=both]
      \addplot[bookpurple,very thick] coordinates{(1e-5,1.2) (5e-5,0.9) (1e-4,0.7) (5e-4,0.55) (1e-3,0.54) (5e-3,1.3) (1e-2,3.0)};
    \end{axis}
  \end{tikzpicture}
  \caption{Learning-rate sweep to identify a stable training regime.}
  \label{fig:lr-sweep}
\end{figure}

\subsection{Historical notes and references}

Debugging by overfitting a tiny subset and systematic ablations has roots in classical ML practice and was emphasized in early deep learning methodology \textcite{GoodfellowEtAl2016}. Modern best practices are also surveyed in open textbooks \textcite{Prince2023,D2LChapterOptimization}.

