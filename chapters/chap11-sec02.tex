% Chapter 11, Section 2

\section{Baseline Models and Debugging \difficultyInline{intermediate}}
\label{sec:baselines-debugging}

Before diving into complex architectures, establish simple reference models to validate data quality, metrics, and the fundamental feasibility of your approach.

\subsection{Establishing Baselines}

A \textbf{baseline model} is a simple, well-understood reference system that provides a performance floor for comparison against more complex approaches. Strong baselines de-risk projects by validating data quality, metrics, and feasibility \textcite{GoodfellowEtAl2016,Prince2023}. They serve as sanity checks to ensure that sophisticated models actually improve upon simple solutions rather than introducing unnecessary complexity. Baselines help identify whether poor performance stems from model limitations or fundamental issues with data quality, preprocessing, or evaluation methodology. By establishing multiple baselines across different complexity levels, practitioners can quantify the marginal value of each architectural choice and avoid over-engineering solutions. These reference points become immutable benchmarks that prevent performance regression and provide confidence that improvements are genuine rather than artifacts of experimental variance. \index{baseline}

Starting with the simplest possible approach, a random baseline generates predictions through pure chance, establishing the absolute minimum performance threshold that any reasonable model must exceed. Moving up in sophistication, simple heuristics employ rule-based systems that encode domain knowledge without learning from data, such as predicting the most frequent class in classification tasks or using moving averages for time series. These rule-based approaches often perform surprisingly well and provide valuable insights into the problem structure. Classical machine learning methods like logistic regression and random forests represent the next level of complexity, offering interpretable models with proven track records across diverse domains. Finally, simple neural networks with just a few layers serve as a bridge between classical approaches and deep architectures, helping isolate whether performance gains truly require depth or stem from other factors like better optimization or data augmentation.

When comparing deep learning improvements against these baselines, it's crucial to maintain experimental rigour through careful attention to \emph{data leakage} \index{data leakage} prevention. This means using identical preprocessing pipelines across all baselines and ensuring that validation splits respect the problem's temporal or hierarchical structure—for example, using time-based splits for forecasting tasks or patient-level splits in medical datasets to prevent information leakage across training and validation sets.

\subsection{Debugging Strategy}

Deep learning models often fail silently or produce unexpected results due to the complexity of neural architectures and the non-convex optimization landscape. Systematic debugging is essential because model failures can stem from multiple sources: implementation bugs, data quality issues, hyperparameter choices, or fundamental limitations of the approach. Without proper debugging methodology, practitioners may waste significant time pursuing ineffective solutions or miss critical insights about their data and model behavior.

\index{silent failures}

\textbf{Step 1: Overfit a small dataset}
\begin{itemize}
    \item Take 10-100 examples
    \item Turn off regularization
    \item If can't overfit, suspect implementation, data, or optimization bugs
\end{itemize}

\textbf{Step 2: Check intermediate outputs}
\begin{itemize}
    \item Visualize activations
    \item Check gradient magnitudes
    \item Verify loss decreases on training set
    \item Plot learning-rate vs. loss; test different seeds
\end{itemize}

\textbf{Step 3: Diagnose underfitting vs. overfitting}
\begin{itemize}
    \item \textbf{Underfitting:} Poor train performance $\to$ increase capacity
    \item \textbf{Overfitting:} Good train, poor validation $\to$ add regularization
\end{itemize}

\subsection{Common Issues}

\textbf{Vanishing/exploding gradients} represent one of the most fundamental challenges in training deep networks, occurring when gradients either decay to near-zero or grow exponentially as they propagate backwards through layers. Batch normalization addresses this by stabilizing training through normalizing inputs to each layer, preventing gradients from becoming too small or large during backpropagation—for example, in a 50-layer network without batch norm, gradients might vanish to near-zero values by layer 20, but with batch norm they remain stable throughout the entire network. Gradient clipping provides another line of defence by capping gradient magnitudes at a threshold (e.g., 1.0 or 5.0), which is particularly important in RNNs where gradients can grow exponentially over long sequences, causing training instability and preventing convergence. Proper weight initialization using Xavier or He initialization ensures gradients start at reasonable magnitudes rather than vanishing or exploding from the first forward pass—for instance, He initialization for ReLU networks sets weights to $\sqrt{2/n}$ where $n$ is the input size, preventing the "dead neuron" problem where all activations become zero. Finally, residual connections provide direct paths for gradient flow, allowing information to bypass layers where gradients might vanish; in ResNet architectures, the skip connection $y = F(x) + x$ ensures that even if $F(x)$ becomes zero, the gradient can still flow through the identity connection.

\index{batch normalization}\index{gradient clipping}

\textbf{Dead ReLUs} occur when neurons never activate because their weights become too negative, often due to aggressive learning rates. Reducing the learning rate from 0.01 to 0.001 can prevent neurons from being "killed" during early training, allowing them to recover and contribute to learning. Alternatively, using Leaky ReLU or ELU instead of standard ReLU addresses the root cause directly: unlike standard ReLU which outputs zero for negative inputs, Leaky ReLU allows small negative values (e.g., 0.01x) and ELU provides smooth negative outputs, preventing the "dying ReLU" problem where neurons become permanently inactive—a phenomenon that can affect 30-50\% of neurons in poorly configured networks.

\textbf{Loss not decreasing} signals fundamental problems with training dynamics that require systematic investigation. First, check the learning rate, as rates that are too high cause the optimizer to overshoot the minimum and oscillate around it (e.g., a learning rate of 0.1 might cause loss to bounce between 0.5 and 0.7), while rates that are too low make training painfully slow (e.g., 0.0001 might show no improvement for 100 epochs). Second, verify gradient computation, as bugs in backpropagation implementations, wrong loss function derivatives, or gradient accumulation errors can cause the optimizer to move in wrong directions or not move at all. Third, check data preprocessing, as incorrect preprocessing can make learning impossible—for example, normalizing images to [0,1] when the model expects [-1,1], or accidentally including future information in time series data can prevent the model from learning meaningful patterns. Finally, confirm label alignment and class indexing, as misaligned labels or incorrect indexing (such as using 1-based indexing when the model expects 0-based) can cause the model to learn completely wrong mappings, resulting in consistently incorrect predictions.

\subsection{Ablation and sanity checks}

Perform \emph{ablation studies} \index{ablation study} to quantify the contribution of each component (augmentation, architecture blocks, regularizers). Use \emph{label shuffling} to verify the pipeline cannot learn when labels are randomized. Train with \emph{frozen features} to isolate head capacity.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (debugging)}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      xlabel={Epoch}, ylabel={Loss}, grid=both, legend pos=north east]
      \addplot[bookpurple,very thick] coordinates{(0,1.0) (1,0.8) (2,0.65) (3,0.55) (4,0.50) (5,0.48)};\addlegendentry{Train}
      \addplot[bookred,very thick,dashed] coordinates{(0,1.1) (1,0.95) (2,0.90) (3,0.92) (4,1.00) (5,1.10)};\addlegendentry{Val}
    \end{axis}
  \end{tikzpicture}
  \caption{Typical overfitting: training loss decreases while validation loss bottoms out and rises.}
  \label{fig:overfit-curve}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      ymode=log,
      xlabel={Layer depth}, ylabel={$\lVert g \rVert_2$}, grid=both]
      \addplot[bookpurple,very thick] coordinates{(1,1e-1) (2,8e-2) (3,5e-2) (4,2e-2) (5,8e-3) (6,3e-3)};
    \end{axis}
  \end{tikzpicture}
  \caption{Gradient norms vanishing with depth; motivates normalization and residual connections.}
  \label{fig:vanishing-grad}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      xlabel={Learning rate}, ylabel={Final loss}, grid=both]
      \addplot[bookpurple,very thick] coordinates{(1e-5,1.2) (5e-5,0.9) (1e-4,0.7) (5e-4,0.55) (1e-3,0.54) (5e-3,1.3) (1e-2,3.0)};
    \end{axis}
  \end{tikzpicture}
  \caption{Learning-rate sweep to identify a stable training regime.}
  \label{fig:lr-sweep}
\end{figure}

\subsection{Historical notes and references}

Debugging by overfitting a tiny subset and systematic ablations has roots in classical ML practice and was emphasized in early deep learning methodology \textcite{GoodfellowEtAl2016}. Modern best practices are also surveyed in open textbooks \textcite{Prince2023,D2LChapterOptimization}.

