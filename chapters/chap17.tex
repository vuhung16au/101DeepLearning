% Chapter 17: Monte Carlo Methods

\chapter{Monte Carlo Methods}
\label{chap:monte-carlo}

This chapter introduces sampling-based approaches for probabilistic inference and learning.


\section*{Learning Objectives}
\addcontentsline{toc}{section}{Learning Objectives}

After studying this chapter, you will be able to:

\begin{enumerate}
    \item Describe Monte Carlo estimation and variance reduction techniques.
    \item Explain MCMC algorithms (Metropolisâ€“Hastings, Gibbs) and their diagnostics.
    \item Apply sampling to approximate expectations and gradients.
    \item Identify pitfalls such as poor mixing and autocorrelation.
\end{enumerate}



\section*{Intuition}
\addcontentsline{toc}{section}{Intuition}

When exact integrals are intractable, we approximate them with random samples. The art is to sample efficiently from complicated posteriors and to estimate uncertainty from finite chains.

For example, estimating the expected value of a complex function over a high-dimensional probability distribution becomes feasible by drawing random samples and averaging their function values. Like a pollster surveying a small random sample of voters to predict election outcomes, Monte Carlo methods use random sampling to approximate complex mathematical expectations that would be impossible to compute exactly.

Think of Monte Carlo methods as a sophisticated dart-throwing game where you're trying to estimate the area of an irregular shape by throwing darts randomly at a board. The more darts you throw, the better your estimate becomes, and the key is learning to throw the darts in the most informative regions rather than just anywhere on the board.


\input{chapters/chap17-sec01}
\input{chapters/chap17-sec02}
\input{chapters/chap17-sec03}
\input{chapters/chap17-sec04}

\input{chapters/chap17-real-world-applications}

% Chapter summary and problems
\input{chapters/chap17-key-takeaways}
\input{chapters/chap17-exercises}
