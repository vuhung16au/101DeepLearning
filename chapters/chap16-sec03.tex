% Chapter 16, Section 3

\section{Deep Learning and Structured Models \difficultyInline{advanced}}
\label{sec:deep-structured}

Deep learning and structured models combine the representational power of neural networks with the structural constraints of graphical models, where neural networks learn complex feature representations while graphical models enforce structural constraints on the output space, enabling the modeling of complex, structured data with both local and global dependencies.

\subsection{Structured Output Prediction}

Structured output prediction uses graphical models to model output structure, where the goal is to predict complex, structured outputs that satisfy certain constraints and dependencies. Conditional Random Fields (CRFs) provide a powerful framework for structured prediction with $p(\vect{y}|\vect{x}) = \frac{1}{Z(\vect{x})} \exp\left(\sum_c \vect{w}^\top \vect{\phi}_c(\vect{x}, \vect{y}_c)\right)$, where the model learns to assign higher probability to valid output structures while penalizing invalid ones. Applications include sequence labeling tasks like named entity recognition and part-of-speech tagging, where the model must ensure that the predicted labels form coherent sequences, image segmentation where the model must produce spatially coherent segmentations, and parsing where the model must generate syntactically valid parse trees that respect grammatical constraints.

\subsection{Structured Prediction with Neural Networks}

Structured prediction with neural networks combines the representational power of neural networks with the structural constraints of graphical models, where the neural network extracts rich feature representations from the input data while the graphical model enforces structural constraints on the output space. The approach typically involves feature extraction using CNNs or RNNs to learn complex patterns in the input data, followed by structured inference using a CRF layer that ensures the output satisfies the required structural constraints, where the entire system is trained end-to-end using backpropagation to optimize both the feature extraction and structured prediction components simultaneously. An example is CNN-CRF for semantic segmentation, where the CNN learns to extract visual features from images while the CRF layer ensures that the predicted segmentation labels form spatially coherent regions, resulting in more accurate and visually plausible segmentations.

\subsection{Neural Module Networks}

Neural Module Networks compose neural modules based on program structure for visual reasoning, where the model learns to decompose complex visual reasoning tasks into simpler sub-tasks that can be solved by specialized neural modules. This approach enables the model to handle compositional reasoning tasks by learning to combine different modules in a structured way, where each module is responsible for a specific type of reasoning operation such as object detection, spatial relationships, or attribute recognition. The modular design allows the model to generalize to new combinations of reasoning operations and provides interpretability by showing which modules are used for each step of the reasoning process.

\subsection{Graph Neural Networks}

Graph Neural Networks (GNNs) operate on graphs via message passing and permutation-invariant aggregations, where the mathematical definition involves updating node representations through $\mathbf{h}_v^{(l+1)} = \text{UPDATE}^{(l)}(\mathbf{h}_v^{(l)}, \text{AGGREGATE}^{(l)}(\{\mathbf{h}_u^{(l)} : u \in \mathcal{N}(v)\}))$ for each node $v$ and layer $l$. We need GNNs because many real-world problems involve data with inherent graph structure, where traditional neural networks cannot directly handle the irregular and variable-sized nature of graph data. We use GNNs by first representing the data as a graph with nodes and edges, then applying message passing layers that aggregate information from neighboring nodes, where the learned node representations can be used for various downstream tasks like node classification, link prediction, or graph-level prediction.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (structured models)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}[>=stealth]
%     \tikzstyle{b}=[draw,rounded corners,align=center,minimum width=2.2cm,minimum height=0.9cm]
%     \node[b,fill=bookpurple!10] at (0,0) (cnn) {CNN features};
%     \node[b,fill=bookpurple!15] at (3.2,0) (crf) {CRF layer};
%     \node[b,fill=bookpurple!20] at (6.4,0) (mask) {Segmentation mask};
%     \draw[->] (cnn) -- (crf);
%     \draw[->] (crf) -- (mask);
%   \end{tikzpicture}
%   \caption{CNN features feeding a CRF layer for structured output.}
%   \label{fig:cnn-crf}
% \end{figure}
