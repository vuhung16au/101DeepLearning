% Chapter 16, Section 3

\section{Deep Learning and Structured Models \difficultyInline{advanced}}
\label{sec:deep-structured}

Deep learning and structured models combine the representational power of neural networks with the structural constraints of graphical models, where neural networks learn complex feature representations while graphical models enforce structural constraints on the output space, enabling the modeling of complex, structured data with both local and global dependencies.

\subsection{Structured Output Prediction}

Structured output prediction uses graphical models to model output structure, where the goal is to predict complex, structured outputs that satisfy constraints and dependencies. Conditional Random Fields (CRFs) provide a powerful framework with
\begin{equation}
p(\vect{y}\,|\,\vect{x}) = \frac{1}{Z(\vect{x})} \, \exp\Big(\sum_{c} \vect{w}^\top \vect{\phi}_c(\vect{x}, \vect{y}_c)\Big).
\end{equation}
\noindent\textbf{Variables.} $\vect{x}$: input; $\vect{y}$: structured output; $c$: cliques/factors; $\vect{\phi}_c$: feature functions on $(\vect{x},\vect{y}_c)$; $\vect{w}$: weights; $Z(\vect{x})$: partition function ensuring normalization.

\subsection{Structured Prediction with Neural Networks}

Structured prediction with neural networks combines the representational power of neural networks with the structural constraints of graphical models, where the neural network extracts rich feature representations from the input data while the graphical model enforces structural constraints on the output space. The approach typically involves feature extraction using CNNs or RNNs to learn complex patterns in the input data, followed by structured inference using a CRF layer that ensures the output satisfies the required structural constraints, where the entire system is trained end-to-end using backpropagation to optimize both the feature extraction and structured prediction components simultaneously. An example is CNN-CRF for semantic segmentation, where the CNN learns to extract visual features from images while the CRF layer ensures that the predicted segmentation labels form spatially coherent regions, resulting in more accurate and visually plausible segmentations.

\subsection{Neural Module Networks}

Neural Module Networks compose neural modules based on program structure for visual reasoning, where the model learns to decompose complex visual reasoning tasks into simpler sub-tasks that can be solved by specialized neural modules. This approach enables the model to handle compositional reasoning tasks by learning to combine different modules in a structured way, where each module is responsible for a specific type of reasoning operation such as object detection, spatial relationships, or attribute recognition. The modular design allows the model to generalize to new combinations of reasoning operations and provides interpretability by showing which modules are used for each step of the reasoning process.

\subsection{Graph Neural Networks}

Graph Neural Networks (GNNs) operate on data structured as graphs via iterative message passing. Let $\mathcal{G}=(\mathcal{V},\mathcal{E})$ with node features $\mathbf{X} \in \mathbb{R}^{|\mathcal{V}| \times D}$ and adjacency matrix $\mathbf{A}$. At layer $l$, each node $v$ aggregates neighbor information and updates its representation:
\begin{align}
\mathbf{m}_{\mathcal{N}(v)}^{(l)} &= \mathrm{AGGREGATE}^{(l)}\big(\{\mathbf{h}_u^{(l-1)}: u \in \mathcal{N}(v)\}\big), \\
\mathbf{h}_v^{(l)} &= \mathrm{UPDATE}^{(l)}\big(\mathbf{h}_v^{(l-1)}, \mathbf{m}_{\mathcal{N}(v)}^{(l)}\big),
\end{align}
where $\mathrm{AGGREGATE}$ is permutation-invariant (e.g., sum/mean/max) and $\mathrm{UPDATE}$ is learnable (e.g., linear map + nonlinearity). A common layer is the GCN:
\begin{equation}
\mathbf{H}^{(l)} = \sigma\!\big( \tilde{\mathbf{D}}^{-1/2} \tilde{\mathbf{A}} \, \tilde{\mathbf{D}}^{-1/2} \, \mathbf{H}^{(l-1)} \, \mathbf{W}^{(l)} \big),
\end{equation}
with $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}$, $\tilde{\mathbf{D}}_{ii}=\sum_j \tilde{A}_{ij}$, $\mathbf{W}^{(l)}$ learnable, and $\sigma$ an activation. After $L$ layers, $\mathbf{h}_v^{(L)}$ aggregates information from the $L$-hop neighborhood.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (structured models)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}[>=stealth]
%     \tikzstyle{b}=[draw,rounded corners,align=center,minimum width=2.2cm,minimum height=0.9cm]
%     \node[b,fill=bookpurple!10] at (0,0) (cnn) {CNN features};
%     \node[b,fill=bookpurple!15] at (3.2,0) (crf) {CRF layer};
%     \node[b,fill=bookpurple!20] at (6.4,0) (mask) {Segmentation mask};
%     \draw[->] (cnn) -- (crf);
%     \draw[->] (crf) -- (mask);
%   \end{tikzpicture}
%   \caption{CNN features feeding a CRF layer for structured output.}
%   \label{fig:cnn-crf}
% \end{figure}
