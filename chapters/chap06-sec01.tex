% Chapter 6, Section 1

\section{Introduction to Feedforward Networks \difficultyInline{intermediate}}
\label{sec:intro-feedforward}

Feedforward neural networks are the fundamental building blocks of deep learning, consisting of interconnected layers of neurons that process information sequentially from input to output without feedback loops.

\subsection{Intuition: What is a Feedforward Network?}

Imagine you're trying to recognize handwritten digits, where a feedforward neural network is like having a team of experts, each specialized in detecting different features. First layer experts look for basic patterns like edges, curves, and lines, middle layer experts combine these basic patterns to detect more complex shapes like loops, corners, and curves, and final layer experts combine these complex shapes to make the final decision like "This is a 3" or "This is a 7". The key insight is that each layer builds upon the previous one, creating increasingly sophisticated representations that mirror how our own visual system works, from detecting simple edges to recognizing complex objects. A feedforward neural network approximates a function $f^*$ where for input $\vect{x}$, the network computes $y = f(\vect{x}; \vect{\theta})$ and learns parameters $\vect{\theta}$ such that $f \approx f^*$.

\subsection{Network Architecture}

A feedforward network consists of layers including an input layer that receives raw features $\vect{x}$, hidden layers that create intermediate representations $\vect{h}^{(1)}, \vect{h}^{(2)}, \ldots$ through nonlinear transformations, and an output layer that produces predictions $\hat{y}$ based on the processed information from the hidden layers.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
    % Input layer
    \foreach \i in {1,2,3}
        \node[circle, draw, fill=bookpurple!20, minimum size=0.8cm] (x\i) at (0,-\i*1.2) {$x_\i$};
    
    % Hidden layer 1
    \foreach \i in {1,2,3,4}
        \node[circle, draw, fill=bookpurple!30, minimum size=0.8cm] (h1\i) at (3,-\i*1.2) {$h_1^{(\i)}$};
    
    % Hidden layer 2
    \foreach \i in {1,2,3}
        \node[circle, draw, fill=bookpurple!30, minimum size=0.8cm] (h2\i) at (6,-\i*1.2) {$h_2^{(\i)}$};
    
    % Output layer
    \foreach \i in {1,2}
        \node[circle, draw, fill=bookred!20, minimum size=0.8cm] (y\i) at (9,-\i*1.2) {$\hat{y}_\i$};
    
    % Connections from input to hidden1
    \foreach \i in {1,2,3}
        \foreach \j in {1,2,3,4}
            \draw[->] (x\i) -- (h1\j);
    
    % Connections from hidden1 to hidden2
    \foreach \i in {1,2,3,4}
        \foreach \j in {1,2,3}
            \draw[->] (h1\i) -- (h2\j);
    
    % Connections from hidden2 to output
    \foreach \i in {1,2,3}
        \foreach \j in {1,2}
            \draw[->] (h2\i) -- (y\j);
    
    % Labels
    \node at (0, 0.5) {\textbf{Input Layer}};
    \node at (3, 0.5) {\textbf{Hidden Layer 1}};
    \node at (6, 0.5) {\textbf{Hidden Layer 2}};
    \node at (9, 0.5) {\textbf{Output Layer}};
\end{tikzpicture}
\caption{Architecture of a feedforward neural network with 2 hidden layers. Each circle represents a neuron, and arrows show the flow of information from input to output.}
\label{fig:feedforward-architecture}
\end{figure}

For a network with $L$ layers:

\begin{equation}
\vect{h}^{(l)} = \sigma(\mat{W}^{(l)} \vect{h}^{(l-1)} + \vect{b}^{(l)})
\end{equation}

where $\vect{h}^{(0)} = \vect{x}$, $\mat{W}^{(l)}$ are weights, $\vect{b}^{(l)}$ are biases, and $\sigma$ is an activation function.

\subsection{Forward Propagation}

The computation proceeds from input to output through a series of matrix multiplications and nonlinear transformations. For each layer $l$, the network computes pre-activations $\vect{z}^{(l)} = \mat{W}^{(l)} \vect{h}^{(l-1)} + \vect{b}^{(l)}$ followed by activations $\vect{h}^{(l)} = \sigma(\vect{z}^{(l)})$ where $\sigma$ is the activation function. The process continues through all hidden layers until reaching the output layer, where the final prediction $\hat{y} = \vect{h}^{(L)}$ is produced.

\begin{algorithm}[htbp]
\caption{Forward Propagation Algorithm}
\label{alg:forward-propagation}
\begin{algorithmic}[1]
\State \textbf{Input:} $\vect{x}$, weights $\{\mat{W}^{(l)}\}_{l=1}^{L}$, biases $\{\vect{b}^{(l)}\}_{l=1}^{L}$
\State Initialize $\vect{h}^{(0)} = \vect{x}$
\For{$l = 1$ to $L$}
    \State Compute pre-activation: $\vect{z}^{(l)} = \mat{W}^{(l)} \vect{h}^{(l-1)} + \vect{b}^{(l)}$
    \State Apply activation function: $\vect{h}^{(l)} = \sigma(\vect{z}^{(l)})$
\EndFor
\State \textbf{Return} $\hat{y} = \vect{h}^{(L)}$
\end{algorithmic}
\end{algorithm}

\begin{example}[Simple Forward Pass]
\label{ex:forward-pass}

Consider a simple network with 2 inputs, 2 hidden neurons, and 1 output for binary classification, where the input is $\vect{x} = [1, 0.5]$, weights to hidden layer are $\mat{W}^{(1)} = \begin{bmatrix} 0.5 & -0.3 \\ 0.8 & 0.2 \end{bmatrix}$, bias is $\vect{b}^{(1)} = [0.1, -0.2]$, and the activation function is ReLU. This example demonstrates how the network processes information through matrix multiplication and nonlinear transformation to produce meaningful representations.

\textbf{Step 1:} Compute pre-activation
\begin{align}
\vect{z}^{(1)} &= \mat{W}^{(1)} \vect{x} + \vect{b}^{(1)} \\
&= \begin{bmatrix} 0.5 & -0.3 \\ 0.8 & 0.2 \end{bmatrix} \begin{bmatrix} 1 \\ 0.5 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.2 \end{bmatrix} \\
&= \begin{bmatrix} 0.5 \cdot 1 + (-0.3) \cdot 0.5 + 0.1 \\ 0.8 \cdot 1 + 0.2 \cdot 0.5 + (-0.2) \end{bmatrix} \\
&= \begin{bmatrix} 0.45 \\ 0.7 \end{bmatrix}
\end{align}

\textbf{Step 2:} Apply activation function
\begin{align}
\vect{h}^{(1)} &= \text{ReLU}(\vect{z}^{(1)}) = \begin{bmatrix} \max(0, 0.45) \\ \max(0, 0.7) \end{bmatrix} = \begin{bmatrix} 0.45 \\ 0.7 \end{bmatrix}
\end{align}

The hidden layer has learned to represent the input in a transformed space where both neurons are active (positive values).
\end{example}

\subsection{Universal Approximation}

The \textbf{universal approximation theorem} states that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of $\mathbb{R}^n$, given appropriate activation functions.

However, deeper networks often learn more efficiently.

% Index entries
\index{feedforward network!introduction}
\index{neural network!feedforward}
\index{multilayer perceptron}
\index{forward propagation}
\index{universal approximation theorem}
\index{network architecture}

