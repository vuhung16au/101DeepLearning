% Chapter 3, Section 3: Expectation, Variance, and Covariance

\section{Expectation, Variance, and Covariance \difficultyInline{beginner}}
\label{sec:expectation-variance}

Expectation, variance, and covariance are fundamental statistical measures that characterize the behavior of random variables, providing essential tools for understanding data distributions, relationships between variables, and uncertainty quantification in machine learning and deep learning applications.

\subsection{Intuition: Characterizing Random Variables}

When we have a random variable, we often want to summarize its behavior with a few key numbers that capture the essential characteristics of the distribution. The expected value or mean represents the "center" or "typical" value around which the data is distributed, while variance measures how much the values spread out from the center, indicating the degree of uncertainty or variability in the data. Covariance tells us how two variables move together, revealing whether they tend to increase or decrease simultaneously, which is crucial for understanding relationships between different features or measurements. Think of it like describing a person: the mean height represents the average height of people in a group, the variance in height shows how much heights vary between tall and short people, and the covariance of height and weight reveals whether taller people tend to weigh more, providing insights into the relationships between different characteristics.

\subsection{Expectation}

The \textbf{expected value} or \textbf{mean} of a function $f(x)$ with respect to distribution $P(x)$ is:

For discrete variables:
\begin{equation}
\mathbb{E}_{x \sim P}[f(x)] = \sum_{x} P(x) f(x)
\end{equation}

For continuous variables:
\begin{equation}
\mathbb{E}_{x \sim p}[f(x)] = \int p(x) f(x) \, dx
\end{equation}

\begin{remark}
Discrete expectation is a special case of continuous expectation. When we have a discrete distribution with probability mass function $P(x)$, we can think of it as a continuous distribution using Dirac delta functions: $p(x) = \sum_i P(x_i) \delta(x - x_i)$. The integral $\int p(x) f(x) \, dx$ then becomes $\sum_i P(x_i) f(x_i)$, which is exactly the discrete expectation formula. This unified view helps us understand that both discrete and continuous expectations follow the same fundamental principle of weighted averaging.
\end{remark}

\begin{examplebox}{Expected Value of Dice}
For a fair six-sided die:
\begin{align}
\mathbb{E}[X] &= \sum_{x=1}^{6} x \cdot P(X=x) \\
&= 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \cdots + 6 \cdot \frac{1}{6} \\
&= \frac{1+2+3+4+5+6}{6} = \frac{21}{6} = 3.5
\end{align}

The expected value is 3.5, even though we can never actually roll 3.5!

\begin{center}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    ylabel={Probability},
    xlabel={Dice Value},
    xtick={1,2,3,4,5,6},
    ymin=0,
    ymax=0.2,
    width=10cm,
    height=6cm
]
\addplot coordinates {(1,1/6) (2,1/6) (3,1/6) (4,1/6) (5,1/6) (6,1/6)};
\draw[bookred, thick] (axis cs:3.5,0) -- (axis cs:3.5,0.2);
\node[bookred] at (axis cs:3.5,0.18) {$\mathbb{E}[X] = 3.5$};
\end{axis}
\end{tikzpicture}
\end{center}
\end{examplebox}

\subsection{Variance}

\subsubsection{Intuition: Measuring Spread}

Variance tells us how "spread out" the values are around the mean. Imagine two dart players with very different playing styles. The first player has low variance in their throws - every dart lands very close to the bullseye, creating a tight cluster of holes around the center. This player is consistent and precise, with their throws showing little variation from the target. The second player has high variance in their throws - their darts are scattered all over the board, some landing far to the left, others to the right, some high, some low. This player is inconsistent and unpredictable, with their throws showing large variation from the target. Variance quantifies this difference in consistency, measuring how much the individual values deviate from the average or expected value.

The \textbf{variance} measures the spread of a distribution:

\begin{equation}
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{equation}

The \textbf{standard deviation} is $\sigma = \sqrt{\text{Var}(X)}$.

\subsubsection{Example: Variance of Dice}

For our fair die:
\begin{align}
\text{Var}(X) &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\
&= \left(\frac{1^2 + 2^2 + \cdots + 6^2}{6}\right) - (3.5)^2 \\
&= \frac{91}{6} - 12.25 = 15.17 - 12.25 = 2.92
\end{align}

So $\sigma = \sqrt{2.92} \approx 1.71$.

\begin{center}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    ylabel={Probability},
    xlabel={Dice Value},
    xtick={1,2,3,4,5,6},
    ymin=0,
    ymax=0.2,
    width=10cm,
    height=6cm
]
\addplot coordinates {(1,1/6) (2,1/6) (3,1/6) (4,1/6) (5,1/6) (6,1/6)};
\draw[bookred, thick] (axis cs:3.5,0) -- (axis cs:3.5,0.2);
\node[bookred] at (axis cs:3.5,0.18) {$\mu = 3.5$};
\draw[bookpurple, thick, <->] (axis cs:1.79,0.1) -- (axis cs:5.21,0.1);
\node[bookpurple] at (axis cs:3.5,0.12) {$\sigma \approx 1.71$};
\end{axis}
\end{tikzpicture}
\end{center}

\subsection{Covariance}

Covariance tells us whether two variables tend to move in the same direction or opposite directions, with positive covariance indicating that when one variable goes up, the other tends to go up too, negative covariance indicating that when one goes up, the other tends to go down, and zero covariance indicating no clear relationship. Examples include height and weight having positive covariance because taller people tend to weigh more, price and demand having negative covariance because higher prices usually mean lower demand, and height and IQ having near zero covariance because there's no clear relationship between physical height and cognitive ability.

The \textbf{covariance} measures how two variables vary together:

\begin{equation}
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
\end{equation}

Positive covariance indicates that $X$ and $Y$ tend to increase together, while negative covariance indicates they tend to vary in opposite directions.

\begin{examplebox}{Height and Weight}
Consider a small dataset of people:
\begin{center}
\begin{tabular}{|c|c|}
\hline
Height (cm) & Weight (kg) \\
\hline
152 & 54 \\
165 & 64 \\
178 & 73 \\
191 & 82 \\
\hline
\end{tabular}
\end{center}

The means are $\mu_X = 171.5$ and $\mu_Y = 68.25$. The covariance is:
\begin{align}
\text{Cov}(X,Y) &= \frac{1}{4}\sum_{i=1}^{4}(x_i - 171.5)(y_i - 68.25) \\
&= \frac{1}{4}[(-19.5)(-14.25) + (-6.5)(-4.25) + (6.5)(4.75) + (19.5)(13.75)] \\
&= \frac{1}{4}[277.875 + 27.625 + 30.875 + 268.125] = \frac{604.5}{4} = 151.125
\end{align}

Positive covariance confirms that taller people tend to weigh more!
\end{examplebox}

\subsection{Correlation}

The correlation coefficient normalizes covariance by dividing it by the product of the standard deviations, providing a standardized measure of linear relationship between variables that ranges from -1 to 1. A correlation of 1 indicates a perfect positive linear relationship, a correlation of -1 indicates a perfect negative linear relationship, and a correlation of 0 indicates no linear relationship, though the variables may still be dependent in non-linear ways.

\begin{definition}[Correlation Coefficient]
The correlation coefficient between random variables $X$ and $Y$ is defined as:
\begin{equation}
\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}
\end{equation}
where $\sigma_X$ and $\sigma_Y$ are the standard deviations of $X$ and $Y$ respectively.
\end{definition}

The correlation coefficient has several important properties. It is always bounded between -1 and 1, with $\rho = 1$ indicating perfect positive linear correlation (as one variable increases, the other increases proportionally), $\rho = -1$ indicating perfect negative linear correlation (as one variable increases, the other decreases proportionally), and $\rho = 0$ indicating no linear correlation (though the variables may still be related in non-linear ways). The correlation coefficient is invariant to linear transformations of the variables, meaning that scaling or shifting the variables doesn't change their correlation. This makes correlation particularly useful for comparing relationships between variables that may have different scales or units.
