% Chapter 8: Optimization for Training Deep Models

\chapter{Optimization for Training Deep Models}
\label{chap:optimization}

This chapter covers optimization algorithms and strategies for training deep neural networks effectively. Modern optimizers go beyond basic gradient descent to accelerate convergence and improve performance.

\begin{learningobjectives}
\objective{Gradient descent variants and appropriate batch size selection}
\objective{Momentum-based methods including classical momentum and Nesterov accelerated gradient}
\objective{Adaptive optimizers (AdaGrad, RMSProp, Adam) and hyperparameter tuning}
\objective{Second-order approaches and when they are practical}
\objective{Optimization challenges and their remedies}
\objective{Optimization plan combining initializer, optimizer, and learning rate schedule}
\end{learningobjectives}




\input{chapters/chap08-sec01}
\input{chapters/chap08-sec02}
\input{chapters/chap08-sec03}
\input{chapters/chap08-sec04}
\input{chapters/chap08-sec05}
\input{chapters/chap08-sec06}
% \input{chapters/chap08-real-world-applications}
% \input{chapters/chap08-sec07}

% Chapter exercises
\input{chapters/chap08-exercises}
