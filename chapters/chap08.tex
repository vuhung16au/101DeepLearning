% Chapter 8: Optimization for Training Deep Models

\chapter{Optimization for Training Deep Models}
\label{chap:optimization}

This chapter covers optimization algorithms and strategies for training deep neural networks effectively. Modern optimizers go beyond basic gradient descent to accelerate convergence and improve performance.

\section*{Learning Objectives}
\label{sec:ch8-learning-objectives}

After completing this chapter, you will be able to:

\begin{enumerate}
    \item \textbf{Compare gradient descent variants} (batch, stochastic, mini-batch) and choose appropriate batch sizes.
    \item \textbf{Explain and implement momentum-based methods} including classical momentum and Nesterov accelerated gradient.
    \item \textbf{Apply adaptive optimizers} (AdaGrad, RMSProp, Adam) and tune key hyperparameters and schedules.
    \item \textbf{Describe second-order approaches} (Newton, quasi-Newton/L-BFGS, natural gradient) and when they are practical.
    \item \textbf{Diagnose optimization challenges} (vanishing/exploding gradients, saddle points, plateaus) and apply remedies.
    \item \textbf{Design an optimization plan} combining initializer choice, optimizer, learning rate schedule, and gradient clipping.
\end{enumerate}




\input{chapters/chap08-sec01}
\input{chapters/chap08-sec02}
\input{chapters/chap08-sec03}
\input{chapters/chap08-sec04}
\input{chapters/chap08-sec05}
\input{chapters/chap08-sec06}
\input{chapters/chap08-real-world-applications}
% \input{chapters/chap08-sec07}

% Chapter exercises
\input{chapters/chap08-problems}
