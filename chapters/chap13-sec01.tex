% Chapter 13, Section 1

\section{Probabilistic PCA \difficultyInline{intermediate}}
\label{sec:prob-pca}

Probabilistic PCA extends classical PCA by providing a probabilistic framework that handles uncertainty and enables principled treatment of noise and missing data in dimensionality reduction.

\subsection{Principal Component Analysis Review}

PCA finds orthogonal directions of maximum variance through the transformation $\vect{z} = \mat{W}^\top (\vect{x} - \boldsymbol{\mu})$ where $\mat{W}$ contains principal components (eigenvectors of covariance matrix). In deep learning, this formula is fundamental for dimensionality reduction in preprocessing steps, where for example, when training a neural network on high-dimensional image data, PCA can reduce the input dimensionality from thousands of pixels to a smaller set of principal components, making training more efficient while preserving the most important variance in the data.

\subsection{Probabilistic Formulation}

The probabilistic formulation models observations through a generative process where latent variables $\vect{z} \sim \mathcal{N}(\boldsymbol{0}, \mat{I})$ follow a standard normal distribution, and observed data $\vect{x} | \vect{z} \sim \mathcal{N}(\mat{W}\vect{z} + \boldsymbol{\mu}, \sigma^2 \mat{I})$ is generated by linearly transforming the latent variables and adding Gaussian noise. This formulation allows us to handle uncertainty in the data generation process, where the noise parameter $\sigma^2$ captures the amount of unexplained variance. When marginalizing over the latent variables $\vect{z}$, we obtain the marginal distribution $\vect{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mat{W}\mat{W}^\top + \sigma^2 \mat{I})$, which shows that the observed data follows a Gaussian distribution with mean $\boldsymbol{\mu}$ and covariance matrix $\mat{W}\mat{W}^\top + \sigma^2 \mat{I}$, where the first term captures the variance explained by the latent factors and the second term represents the noise variance.

\subsection{Learning}

Learning in probabilistic PCA maximizes the likelihood using the EM algorithm, where the E-step computes the posterior distribution $p(\vect{z}|\vect{x})$ to estimate the latent variables given the observed data, and the M-step updates the parameters $\mat{W}$, $\boldsymbol{\mu}$, and $\sigma^2$ to maximize the expected log-likelihood. This iterative process allows the model to learn the optimal linear transformation and noise parameters that best explain the observed data, where the EM algorithm is particularly useful because it handles the latent variables naturally and provides a principled way to estimate parameters in the presence of missing data. As the noise variance $\sigma^2 \to 0$, the probabilistic PCA recovers standard PCA, showing that classical PCA is a special case of the probabilistic formulation when we assume no noise in the data generation process.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (PPCA)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={$x_1$}, ylabel={$x_2$}, grid=both]
%       % data cloud
%       \addplot+[only marks,mark=*,mark size=0.8pt,bookpurple!60] coordinates{(-1.2,-0.9) (-0.8,-0.7) (-0.2,-0.3) (0.2,0.1) (0.6,0.4) (1.0,0.8)};
%       % principal axis
%       \addplot[bookred,very thick,domain=-1.5:1.5]{0.8*x};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{PCA principal axis capturing maximum variance.}
%   \label{fig:ppca-axis}
% \end{figure}

\subsection{Historical context and references}

The probabilistic formulation of PCA was developed to connect classical PCA with latent variable models and enable principled handling of noise and missing data, representing a significant advancement in dimensionality reduction techniques. This probabilistic approach has been widely adopted in machine learning and deep learning applications, where it provides a foundation for understanding how linear transformations can capture the essential structure in high-dimensional data. The work by Bishop and colleagues has been particularly influential in establishing the theoretical foundations and practical applications of probabilistic PCA, while Goodfellow and colleagues have shown how these concepts extend to modern deep learning architectures. Real-world applications include image compression, noise reduction in signal processing, and preprocessing for neural network training, where the probabilistic framework allows for better handling of uncertainty and missing data compared to classical PCA.
