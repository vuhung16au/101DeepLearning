% Chapter 13, Section 1

\section{Probabilistic PCA \difficultyInline{intermediate}}
\label{sec:prob-pca}

Probabilistic PCA extends classical PCA by providing a probabilistic framework that handles uncertainty and enables principled treatment of noise and missing data in dimensionality reduction.

\subsection{Principal Component Analysis Review}

PCA finds orthogonal directions of maximum variance through the transformation $\vect{z} = \mat{W}^\top (\vect{x} - \boldsymbol{\mu})$ where $\mat{W}$ contains principal components (eigenvectors of covariance matrix). In deep learning, this formula is fundamental for dimensionality reduction in preprocessing steps, where for example, when training a neural network on high-dimensional image data, PCA can reduce the input dimensionality from thousands of pixels to a smaller set of principal components, making training more efficient while preserving the most important variance in the data.

\subsection{Probabilistic Formulation}

\begin{remark}[Key Differences between PCA and Probabilistic PCA]
The key difference is that classical PCA is deterministic and finds orthogonal directions of maximum variance, while Probabilistic PCA provides a probabilistic framework that models uncertainty through a generative process with noise, enabling principled treatment of missing data and uncertainty quantification.
\end{remark}

The probabilistic formulation models observations through a generative process:
\begin{align}
\vect{z} &\sim \mathcal{N}(\boldsymbol{0}, \mat{I}) \\
\vect{x} \,|\, \vect{z} &\sim \mathcal{N}(\mat{W}\vect{z} + \boldsymbol{\mu}, \sigma^2 \mat{I})
\end{align}
\noindent\textbf{Explanations.}
\begin{itemize}[leftmargin=1.5em]
  \item $\vect{z} \sim \mathcal{N}(\boldsymbol{0}, \mat{I})$ means the latent variable has a \emph{standard normal} prior: zero mean and identity covariance (independent unit-variance components).
  \item $\vect{x} | \vect{z} \sim \mathcal{N}(\mat{W}\vect{z}+\boldsymbol{\mu}, \sigma^2\mat{I})$ is a \emph{Gaussian likelihood}: data is a linear mapping of $\vect{z}$ via loadings $\mat{W}$ plus mean $\boldsymbol{\mu}$, with isotropic noise variance $\sigma^2$.
\end{itemize}
Marginalizing the latent variable yields
\begin{equation}
\vect{x} \sim \mathcal{N}\big(\boldsymbol{\mu},\; \underbrace{\mat{W}\mat{W}^\top}_{\text{signal}} + \underbrace{\sigma^2\mat{I}}_{\text{noise}}\big).
\end{equation}
Here $\mat{W}\mat{W}^\top + \sigma^2\mat{I}$ is the \emph{covariance matrix} of the observed data: $\mat{W}\mat{W}^\top$ captures variance explained by latent factors and $\sigma^2\mat{I}$ is residual (isotropic) noise.

\begin{definition}[Probabilistic PCA]
Probabilistic PCA is a probabilistic extension of classical PCA that models observed data through a generative process where latent variables $\vect{z}$ follow a standard normal distribution and observed data $\vect{x}$ is generated as a linear transformation of the latent variables plus Gaussian noise, enabling principled treatment of uncertainty and missing data.
\end{definition}

\subsection{Learning}

\begin{remark}[EM Algorithm]
The EM (Expectation-Maximization) algorithm is an iterative optimization method for maximum likelihood estimation in latent variable models, defined as $\theta^{(t+1)} = \arg\max_\theta \mathbb{E}_{p(\vect{z}|\vect{x},\theta^{(t)})}[\log p(\vect{x},\vect{z}|\theta)]$ where the E-step computes the posterior expectation and the M-step maximizes the expected log-likelihood.
\end{remark}

Learning in probabilistic PCA maximizes the likelihood using the EM algorithm, where the E-step computes the posterior distribution $p(\vect{z}|\vect{x})$ to estimate the latent variables given the observed data, and the M-step updates the parameters $\mat{W}$, $\boldsymbol{\mu}$, and $\sigma^2$ to maximize the expected log-likelihood. This iterative process allows the model to learn the optimal linear transformation and noise parameters that best explain the observed data, where the EM algorithm is particularly useful because it handles the latent variables naturally and provides a principled way to estimate parameters in the presence of missing data. As the noise variance $\sigma^2 \to 0$, the probabilistic PCA recovers standard PCA, showing that classical PCA is a special case of the probabilistic formulation when we assume no noise in the data generation process.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (PPCA)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={$x_1$}, ylabel={$x_2$}, grid=both]
%       % data cloud
%       \addplot+[only marks,mark=*,mark size=0.8pt,bookpurple!60] coordinates{(-1.2,-0.9) (-0.8,-0.7) (-0.2,-0.3) (0.2,0.1) (0.6,0.4) (1.0,0.8)};
%       % principal axis
%       \addplot[bookred,very thick,domain=-1.5:1.5]{0.8*x};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{PCA principal axis capturing maximum variance.}
%   \label{fig:ppca-axis}
% \end{figure}

\subsection{Historical context and references}

The probabilistic formulation of PCA was developed to connect classical PCA with latent variable models and enable principled handling of noise and missing data, representing a significant advancement in dimensionality reduction techniques. This probabilistic approach has been widely adopted in machine learning and deep learning applications, where it provides a foundation for understanding how linear transformations can capture the essential structure in high-dimensional data. Previous work has been establishing the theoretical foundations and practical applications of probabilistic PCA, while Goodfellow and colleagues have shown how these concepts extend to modern deep learning architectures. Real-world applications include image compression, noise reduction in signal processing, and preprocessing for neural network training, where the probabilistic framework allows for better handling of uncertainty and missing data compared to classical PCA.
