% Problems (Hands-On Exercises) for Chapter 2: Linear Algebra

\section*{Problems}
\addcontentsline{toc}{section}{Problems}

\subsection*{Easy}

\begin{problem}[Matrix Multiplication]
Given $\mat{A} = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}$ and $\mat{B} = \begin{bmatrix} 1 & 2 \\ 3 & 1 \end{bmatrix}$, compute $\mat{A}\mat{B}$.

\textbf{Hint:} Remember that $(\mat{A}\mat{B})_{ij} = \sum_k a_{ik}b_{kj}$.
\end{problem}

\begin{problem}[Vector Norms]
Calculate the L1, L2, and $L_\infty$ norms of the vector $\vect{v} = [3, -4, 0]$.

\textbf{Hint:} L1 norm is sum of absolute values, L2 norm is Euclidean length, $L_\infty$ is maximum absolute value.
\end{problem}

\begin{problem}[Linear Independence]
Determine whether the vectors $\vect{v}_1 = [1, 0, 1]$, $\vect{v}_2 = [0, 1, 0]$, and $\vect{v}_3 = [1, 1, 1]$ are linearly independent.

\textbf{Hint:} Check if $c_1\vect{v}_1 + c_2\vect{v}_2 + c_3\vect{v}_3 = \boldsymbol{0}$ has only the trivial solution $c_1 = c_2 = c_3 = 0$.
\end{problem}

\begin{problem}[Matrix Transpose Properties]
Prove that $(\mat{A}\mat{B})^\top = \mat{B}^\top\mat{A}^\top$ for any compatible matrices $\mat{A}$ and $\mat{B}$.

\textbf{Hint:} Use the definition of transpose and matrix multiplication element-wise.
\end{problem}

\begin{problem}[Dot Product and Angle]
Calculate the dot product of vectors $\vect{u} = [1, 2, 3]$ and $\vect{v} = [4, -1, 2]$, and find the angle between them.

\textbf{Hint:} Use $\vect{u} \cdot \vect{v} = |\vect{u}||\vect{v}|\cos\theta$ and the dot product formula.
\end{problem}

\begin{problem}[Matrix Addition and Scalar Multiplication]
Given $\mat{A} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ and $\mat{B} = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}$, compute $2\mat{A} + 3\mat{B}$.

\textbf{Hint:} Perform scalar multiplication first, then add the resulting matrices element-wise.
\end{problem}

\begin{problem}[Identity Matrix Properties]
Show that $\mat{I}\mat{A} = \mat{A}\mat{I} = \mat{A}$ for any square matrix $\mat{A}$ of the same size as the identity matrix $\mat{I}$.

\textbf{Hint:} Use the definition of the identity matrix where $I_{ij} = 1$ if $i = j$ and $0$ otherwise.
\end{problem}

\begin{problem}[Vector Space Axioms]
Verify that the set of all $2 \times 2$ matrices forms a vector space under matrix addition and scalar multiplication.

\textbf{Hint:} Check closure, associativity, commutativity, identity element, inverse element, and distributive properties.
\end{problem}

\begin{problem}[Cross Product in 3D]
Calculate the cross product of $\vect{a} = [1, 0, 2]$ and $\vect{b} = [3, 1, 1]$, and verify that the result is perpendicular to both vectors.

\textbf{Hint:} Use the determinant formula for cross product and check that $\vect{a} \cdot (\vect{a} \times \vect{b}) = 0$.
\end{problem}

\begin{problem}[Matrix Rank]
Find the rank of the matrix $\mat{C} = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 1 & 1 & 2 \end{bmatrix}$.

\textbf{Hint:} Use row reduction to find the number of linearly independent rows or columns.
\end{problem}

\begin{problem}[Determinant Properties]
Calculate the determinant of $\mat{D} = \begin{bmatrix} 2 & 1 & 0 \\ 1 & 3 & 2 \\ 0 & 1 & 1 \end{bmatrix}$ using cofactor expansion.

\textbf{Hint:} Expand along the first row: $\det(\mat{D}) = \sum_{j=1}^{3} (-1)^{1+j} d_{1j} M_{1j}$.
\end{problem}

\subsection*{Medium}

\begin{problem}[Eigenvalues and Eigenvectors]
Find the eigenvalues and eigenvectors of the matrix $\mat{A} = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}$.

\textbf{Hint:} Solve $\det(\mat{A} - \lambda \mat{I}) = 0$ for eigenvalues, then find eigenvectors by solving $(\mat{A} - \lambda \mat{I})\vect{v} = \boldsymbol{0}$.
\end{problem}

\begin{problem}[SVD Application]
Explain how Singular Value Decomposition (SVD) can be used for dimensionality reduction. Describe the relationship between SVD and Principal Component Analysis (PCA).

\textbf{Hint:} Consider which singular values and vectors to keep, and how this relates to variance in the data.
\end{problem}

\begin{problem}[Matrix Inversion and Linear Systems]
Solve the system of linear equations using matrix inversion: $2x + y = 5$ and $x - 3y = -1$.

\textbf{Hint:} Write as $\mat{A}\vect{x} = \vect{b}$, then $\vect{x} = \mat{A}^{-1}\vect{b}$.
\end{problem}

\begin{problem}[Orthogonal Matrices]
Prove that the columns of an orthogonal matrix are orthonormal vectors. Show that $\mat{Q}^\top\mat{Q} = \mat{I}$ for an orthogonal matrix $\mat{Q}$.

\textbf{Hint:} Use the definition of orthogonality and the properties of matrix multiplication.
\end{problem}

\begin{problem}[Eigenvalue Decomposition]
Find the eigenvalue decomposition of the symmetric matrix $\mat{S} = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}$ and verify that $\mat{S} = \mat{Q}\mat{\Lambda}\mat{Q}^\top$.

\textbf{Hint:} For symmetric matrices, eigenvectors are orthogonal and can be normalised to form an orthogonal matrix.
\end{problem}

\begin{problem}[Matrix Norms and Condition Number]
Calculate the Frobenius norm and the condition number of the matrix $\mat{A} = \begin{bmatrix} 1 & 2 \\ 0.5 & 1 \end{bmatrix}$.

\textbf{Hint:} Frobenius norm is $\|\mat{A}\|_F = \sqrt{\sum_{i,j} a_{ij}^2}$, condition number is $\kappa(\mat{A}) = \|\mat{A}\|\|\mat{A}^{-1}\|$.
\end{problem}

\subsection*{Hard}

\begin{problem}[Matrix Decomposition for Neural Networks]
Show how the weight matrix in a neural network layer can be decomposed using SVD to reduce the number of parameters. Analyse the computational and memory trade-offs.

\textbf{Hint:} Consider low-rank approximation $\mat{W} \approx \mat{U}_k \mat{\Sigma}_k \mat{V}_k^\top$ where $k < \min(m,n)$.
\end{problem}

\begin{problem}[Krylov Subspace Methods]
Explain how Krylov subspace methods can be used to solve large linear systems efficiently. Compare the computational complexity with direct methods.

\textbf{Hint:} Consider the Arnoldi iteration and how it constructs an orthogonal basis for $\mathcal{K}_k(\mat{A}, \vect{b})$.
\end{problem}

\begin{problem}[Tensor Decomposition]
Derive the CP (CANDECOMP/PARAFAC) decomposition for a 3-way tensor and show how it relates to matrix factorisation. Discuss applications in deep learning.

\textbf{Hint:} Express the tensor as a sum of rank-1 components: $\mathcal{T} = \sum_{r=1}^R \lambda_r \vect{a}_r \circ \vect{b}_r \circ \vect{c}_r$.
\end{problem}

\begin{problem}[Numerical Stability in Matrix Computations]
Analyse the numerical stability of computing eigenvalues using the QR algorithm. Discuss the role of Householder transformations and Givens rotations.

\textbf{Hint:} Consider the accumulation of rounding errors and the convergence properties of the QR iteration.
\end{problem}

\begin{problem}[Sparse Matrix Representations]
Design an efficient storage scheme for sparse matrices and implement key operations (matrix-vector multiplication, matrix-matrix multiplication). Analyse the computational complexity.

\textbf{Hint:} Consider formats like CSR (Compressed Sparse Row) or COO (Coordinate) and their trade-offs in terms of storage and access patterns.
\end{problem}

