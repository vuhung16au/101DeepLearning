% Chapter 18, Section 2

\section{Contrastive Divergence \difficultyInline{advanced}}
\label{sec:contrastive-divergence}

Contrastive divergence is a practical training algorithm that approximates the intractable gradients in energy-based models by using short Markov chains instead of requiring exact sampling from the model distribution.

\subsection{Motivation}

For Restricted Boltzmann Machines (RBMs), the joint probability distribution is given by:
\begin{equation}
p(\vect{v}, \vect{h}) = \frac{1}{Z} \exp(-E(\vect{v}, \vect{h}))
\end{equation}

This equation (18.2) shows that the probability of a configuration is proportional to the exponential of the negative energy, normalized by the partition function $Z$. The exact gradient for learning requires computing expectations under the model distribution, which involves the intractable partition function.

The gradient computation involves two terms:
\begin{equation}
\frac{\partial \log p(\vect{v})}{\partial \theta} = -\mathbb{E}_{p(\vect{h}|\vect{v})}\left[\frac{\partial E}{\partial \theta}\right] + \mathbb{E}_{p(\vect{v}, \vect{h})}\left[\frac{\partial E}{\partial \theta}\right]
\end{equation}

This equation (18.3) shows that the gradient consists of a positive term (expectation under the data distribution) and a negative term (expectation under the model distribution). The second term requires sampling from the full model distribution, which is intractable due to the partition function, motivating the need for approximate methods like contrastive divergence.

\subsection{CD-k Algorithm}

The CD-k algorithm approximates the intractable second term in the gradient by using a short Markov Chain Monte Carlo (MCMC) chain. An MCMC chain is a sequence of samples generated by iteratively applying a transition operator that preserves the target distribution, allowing us to sample from complex distributions without computing the partition function.

The algorithm works by starting from the data and running a short chain of k Gibbs sampling steps, which provides a biased but computationally efficient approximation to the true gradient. The key insight is that we don't need to run the chain until convergence; even a few steps provide a useful approximation that enables effective learning. This approach trades off between computational efficiency and approximation accuracy, making it practical for training energy-based models on large datasets.

Despite being biased, the CD-k algorithm works surprisingly well in practice because the bias tends to be in a direction that still provides useful gradient information for learning. The algorithm has been successfully applied to training Restricted Boltzmann Machines and other energy-based models, demonstrating that approximate methods can be highly effective even when they don't provide exact gradients.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (contrastive divergence)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={$k$ steps}, ylabel={Reconstruction error}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1,0.20) (2,0.15) (5,0.12) (10,0.11)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{CD-$k$: reconstruction error vs. number of Gibbs steps (illustrative).}
%   \label{fig:cdk}
% \end{figure}

% \subsection{Notes and references}

% Contrastive divergence for RBMs is covered in \textcite{GoodfellowEtAl2016,Prince2023}.

