% Chapter 4: Numerical Computation

\chapter{Numerical Computation}
\label{chap:numerical-computation}

This chapter covers numerical methods and computational considerations essential for implementing deep learning algorithms. Topics include gradient-based optimization, numerical stability, and conditioning.

\section*{Learning Objectives}

After studying this chapter, you will be able to:

\begin{itemize}
    \item \textbf{Understand numerical precision issues}: Recognize how finite precision arithmetic affects deep learning computations and implement solutions for overflow, underflow, and numerical instability.
    
    \item \textbf{Master gradient-based optimization}: Apply gradient descent and understand the role of Jacobian and Hessian matrices in optimization landscapes, including critical points and saddle points.
    
    \item \textbf{Handle constrained optimization}: Use Lagrange multipliers and KKT conditions to solve optimization problems with constraints, relevant for regularization and fairness in deep learning.
    
    \item \textbf{Assess numerical stability}: Calculate condition numbers, identify ill-conditioned problems, and implement gradient checking and other numerical verification techniques.
    
    \item \textbf{Apply practical numerical techniques}: Use log-sum-exp tricks, mixed precision training, and other numerical stability methods in real deep learning implementations.
    
    \item \textbf{Debug numerical issues}: Recognize common numerical problems in deep learning and apply appropriate solutions for robust model training.
\end{itemize}

\input{chapters/chap04-sec01}
\input{chapters/chap04-sec02}
\input{chapters/chap04-sec03}
\input{chapters/chap04-sec04}

% Chapter summary and problems
\input{chapters/chap04-key-takeaways}

\section{Problems}
\label{sec:problems}

This section contains exercises to reinforce your understanding of numerical computation concepts. Problems are categorised by difficulty level.

\subsection{Easy Problems}

\begin{problem}[Floating-Point Basics]
\label{prob:float-basics}
Consider a hypothetical 4-bit floating-point system with 1 sign bit, 2 exponent bits, and 1 mantissa bit. What is the smallest positive number that can be represented? What is the largest number?

\textbf{Hint:} Use the IEEE 754 format: $(-1)^s \times 2^{e-b} \times (1 + m)$ where $s$ is the sign bit, $e$ is the exponent, $b$ is the bias, and $m$ is the mantissa.
\end{problem}

\begin{problem}[Softmax Stability]
\label{prob:softmax-stability}
Compute the softmax of $\vect{x} = [1000, 1001, 1002]$ using both the naive approach and the numerically stable approach. Show your work step by step.

\textbf{Hint:} Use the identity $\text{softmax}(\vect{x}) = \text{softmax}(\vect{x} - c)$ where $c = \max_i x_i$.
\end{problem}

\begin{problem}[Gradient Computation]
\label{prob:gradient-computation}
Compute the gradient of $f(x, y) = x^2 + 3xy + y^2$ at the point $(2, 1)$.

\textbf{Hint:} The gradient is $\nabla f = \left[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right]$.
\end{problem}

\begin{problem}[Condition Number]
\label{prob:condition-number}
Calculate the condition number of the matrix $\mat{A} = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$.

\textbf{Hint:} For a 2Ã—2 matrix, $\kappa(\mat{A}) = \frac{\lambda_{\max}}{\lambda_{\min}}$ where $\lambda_{\max}$ and $\lambda_{\min}$ are the eigenvalues.
\end{problem}

\subsection{Medium Problems}

\begin{problem}[Log-Sum-Exp Trick]
\label{prob:log-sum-exp}
Derive the log-sum-exp trick: $\log\left(\sum_{i=1}^n \exp(x_i)\right) = c + \log\left(\sum_{i=1}^n \exp(x_i - c)\right)$ where $c = \max_i x_i$.

\textbf{Hint:} Start by factoring out $\exp(c)$ from the sum.
\end{problem}

\begin{problem}[Lagrange Multipliers]
\label{prob:lagrange-multipliers}
Find the maximum value of $f(x, y) = xy$ subject to the constraint $x^2 + y^2 = 1$ using Lagrange multipliers.

\textbf{Hint:} Set up the Lagrangian $\mathcal{L}(x, y, \lambda) = xy + \lambda(1 - x^2 - y^2)$ and solve the system of equations.
\end{problem}

\subsection{Hard Problems}

\begin{problem}[Numerical Stability of Matrix Inversion]
\label{prob:matrix-inversion-stability}
Consider the matrix $\mat{A} = \begin{bmatrix} 1 & 1 \\ 1 & 1 + \epsilon \end{bmatrix}$ where $\epsilon$ is small. Show that the condition number grows as $\epsilon \to 0$. Implement a numerical experiment to demonstrate this and show how the error in $\mat{A}^{-1}$ grows.

\textbf{Hint:} Use the formula $\kappa(\mat{A}) = \frac{\lambda_{\max}}{\lambda_{\min}}$ and compute the eigenvalues analytically.
\end{problem}

\begin{problem}[KKT Conditions Application]
\label{prob:kkt-conditions}
Consider the optimisation problem:
\begin{align}
\min_{x, y} \quad & x^2 + y^2 \\
\text{subject to} \quad & x + y \geq 1 \\
& x \geq 0, y \geq 0
\end{align}
Find the optimal solution using the KKT conditions and verify that all conditions are satisfied.

\textbf{Hint:} Set up the Lagrangian with multiple constraints and check the complementary slackness conditions.
\end{problem}
