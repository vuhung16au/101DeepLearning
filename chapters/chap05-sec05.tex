% Chapter 5, Section 05

\section{k-Nearest Neighbors \difficultyInline{intermediate}}
\label{sec:knn}

\textbf{k-Nearest Neighbors} (k-NN) is a simple yet powerful non-parametric algorithm that makes predictions based on the similarity to training examples. It's called "lazy learning" because it doesn't build a model during training—all computation happens at prediction time.

\subsection{Intuition and Motivation}

The k-NN algorithm is based on a simple principle: "similar things are close to each other." If you want to predict whether someone will like a movie, look at what movies similar people (with similar tastes) liked. If you want to predict house prices, look at prices of similar houses in the neighborhood.

The key insight is that we can make good predictions by finding the most similar examples in our training data and using their outcomes as a guide.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={Feature 1},
    ylabel={Feature 2},
    title={k-NN Classification Example (k=3)},
    grid=major,
    width=10cm,
    height=8cm
]
% Generate some sample data
\addplot[only marks, mark=*, mark size=3pt, color=bookpurple] coordinates {
    (1,1) (1.5,1.5) (2,1) (2.5,2) (3,1.5)
};
\addplot[only marks, mark=square*, mark size=3pt, color=bookred] coordinates {
    (2,3) (2.5,3.5) (3,3) (3.5,3.5) (4,3)
};
% Add query point
\addplot[only marks, mark=triangle*, mark size=4pt, color=bookblack] coordinates {
    (2.5,2.5)
};
% Add circles around query point
\draw[thick, color=bookblack, dashed] (2.5,2.5) circle (1.2);
\node at (axis cs:2.5,2.5) [anchor=south] {Query point};
\node at (axis cs:2.5,1.3) [anchor=west] {k=3 neighbors};
\legend{Class 0, Class 1, Query point, Neighbors}
\end{axis}
\end{tikzpicture}
\caption{k-NN finds k nearest neighbors. With k=3, query classified as class 0 (2/3 neighbors).}
\label{fig:knn-example}
\end{figure}

\subsection{Algorithm}

For a query point $\vect{x}$, the k-NN algorithm works by finding the $k$ closest training examples based on a distance metric, then for classification returning the majority class among the $k$ neighbors, and for regression returning the average of the target values of the $k$ neighbors. The key ideas of the algorithm include using distance-based similarity to find relevant examples, leveraging the assumption that similar examples have similar outcomes, and making predictions based on local patterns in the data rather than global models.

\subsubsection{Mathematical Formulation}

For classification, the predicted class is:
\begin{equation}
\hat{y} = \arg\max_{c} \sum_{i=1}^{k} \mathbb{I}(y^{(i)} = c)
\end{equation}

For regression, the predicted value is:
\begin{equation}
\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y^{(i)}
\end{equation}

where $y^{(i)}$ are the target values of the $k$ nearest neighbors.

\subsection{Distance Metrics}

The choice of distance metric significantly affects k-NN performance, where different metrics are used in various machine learning algorithms to measure similarity between data points. Euclidean distance is commonly used in k-NN and clustering algorithms, Manhattan distance is useful in recommendation systems and text analysis, and Minkowski distance provides a flexible framework for different distance measures in various machine learning applications.

\subsubsection{Euclidean Distance}

\begin{definition}[Euclidean Distance]
\begin{equation}
d(\vect{x}, \vect{x}') = \sqrt{\sum_{i=1}^{d} (x_i - x_i')^2}
\end{equation}
This is the most common choice, measuring the straight-line distance between points.
\end{definition}

\subsubsection{Manhattan Distance}

\begin{definition}[Manhattan Distance]
\begin{equation}
d(\vect{x}, \vect{x}') = \sum_{i=1}^{d} |x_i - x_i'|
\end{equation}
Also known as L1 distance, it measures the sum of absolute differences. Useful when features have different scales.
\end{definition}

\subsubsection{Minkowski Distance}

\begin{definition}[Minkowski Distance]
\begin{equation}
d(\vect{x}, \vect{x}') = \left(\sum_{i=1}^{d} |x_i - x_i'|^p\right)^{1/p}
\end{equation}
This is a generalization where:
\begin{itemize}
    \item $p = 1$: Manhattan distance
    \item $p = 2$: Euclidean distance
    \item $p \to \infty$: Chebyshev distance (maximum coordinate difference)
\end{itemize}
\end{definition}

\subsection{Choosing k}

The choice of $k$ is crucial and involves a bias-variance trade-off, where small $k$ values like k=1 result in low bias and high variance with flexible decision boundaries that are sensitive to noise and outliers and may overfit to training data, while large $k$ values like k=n result in high bias and low variance with smooth decision boundaries that may miss local patterns and underfit the data. The optimal $k$ is typically found through cross-validation, balancing the need for local pattern recognition with robustness to noise.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={k value},
    ylabel={Error Rate},
    title={Effect of k on k-NN Performance},
    grid=major,
    width=10cm,
    height=6cm,
    xmin=1, xmax=20
]
% Generate a U-shaped curve
\addplot[thick, color=bookpurple, domain=1:20] {0.1 + 0.05*(x-5)^2/25};
\node at (axis cs:5,0.1) [anchor=south] {Optimal k};
\node at (axis cs:1,0.15) [anchor=east] {High variance};
\node at (axis cs:20,0.15) [anchor=west] {High bias};
\end{axis}
\end{tikzpicture}
\caption{Effect of k on k-NN: small k→high variance, large k→high bias. Optimal k via CV.}
\label{fig:knn-k-selection}
\end{figure}

\subsection{Weighted k-NN}

Instead of giving equal weight to all $k$ neighbors, we can weight them by their distance:

\begin{equation}
\hat{y} = \frac{\sum_{i=1}^{k} w_i y^{(i)}}{\sum_{i=1}^{k} w_i}
\end{equation}

where $w_i = \frac{1}{d(\vect{x}, \vect{x}^{(i)})}$ is the weight based on distance.

\subsection{Computational Considerations}

k-NN has several computational characteristics including no training time since it's a lazy learner that defers all computation to prediction time, memory usage that requires storing all training examples, and scalability issues where performance degrades with large datasets. The naive approach has $O(n \cdot d)$ complexity for each prediction, but optimized approaches using data structures like KD-trees, ball trees, and locality sensitive hashing can significantly reduce this complexity for large-scale applications.

\subsubsection{Speedup Techniques}

Several speedup techniques can be employed to improve the computational efficiency of k-NN algorithms. KD-Trees partition the space into regions using hyperplanes, reducing search time to $O(\log n)$ in low dimensions, though they become less effective in high dimensions where the curse of dimensionality affects their performance. Ball Trees use hyperspheres instead of hyperplanes to partition the space, making them better suited for high-dimensional data, though they are more complex to implement than KD-trees. Locality Sensitive Hashing (LSH) provides approximate nearest neighbor search capabilities, offering significant speedup for very large datasets, though it may sacrifice some accuracy in exchange for computational efficiency, making it suitable for applications where approximate results are acceptable.

\subsection{Advantages and Limitations}

k-NN has several advantages including being simple and easy to understand and implement, making no assumptions about data distribution and working with any data type, being non-parametric with no model to fit, being able to capture complex decision boundaries through local patterns, and being incremental by making it easy to add new training examples. However, it also has limitations including computational cost where it's slow for large datasets, memory usage where it must store all training data, sensitivity to irrelevant features where all features are treated equally, the curse of dimensionality where performance degrades in high dimensions, and lack of interpretability where it's hard to understand why a prediction was made.

\subsection{Curse of Dimensionality}

In high-dimensional spaces, k-NN faces the curse of dimensionality where all points become roughly equidistant due to distance concentration, most of the space is empty making it difficult to find meaningful neighbors, and performance degrades with irrelevant features that can dominate the distance calculations, making the algorithm less effective in high-dimensional spaces.

\begin{example}
In a 1000-dimensional space, even if only 10 features are relevant, the 990 irrelevant features can dominate the distance calculations, making k-NN ineffective.
\end{example}

\subsection{Feature Selection and Scaling}

To improve k-NN performance, several techniques can be employed including feature selection to remove irrelevant features that can hurt performance, feature scaling to normalize features to the same scale so that all features contribute equally to distance calculations, dimensionality reduction using PCA or other techniques to reduce the curse of dimensionality, and distance weighting to weight features by importance so that more relevant features have greater influence on the similarity calculations.

