% Chapter 10: Sequence Modeling: Recurrent and Recursive Nets

\chapter{Sequence Modeling: Recurrent and Recursive Nets}
\label{chap:sequence-modeling}

This chapter covers architectures designed for sequential and temporal data, including recurrent neural networks (RNNs) and their variants.


\section*{Learning Objectives}
\label{sec:ch10-learning-objectives}

After completing this chapter, you will be able to:

\begin{enumerate}
    \item \textbf{Explain why sequence models are needed} and identify data modalities that require temporal context.
    \item \textbf{Describe and compare} vanilla RNNs, LSTMs, and GRUs, including their gating mechanisms and trade-offs.
    \item \textbf{Implement and reason about} backpropagation through time (BPTT) and truncated BPTT, including gradient clipping.
    \item \textbf{Build sequence-to-sequence models with attention} and explain the intuition behind alignment and context vectors.
    \item \textbf{Apply advanced decoding and architecture variants} such as bidirectional RNNs, teacher forcing, and beam search.
    \item \textbf{Evaluate common failure modes} (vanishing/exploding gradients, exposure bias) and mitigation strategies.
\end{enumerate}




\input{chapters/chap10-sec01}
\input{chapters/chap10-sec02}
\input{chapters/chap10-sec03}
\input{chapters/chap10-sec04}
\input{chapters/chap10-sec05}
\input{chapters/chap10-sec06}
\input{chapters/chap10-sec07}
\input{chapters/chap10-real-world-applications}
\input{chapters/chap10-sec08}

% Chapter summary and problems
\input{chapters/chap10-key-takeaways}
\input{chapters/chap10-problems}
