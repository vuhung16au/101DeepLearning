% Chapter 10: Sequence Modeling: Recurrent and Recursive Nets

\chapter{Sequence Modeling: Recurrent and Recursive Nets}
\label{chap:sequence-modeling}

This chapter covers architectures designed for sequential and temporal data, including recurrent neural networks (RNNs) and their variants.


\begin{learningobjectives}
\objective{Why sequence models are needed and identify data modalities that require temporal context}
\objective{And compare vanilla RNNs, LSTMs, and GRUs, including their gating mechanisms and trade-offs}
\objective{And reason about backpropagation through time (BPTT) and truncated BPTT, including gradient clipping}
\objective{Sequence-to-sequence models with attention and explain the intuition behind alignment and context vectors}
\objective{Advanced decoding and architecture variants such as bidirectional RNNs, teacher forcing, and beam search}
\objective{Common failure modes (vanishing/exploding gradients, exposure bias) and mitigation strategies}
\end{learningobjectives}




\input{chapters/chap10-sec01}
\input{chapters/chap10-sec02}
\input{chapters/chap10-sec03}
\input{chapters/chap10-sec04}
\input{chapters/chap10-sec05}
\input{chapters/chap10-sec06}
% \input{chapters/chap10-sec07}
\input{chapters/chap10-real-world-applications}
% \input{chapters/chap10-sec08}

% Chapter summary and problems
\input{chapters/chap10-key-takeaways}
\input{chapters/chap10-exercises}
