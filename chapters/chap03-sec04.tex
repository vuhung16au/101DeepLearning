% Chapter 3, Section 4: Common Probability Distributions

\section{Common Probability Distributions \difficultyInline{beginner}}
\label{sec:common-distributions}

Understanding common probability distributions is essential for deep learning because different distributions model different types of data and uncertainty, from binary outcomes in classification tasks to continuous values in regression problems, and from simple univariate cases to complex multivariate scenarios. These distributions provide the mathematical foundation for modeling the behavior of neural network weights, activation functions, and output predictions, enabling us to make principled decisions about model architecture, loss functions, and regularization strategies. By learning these distributions, we can better understand how to design neural networks that can handle various types of data and uncertainty, from the simple Bernoulli distribution for binary classification to the complex multivariate Gaussian for high-dimensional feature representations.

\subsection{Bernoulli Distribution}

Models a binary random variable (0 or 1):

\begin{equation}
P(X=1) = \phi, \quad P(X=0) = 1-\phi
\end{equation}

Used for binary classification problems.

\subsection{Categorical Distribution}

Generalizes Bernoulli to $k$ discrete outcomes. If $X$ can take values $\{1, 2, \ldots, k\}$:

\begin{equation}
P(X=i) = p_i \quad \text{where} \quad \sum_{i=1}^{k} p_i = 1
\end{equation}

\subsection{Gaussian (Normal) Distribution}

The Gaussian or normal distribution is the most important continuous distribution in deep learning, characterized by its bell-shaped curve and defined by its mean $\mu$ and variance $\sigma^2$. This distribution is particularly important because of the central limit theorem, which states that sums of independent variables approach a Gaussian distribution, making it a natural choice for modeling many real-world phenomena and neural network activations. The Gaussian distribution has several key properties that make it useful in deep learning: it has a single peak at the mean, it's symmetric around the mean, and it has the property that about 68% of the data falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations.

The multivariate Gaussian with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$ is:

\begin{equation}
\mathcal{N}(\vect{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)^n |\boldsymbol{\Sigma}|}} \exp\left(-\frac{1}{2}(\vect{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\vect{x}-\boldsymbol{\mu})\right)
\end{equation}

\subsection{Exponential Distribution}

Models the time between events in a Poisson process:

\begin{equation}
p(x; \lambda) = \lambda e^{-\lambda x} \quad \text{for } x \geq 0
\end{equation}

\subsection{Laplace Distribution}

Heavy-tailed alternative to Gaussian:

\begin{equation}
\text{Laplace}(x; \mu, b) = \frac{1}{2b} \exp\left(-\frac{|x-\mu|}{b}\right)
\end{equation}

Used in robust statistics and L1 regularization.

\subsection{Dirac Delta and Mixture Distributions}

The \textbf{Dirac delta} $\delta(x)$ concentrates all probability at a single point:

\begin{equation}
p(x) = \delta(x - \mu)
\end{equation}

\textbf{Mixture distributions} combine multiple distributions:

\begin{equation}
p(x) = \sum_{i=1}^{k} \alpha_i p_i(x), \quad \sum_{i=1}^{k} \alpha_i = 1
\end{equation}

Example: Gaussian Mixture Model (GMM).
