% Chapter 3, Section 4: Common Probability Distributions

\section{Common Probability Distributions \difficultyInline{beginner}}
\label{sec:common-distributions}

Understanding common probability distributions is essential for deep learning because different distributions model different types of data and uncertainty, from binary outcomes in classification tasks to continuous values in regression problems, and from simple univariate cases to complex multivariate scenarios. These distributions provide the mathematical foundation for modeling the behavior of neural network weights, activation functions, and output predictions, enabling us to make principled decisions about model architecture, loss functions, and regularization strategies. By learning these distributions, we can better understand how to design neural networks that can handle various types of data and uncertainty, from the simple Bernoulli distribution for binary classification to the complex multivariate Gaussian for high-dimensional feature representations.

\subsection{Bernoulli Distribution}

\begin{definition}[Bernoulli Distribution]
The Bernoulli distribution models a binary random variable (0 or 1):
\begin{equation}
P(X=1) = \phi, \quad P(X=0) = 1-\phi
\end{equation}
where $\phi$ is the probability of success (1). Used for binary classification problems.
\end{definition}

\begin{example}
Consider a biased coin that lands heads with probability 0.7. Let $X$ be the random variable representing the outcome:
\begin{itemize}
    \item $X = 1$ if the coin lands heads
    \item $X = 0$ if the coin lands tails
\end{itemize}

The probability mass function is:
\begin{align}
P(X=1) &= 0.7 \quad \text{(probability of heads)} \\
P(X=0) &= 0.3 \quad \text{(probability of tails)}
\end{align}

The expected value is $\mathbb{E}[X] = 1 \cdot 0.7 + 0 \cdot 0.3 = 0.7$, and the variance is $\text{Var}(X) = 0.7 \cdot 0.3 = 0.21$.
\end{example}

\subsection{Categorical Distribution}

\begin{definition}[Categorical Distribution]
The categorical distribution generalizes Bernoulli to $k$ discrete outcomes. If $X$ can take values $\{1, 2, \ldots, k\}$:
\begin{equation}
P(X=i) = p_i \quad \text{where} \quad \sum_{i=1}^{k} p_i = 1
\end{equation}
where $p_i$ is the probability of outcome $i$.
\end{definition}

\subsection{Gaussian (Normal) Distribution}

The Gaussian or normal distribution is the most important continuous distribution in deep learning, characterized by its bell-shaped curve and defined by its mean $\mu$ and variance $\sigma^2$. This distribution is particularly important because of the central limit theorem, which states that sums of independent variables approach a Gaussian distribution, making it a natural choice for modeling many real-world phenomena and neural network activations. The Gaussian distribution has several key properties that make it useful in deep learning: it has a single peak at the mean, it's symmetric around the mean, and it has the property that about 68% of the data falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations.

The multivariate Gaussian with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$ is:

\begin{equation}
\mathcal{N}(\vect{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)^n |\boldsymbol{\Sigma}|}} \exp\left(-\frac{1}{2}(\vect{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\vect{x}-\boldsymbol{\mu})\right)
\end{equation}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ylabel={Probability Density},
    xlabel={Value},
    domain=-4:4,
    width=10cm,
    height=6cm,
    samples=100,
    grid=major,
    grid style={line width=.1pt, draw=gray!10},
    major grid style={line width=.2pt, draw=gray!50}
]
\addplot[bookpurple, thick] {1/sqrt(2*pi) * exp(-x^2/2)};
\node[bookpurple] at (axis cs:0,0.4) {$\mathcal{N}(0, 1)$};
\end{axis}
\end{tikzpicture}
\caption{Standard normal distribution $\mathcal{N}(0,1)$: bell-shaped curve with mean 0, std 1.}
\label{fig:gaussian-distribution}
\end{figure}

\subsection{Exponential Distribution}

\begin{definition}[Exponential Distribution]
The exponential distribution models the time between events in a Poisson process:
\begin{equation}
p(x; \lambda) = \lambda e^{-\lambda x} \quad \text{for } x \geq 0
\end{equation}
where $\lambda > 0$ is the rate parameter.
\end{definition}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ylabel={Probability Density},
    xlabel={Value},
    domain=0:5,
    width=10cm,
    height=6cm,
    samples=100,
    grid=major,
    grid style={line width=.1pt, draw=gray!10},
    major grid style={line width=.2pt, draw=gray!50}
]
\addplot[bookred, thick] {exp(-x)};
\node[bookred] at (axis cs:1,0.3) {$\lambda = 1$};
\end{axis}
\end{tikzpicture}
\caption{Exponential distribution ($\lambda=1$): starts at max, decreases exponentially.}
\label{fig:exponential-distribution}
\end{figure}

\subsection{Laplace Distribution}

\begin{definition}[Laplace Distribution]
The Laplace distribution is a heavy-tailed alternative to Gaussian:
\begin{equation}
\text{Laplace}(x; \mu, b) = \frac{1}{2b} \exp\left(-\frac{|x-\mu|}{b}\right)
\end{equation}
where $\mu$ is the location parameter (mean) and $b$ is the scale parameter (controls the spread).
\end{definition}

The Laplace distribution is used in robust statistics because it's less sensitive to outliers than the Gaussian distribution, and in L1 regularization because its sharp peak at the mean encourages sparsity by penalizing small weights more heavily than the Gaussian distribution.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ylabel={Probability Density},
    xlabel={Value},
    domain=-4:4,
    width=10cm,
    height=6cm,
    samples=100,
    grid=major,
    grid style={line width=.1pt, draw=gray!10},
    major grid style={line width=.2pt, draw=gray!50}
]
\addplot[bookpurple, thick] {0.5 * exp(-abs(x))};
\node[bookpurple] at (axis cs:0,0.4) {$\mu = 0, b = 1$};
\end{axis}
\end{tikzpicture}
\caption{Laplace ($\mu=0, b=1$): sharp peak with exponential tails, more robust to outliers than Gaussian.}
\label{fig:laplace-distribution}
\end{figure}

\subsection{Dirac Delta and Mixture Distributions}

\begin{definition}[Dirac Delta]
The Dirac delta $\delta(x)$ concentrates all probability at a single point:
\begin{equation}
p(x) = \delta(x - \mu)
\end{equation}
where $\mu$ is the location where all probability mass is concentrated.
\end{definition}

\begin{definition}[Mixture Distributions]
Mixture distributions combine multiple distributions:
\begin{equation}
p(x) = \sum_{i=1}^{k} \alpha_i p_i(x), \quad \sum_{i=1}^{k} \alpha_i = 1
\end{equation}
where $\alpha_i$ are the mixing weights and $p_i(x)$ are the component distributions.
\end{definition}

\begin{example}[Gaussian Mixture Model (GMM)]
A Gaussian Mixture Model is a probabilistic model that represents a probability distribution as a weighted sum of multiple Gaussian distributions, allowing it to model complex, multi-modal data that cannot be captured by a single Gaussian. GMMs are particularly useful in clustering, density estimation, and as building blocks for more complex generative models in deep learning.

The probability density function of a GMM with $K$ components is:
\begin{equation}
p(\vect{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\vect{x}; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}

where:
\begin{itemize}
    \item $\pi_k$ are the mixing weights (probabilities) satisfying $\sum_{k=1}^{K} \pi_k = 1$
    \item $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$ are the mean and covariance matrix of the $k$-th Gaussian component
    \item $\mathcal{N}(\vect{x}; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ is the multivariate Gaussian density function
\end{itemize}

The key insight is that each data point $\vect{x}$ can be thought of as being generated by first selecting one of the $K$ Gaussian components according to the mixing weights $\pi_k$, and then sampling from that selected component. This allows the model to capture complex, multi-modal distributions that arise naturally in real-world data, such as images with multiple object classes or speech signals with different phonemes.
\end{example}
