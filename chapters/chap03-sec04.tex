% Chapter 3, Section 4: Common Probability Distributions

\section{Common Probability Distributions \difficultyInline{beginner}}
\label{sec:common-distributions}

Understanding common probability distributions is essential for deep learning because different distributions model different types of data and uncertainty, from binary outcomes in classification tasks to continuous values in regression problems, and from simple univariate cases to complex multivariate scenarios. These distributions provide the mathematical foundation for modeling the behavior of neural network weights, activation functions, and output predictions, enabling us to make principled decisions about model architecture, loss functions, and regularization strategies. By learning these distributions, we can better understand how to design neural networks that can handle various types of data and uncertainty, from the simple Bernoulli distribution for binary classification to the complex multivariate Gaussian for high-dimensional feature representations.

\subsection{Bernoulli Distribution}

Models a binary random variable (0 or 1):

\begin{equation}
P(X=1) = \phi, \quad P(X=0) = 1-\phi
\end{equation}

Used for binary classification problems.

\begin{example}
Consider a biased coin that lands heads with probability 0.7. Let $X$ be the random variable representing the outcome:
\begin{itemize}
    \item $X = 1$ if the coin lands heads
    \item $X = 0$ if the coin lands tails
\end{itemize}

The probability mass function is:
\begin{align}
P(X=1) &= 0.7 \quad \text{(probability of heads)} \\
P(X=0) &= 0.3 \quad \text{(probability of tails)}
\end{align}

The expected value is $\mathbb{E}[X] = 1 \cdot 0.7 + 0 \cdot 0.3 = 0.7$, and the variance is $\text{Var}(X) = 0.7 \cdot 0.3 = 0.21$.
\end{example}

\subsection{Categorical Distribution}

Generalizes Bernoulli to $k$ discrete outcomes. If $X$ can take values $\{1, 2, \ldots, k\}$:

\begin{equation}
P(X=i) = p_i \quad \text{where} \quad \sum_{i=1}^{k} p_i = 1
\end{equation}

\subsection{Gaussian (Normal) Distribution}

The Gaussian or normal distribution is the most important continuous distribution in deep learning, characterized by its bell-shaped curve and defined by its mean $\mu$ and variance $\sigma^2$. This distribution is particularly important because of the central limit theorem, which states that sums of independent variables approach a Gaussian distribution, making it a natural choice for modeling many real-world phenomena and neural network activations. The Gaussian distribution has several key properties that make it useful in deep learning: it has a single peak at the mean, it's symmetric around the mean, and it has the property that about 68% of the data falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations.

The multivariate Gaussian with mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$ is:

\begin{equation}
\mathcal{N}(\vect{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)^n |\boldsymbol{\Sigma}|}} \exp\left(-\frac{1}{2}(\vect{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\vect{x}-\boldsymbol{\mu})\right)
\end{equation}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ylabel={Probability Density},
    xlabel={Value},
    domain=-4:4,
    width=10cm,
    height=6cm,
    samples=100,
    grid=major,
    grid style={line width=.1pt, draw=gray!10},
    major grid style={line width=.2pt, draw=gray!50}
]
\addplot[bookpurple, thick] {1/sqrt(2*pi) * exp(-x^2/2)};
\node[bookpurple] at (axis cs:0,0.4) {$\mathcal{N}(0, 1)$};
\end{axis}
\end{tikzpicture}
\caption{Gaussian (Normal) distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. This is the standard normal distribution, which is commonly used as a reference in statistics and machine learning.}
\label{fig:gaussian-distribution}
\end{figure}

\subsection{Exponential Distribution}

Models the time between events in a Poisson process:

\begin{equation}
p(x; \lambda) = \lambda e^{-\lambda x} \quad \text{for } x \geq 0
\end{equation}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ylabel={Probability Density},
    xlabel={Value},
    domain=0:5,
    width=10cm,
    height=6cm,
    samples=100,
    grid=major,
    grid style={line width=.1pt, draw=gray!10},
    major grid style={line width=.2pt, draw=gray!50}
]
\addplot[bookred, thick] {exp(-x)};
\node[bookred] at (axis cs:1,0.3) {$\lambda = 1$};
\end{axis}
\end{tikzpicture}
\caption{Exponential distribution with rate parameter $\lambda = 1$. The distribution starts at its maximum value and decreases exponentially.}
\label{fig:exponential-distribution}
\end{figure}

\subsection{Laplace Distribution}

Heavy-tailed alternative to Gaussian:

\begin{equation}
\text{Laplace}(x; \mu, b) = \frac{1}{2b} \exp\left(-\frac{|x-\mu|}{b}\right)
\end{equation}

Used in robust statistics and L1 regularization.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ylabel={Probability Density},
    xlabel={Value},
    domain=-4:4,
    width=10cm,
    height=6cm,
    samples=100,
    grid=major,
    grid style={line width=.1pt, draw=gray!10},
    major grid style={line width=.2pt, draw=gray!50}
]
\addplot[bookpurple, thick] {0.5 * exp(-abs(x))};
\node[bookpurple] at (axis cs:0,0.4) {$\mu = 0, b = 1$};
\end{axis}
\end{tikzpicture}
\caption{Laplace distribution with location parameter $\mu = 0$ and scale parameter $b = 1$. The distribution has a sharp peak at the mean and exponential tails, making it more robust to outliers than the Gaussian distribution.}
\label{fig:laplace-distribution}
\end{figure}

\subsection{Dirac Delta and Mixture Distributions}

The \textbf{Dirac delta} $\delta(x)$ concentrates all probability at a single point:

\begin{equation}
p(x) = \delta(x - \mu)
\end{equation}

\textbf{Mixture distributions} combine multiple distributions:

\begin{equation}
p(x) = \sum_{i=1}^{k} \alpha_i p_i(x), \quad \sum_{i=1}^{k} \alpha_i = 1
\end{equation}

\begin{example}[Gaussian Mixture Model (GMM)]
A Gaussian Mixture Model is a probabilistic model that represents a probability distribution as a weighted sum of multiple Gaussian distributions, allowing it to model complex, multi-modal data that cannot be captured by a single Gaussian. GMMs are particularly useful in clustering, density estimation, and as building blocks for more complex generative models in deep learning.

The probability density function of a GMM with $K$ components is:
\begin{equation}
p(\vect{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\vect{x}; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}

where:
\begin{itemize}
    \item $\pi_k$ are the mixing weights (probabilities) satisfying $\sum_{k=1}^{K} \pi_k = 1$
    \item $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$ are the mean and covariance matrix of the $k$-th Gaussian component
    \item $\mathcal{N}(\vect{x}; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ is the multivariate Gaussian density function
\end{itemize}

The key insight is that each data point $\vect{x}$ can be thought of as being generated by first selecting one of the $K$ Gaussian components according to the mixing weights $\pi_k$, and then sampling from that selected component. This allows the model to capture complex, multi-modal distributions that arise naturally in real-world data, such as images with multiple object classes or speech signals with different phonemes.
\end{example}
