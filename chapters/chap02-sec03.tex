% Chapter 2, Section 3: Identity and Inverse Matrices

\section{Identity and Inverse Matrices \difficultyInline{beginner}}
\label{sec:identity-inverse}

Special matrices play important roles in linear algebra and deep learning. The identity matrix and matrix inverses are among the most fundamental.

\subsection{Identity Matrix}

The identity matrix is the matrix analog of the number 1.

\begin{definition}[Identity Matrix]
The identity matrix $\mat{I}_n \in \mathbb{R}^{n \times n}$ is a square matrix with ones on the diagonal and zeros elsewhere:
\begin{equation}
    \mat{I}_n = \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1
    \end{bmatrix}
\end{equation}
Formally, $(\mat{I}_n)_{ij} = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta:
\begin{equation}
    \delta_{ij} = \begin{cases}
        1 & \text{if } i = j \\
        0 & \text{if } i \neq j
    \end{cases}
\end{equation}
\end{definition}

The key property of the identity matrix is:
\begin{equation}
    \mat{I}_n\mat{A} = \mat{A}\mat{I}_n = \mat{A}
\end{equation}
for any matrix $\mat{A} \in \mathbb{R}^{n \times n}$.

\subsection{Matrix Inverse}

The inverse of a matrix, when it exists, allows us to solve systems of linear equations.

\begin{definition}[Matrix Inverse]
A square matrix $\mat{A} \in \mathbb{R}^{n \times n}$ is \emph{invertible} (or \emph{non-singular}) if there exists a matrix $\mat{A}^{-1} \in \mathbb{R}^{n \times n}$ such that:
\begin{equation}
    \mat{A}^{-1}\mat{A} = \mat{A}\mat{A}^{-1} = \mat{I}_n
\end{equation}
\end{definition}

\begin{example}
The matrix $\mat{A} = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}$ has inverse:
\begin{equation}
    \mat{A}^{-1} = \begin{bmatrix} 1 & -1 \\ -1 & 2 \end{bmatrix}
\end{equation}
We can verify: $\mat{A}\mat{A}^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \mat{I}_2$.
\end{example}

\subsection{Properties of Inverses}

If $\mat{A}$ and $\mat{B}$ are invertible, then:

\begin{align}
    (\mat{A}^{-1})^{-1} &= \mat{A} \\
    (\mat{AB})^{-1} &= \mat{B}^{-1}\mat{A}^{-1} \\
    (\mat{A}\transpose)^{-1} &= (\mat{A}^{-1})\transpose
\end{align}

\subsection{Solving Linear Systems}

The inverse allows us to solve systems of linear equations. Given $\mat{A}\vect{x} = \vect{b}$, if $\mat{A}$ is invertible:
\begin{equation}
    \vect{x} = \mat{A}^{-1}\vect{b}
\end{equation}

However, computing inverses is expensive ($O(n^3)$ for dense matrices) and numerically unstable. In practice, we often use more efficient methods like LU decomposition or iterative solvers.

\subsection{Conditions for Invertibility}

A matrix $\mat{A} \in \mathbb{R}^{n \times n}$ is invertible if and only if it satisfies several equivalent conditions that are fundamental to understanding when linear transformations can be reversed. The determinant condition $\det(\mat{A}) \neq 0$ provides a direct computational test for invertibility, as the determinant captures the volume scaling factor of the linear transformation and a zero determinant indicates that the transformation collapses the space to a lower dimension. The linear independence condition requires that both the columns and rows of the matrix are linearly independent, meaning that no column or row can be expressed as a linear combination of the others, which ensures that the transformation preserves the full dimensionality of the space. The rank condition $\text{rank}(\mat{A}) = n$ ensures that the matrix has full rank, meaning that all $n$ dimensions of the input space are preserved in the output space, which is necessary for the transformation to be reversible. The null space condition requires that the null space contains only the zero vector, meaning that the only vector that maps to zero is the zero vector itself, which ensures that the transformation is one-to-one and therefore invertible.

\subsection{Singular Matrices}

Matrices that are not invertible are called \emph{singular} or \emph{degenerate}.

\begin{example}
The matrix $\mat{A} = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}$ is singular because its rows are linearly dependent (the second row is twice the first). Its determinant is $\det(\mat{A}) = 4 - 4 = 0$.
\end{example}

Singular matrices arise in deep learning when several problematic conditions occur that can significantly impact model performance and training stability. Features that are perfectly correlated create linear dependencies in the data matrix, leading to singular matrices that cannot be inverted and causing numerical instability in optimization algorithms. Overparameterized models with more parameters than training examples can lead to singular weight matrices, particularly in the early layers where the model has not yet learned meaningful representations and the weight matrices may have insufficient rank. Numerical precision issues occur when the computational precision of floating-point arithmetic is insufficient to distinguish between nearly identical values, causing matrices that should be invertible to become singular due to rounding errors, which is particularly problematic in deep networks where small errors can accumulate and propagate through multiple layers.

\subsection{Pseudo-inverse}

For non-square or singular matrices, we can use the Moore-Penrose pseudo-inverse $\mat{A}^+$, which provides a generalized notion of inversion. The pseudo-inverse is particularly useful in least squares problems and is discussed further in later chapters.

\subsection{Practical Considerations}

In deep learning implementations, several practical considerations are essential for maintaining numerical stability and computational efficiency when working with matrix inverses and related operations. Avoiding explicit computation of matrix inverses whenever possible is crucial, as direct inversion is computationally expensive with $O(n^3)$ complexity and can be numerically unstable, particularly for ill-conditioned matrices that are common in deep learning applications. Using numerically stable algorithms such as QR decomposition and singular value decomposition (SVD) provides more robust alternatives to direct inversion, as these methods are designed to handle numerical precision issues and can provide meaningful results even when matrices are nearly singular. Adding regularization terms such as $(\mat{A}\transpose\mat{A} + \lambda\mat{I})^{-1}$ helps ensure invertibility by adding a small positive value to the diagonal elements, which prevents singular matrices and improves numerical stability while maintaining the mathematical properties of the original problem. Leveraging optimized linear algebra libraries such as BLAS, LAPACK, and cuBLAS is essential for achieving high performance in deep learning applications, as these libraries are highly optimized for specific hardware architectures and can provide significant speedups over naive implementations, particularly when working with large matrices on GPU hardware.
