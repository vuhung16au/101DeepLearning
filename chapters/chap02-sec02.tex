% Chapter 2, Section 2: Matrix Operations

\section{Matrix Operations \difficultyInline{beginner}}
\label{sec:matrix-operations}

Matrix operations form the computational backbone of neural networks. Understanding these operations is crucial for implementing and analyzing deep learning algorithms.

\subsection{Matrix Addition and Scalar Multiplication}

Matrices of the same dimensions can be added element-wise:

\begin{definition}[Matrix Addition]
Given $\mat{A}, \mat{B} \in \mathbb{R}^{m \times n}$, their sum $\mat{C} = \mat{A} + \mat{B}$ is defined as:
\begin{equation}
    C_{ij} = A_{ij} + B_{ij}
\end{equation}
for all $i = 1, \ldots, m$ and $j = 1, \ldots, n$.
\end{definition}

\begin{definition}[Scalar Multiplication]
Given a scalar $\alpha \in \mathbb{R}$ and a matrix $\mat{A} \in \mathbb{R}^{m \times n}$, the product $\mat{B} = \alpha\mat{A}$ is:
\begin{equation}
    B_{ij} = \alpha A_{ij}
\end{equation}
\end{definition}

\subsection{Matrix Transpose}

The transpose is a fundamental operation that exchanges rows and columns.

\begin{definition}[Transpose]
The transpose of a matrix $\mat{A} \in \mathbb{R}^{m \times n}$ is a matrix $\mat{A}\transpose \in \mathbb{R}^{n \times m}$ where:
\begin{equation}
    (\mat{A}\transpose)_{ij} = A_{ji}
\end{equation}
\end{definition}

Properties of transpose:
\begin{align}
    (\mat{A}\transpose)\transpose &= \mat{A} \\
    (\mat{A} + \mat{B})\transpose &= \mat{A}\transpose + \mat{B}\transpose \\
    (\alpha\mat{A})\transpose &= \alpha\mat{A}\transpose \\
    (\mat{AB})\transpose &= \mat{B}\transpose\mat{A}\transpose
\end{align}

\subsection{Matrix Multiplication}

Matrix multiplication is central to neural network computations.

\begin{definition}[Matrix Multiplication]
Given $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{n \times p}$, their product $\mat{C} = \mat{AB} \in \mathbb{R}^{m \times p}$ is defined as:
\begin{equation}
    C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj}
\end{equation}
\end{definition}

\begin{example}
Consider the multiplication:
\begin{equation}
    \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}
    \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}
    = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}
\end{equation}
where $C_{11} = 1 \cdot 5 + 2 \cdot 7 = 19$.
\end{example}

Matrix multiplication exhibits several important properties that are fundamental to understanding neural network computations. The associative property $(\mat{AB})\mat{C} = \mat{A}(\mat{BC})$ allows us to group matrix multiplications in different ways without changing the result, which is crucial for optimizing the order of operations in deep neural networks and enables efficient computation strategies. The distributive property $\mat{A}(\mat{B} + \mat{C}) = \mat{AB} + \mat{AC}$ allows us to break down complex matrix operations into simpler components, which is particularly useful in gradient computation and backpropagation algorithms where we need to compute derivatives of matrix expressions. Unlike scalar multiplication, matrix multiplication is generally not commutative, meaning that $\mat{AB} \neq \mat{BA}$ in most cases, which has important implications for the order of operations in neural networks and explains why the order of matrix multiplications in forward and backward passes must be carefully considered.

\subsection{Element-wise (Hadamard) Product}

The element-wise product is denoted by $\odot$ and operates on corresponding elements.

\begin{definition}[Hadamard Product]
Given $\mat{A}, \mat{B} \in \mathbb{R}^{m \times n}$, the Hadamard product $\mat{C} = \mat{A} \odot \mat{B}$ is:
\begin{equation}
    C_{ij} = A_{ij}B_{ij}
\end{equation}
\end{definition}

This operation is common in neural networks, particularly in activation functions and gating mechanisms.

\subsection{Matrix-Vector Products}

When multiplying a matrix by a vector, we can view it as a special case of matrix multiplication:

\begin{equation}
    \mat{A}\vect{x} = \vect{b}
\end{equation}

where $\mat{A} \in \mathbb{R}^{m \times n}$, $\vect{x} \in \mathbb{R}^n$, and $\vect{b} \in \mathbb{R}^m$.

This operation is fundamental in neural networks, where it represents the linear transformation:
\begin{equation}
    b_i = \sum_{j=1}^{n} A_{ij}x_j
\end{equation}

\begin{example}
Let's compute a matrix-vector product using the numbers 11 and 16:

\begin{equation}
    \begin{bmatrix} 11 & 3 \\ 2 & 16 \end{bmatrix}
    \begin{bmatrix} 4 \\ 5 \end{bmatrix}
    = \begin{bmatrix} ? \\ ? \end{bmatrix}
\end{equation}

Computing each element:
\begin{align}
    b_1 &= A_{11}x_1 + A_{12}x_2 = 11 \cdot 4 + 3 \cdot 5 = 44 + 15 = 59 \\
    b_2 &= A_{21}x_1 + A_{22}x_2 = 2 \cdot 4 + 16 \cdot 5 = 8 + 80 = 88
\end{align}

So the result is:
\begin{equation}
    \begin{bmatrix} 11 & 3 \\ 2 & 16 \end{bmatrix}
    \begin{bmatrix} 4 \\ 5 \end{bmatrix}
    = \begin{bmatrix} 59 \\ 88 \end{bmatrix}
\end{equation}
\end{example}

\subsection{Dot Product}

The dot product (or inner product) of two vectors is a special case of matrix multiplication:

\begin{definition}[Dot Product]
For vectors $\vect{x}, \vect{y} \in \mathbb{R}^n$, their dot product is:
\begin{equation}
    \vect{x} \cdot \vect{y} = \vect{x}\transpose\vect{y} = \sum_{i=1}^{n} x_i y_i
\end{equation}
\end{definition}

The dot product has geometric interpretation:
\begin{equation}
    \vect{x} \cdot \vect{y} = \norm{\vect{x}} \norm{\vect{y}} \cos\theta
\end{equation}
where $\theta$ is the angle between the vectors.

\subsubsection{Cosine Similarity}

From the geometric interpretation, we can derive the cosine similarity, which measures the angle between two vectors regardless of their magnitude:

\begin{equation}
    \cos\theta = \frac{\vect{x} \cdot \vect{y}}{\norm{\vect{x}} \norm{\vect{y}}}
\end{equation}

Cosine similarity ranges from -1 to 1:
\begin{itemize}
    \item $\cos\theta = 1$: Vectors point in the same direction (perfectly similar)
    \item $\cos\theta = 0$: Vectors are perpendicular (no similarity)
    \item $\cos\theta = -1$: Vectors point in opposite directions (perfectly dissimilar)
\end{itemize}

Cosine similarity is particularly useful in machine learning for measuring similarity between feature vectors, as it focuses on the direction rather than the magnitude of the vectors. This makes it robust to differences in scale between features.

\subsection{Computational Complexity}

Understanding computational costs is crucial for efficient implementation of deep learning algorithms, as the computational complexity of matrix operations directly impacts training time, memory requirements, and the feasibility of deploying models in production environments. Matrix-matrix multiplication between $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{n \times p}$ requires $O(mnp)$ operations, making it the most computationally expensive operation in neural networks and the primary bottleneck in training large models. Matrix-vector multiplication requires $O(mn)$ operations, which is significantly more efficient than full matrix multiplication and is commonly used in forward passes through neural network layers. Element-wise operations such as activation functions and Hadamard products require $O(mn)$ operations, making them relatively inexpensive compared to matrix multiplications but still important for overall computational efficiency. These operations can be efficiently parallelized on modern hardware such as GPUs and TPUs, which is one of the key reasons why deep learning has become practical for large-scale applications, as the parallel nature of matrix operations maps well to the parallel processing capabilities of specialized hardware.

\begin{remark}[Matrix Operations on Different Hardware]
Matrix operations are handled very differently across CPU, GPU, and TPU architectures, each offering distinct advantages for deep learning workloads. CPUs process matrix operations sequentially using general-purpose cores, making them suitable for small-scale computations and development, but they struggle with the massive parallelization requirements of large neural networks. GPUs excel at matrix operations through their thousands of small, efficient cores designed for parallel processing, providing 10-100x speedups over CPUs for matrix multiplications and enabling the training of deep networks that would be impractical on CPUs alone. TPUs take this specialization even further, using systolic array architectures specifically optimized for tensor contractions and matrix multiplications, delivering superior performance for large-scale training with lower power consumption than GPUs. The choice between these platforms depends on the scale of computation: CPUs for prototyping and small models, GPUs for most deep learning applications, and TPUs for training the largest models where computational efficiency and power consumption are critical factors.
\end{remark}
