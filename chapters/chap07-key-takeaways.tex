% Key Takeaways for Chapter 7

\section*{Key Takeaways}
\addcontentsline{toc}{section}{Key Takeaways}

\begin{keytakeaways}
\begin{itemize}[leftmargin=2em]
    \item \textbf{Regularisation} constrains model capacity to improve generalisation and prevent overfitting to training data.
    \item \textbf{Norm penalties} (L1, L2) encourage simpler models; L1 induces sparsity whilst L2 shrinks weights uniformly.
    \item \textbf{Data augmentation} artificially expands training sets by applying semantics-preserving transformations.
    \item \textbf{Dropout} randomly drops units during training, forcing redundant representations and reducing co-adaptation.
    \item \textbf{Early stopping and batch normalisation} are simple yet powerful techniques for better training dynamics and generalisation.
\end{itemize}
\end{keytakeaways}


