% Chapter 7, Section 1

\section{Parameter Norm Penalties \difficultyInline{intermediate}}
\label{sec:parameter-penalties}

\textbf{Parameter norm penalties} constrain model capacity by penalizing large weights.

\subsection{Intuition: Shrinking the Model's "Complexity"}

Think of a model as a musical band with many instruments (parameters). If every instrument plays loudly (large weights), the result can be noisy and overfit to the training song. Norm penalties are like asking the band to lower the volume uniformly (L2) or mute many instruments entirely (L1) so the melody (true signal) stands out. This discourages memorization and encourages simpler patterns that generalize.

\subsection{L2 Regularization (Weight Decay)}

Add squared L2 norm of weights to the loss:

\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \frac{\lambda}{2} \|\vect{w}\|^2
\end{equation}

Gradient update becomes:
\begin{equation}
\vect{w} \leftarrow (1 - \alpha\lambda)\vect{w} - \alpha \nabla_{\vect{w}} L
\end{equation}

The factor $(1 - \alpha\lambda)$ causes "weight decay."

\subsection{L1 Regularization}

L1 regularization adds the L1 norm of weights to the loss function: $\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \lambda \|\vect{w}\|_1$. L1 regularization promotes sparsity by driving many weights to become exactly zero, making it useful for feature selection where only the most important features are retained. The gradient of L1 regularization is $\text{sign}(\vect{w})$, which provides a constant magnitude update that encourages weights to move toward zero, effectively performing automatic feature selection by eliminating less important parameters.

% Index entries
\index{regularization!L2}
\index{regularization!L1}
\index{elastic net}

\subsection{Elastic Net}

Combines L1 and L2:
\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \lambda_1 \|\vect{w}\|_1 + \lambda_2 \|\vect{w}\|^2
\end{equation}

Why combine them? The L1 term promotes sparsity, selecting a compact set of features, while the L2 term stabilizes the solution by shrinking correlated coefficients together rather than arbitrarily picking one. In high-dimensional settings with collinearity, pure L1 can be unstable; the added L2 induces grouping and reduces variance, yielding models that are both interpretable (sparse) and numerically well-conditioned.

