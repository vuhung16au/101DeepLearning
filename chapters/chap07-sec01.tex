% Chapter 7, Section 1

\section{Parameter Norm Penalties \difficultyInline{intermediate}}
\label{sec:parameter-penalties}

\textbf{Parameter norm penalties} constrain model capacity by penalizing large weights.

\subsection{Intuition: Shrinking the Model's "Complexity"}

Think of a model as a musical band with many instruments (parameters). If every instrument plays loudly (large weights), the result can be noisy and overfit to the training song. Norm penalties are like asking the band to lower the volume uniformly (L2) or mute many instruments entirely (L1) so the melody (true signal) stands out. This discourages memorization and encourages simpler patterns that generalize.

\subsection{L2 Regularization (Weight Decay)}

Add squared L2 norm of weights to the loss:

\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \frac{\lambda}{2} \|\vect{w}\|^2
\end{equation}

Gradient update becomes:
\begin{equation}
\vect{w} \leftarrow (1 - \alpha\lambda)\vect{w} - \alpha \nabla_{\vect{w}} L
\end{equation}

The factor $(1 - \alpha\lambda)$ causes "weight decay."

\subsection{L1 Regularization}

Add L1 norm:
\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \lambda \|\vect{w}\|_1
\end{equation}

L1 regularization:
\begin{itemize}
    \item Promotes sparsity (many weights become exactly zero)
    \item Useful for feature selection
    \item Gradient: $\text{sign}(\vect{w})$
\end{itemize}

% Index entries
\index{regularization!L2}
\index{regularization!L1}
\index{elastic net}

\subsection{Elastic Net}

Combines L1 and L2:
\begin{equation}
\tilde{L}(\vect{\theta}) = L(\vect{\theta}) + \lambda_1 \|\vect{w}\|_1 + \lambda_2 \|\vect{w}\|^2
\end{equation}

