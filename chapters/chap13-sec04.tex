% Chapter 13, Section 4

\section{Sparse Coding \difficultyInline{intermediate}}
\label{sec:sparse-coding}

Sparse coding learns an overcomplete dictionary where data has sparse representation through the optimization problem $\min_{\mat{D}, \vect{z}} \|\vect{x} - \mat{D}\vect{z}\|^2 + \lambda \|\vect{z}\|_1$, where the first term ensures accurate reconstruction and the second term promotes sparsity in the representation. This formulation is particularly powerful because it can learn meaningful features from data by finding a dictionary that allows each data point to be represented as a sparse combination of dictionary elements, where the sparsity constraint forces the model to identify the most important and distinctive features. Sparse coding has found widespread applications in image denoising where it can separate noise from signal by learning sparse representations, in feature learning where it discovers meaningful patterns in data, and in compression where sparse representations can significantly reduce storage requirements while maintaining reconstruction quality.

\subsection{Optimization and interpretation}

The $\ell_1$ penalty promotes sparsity, yielding parts-based representations and robust denoising. Alternating minimization over dictionary $\mat{D}$ and codes $\vect{z}$ is common; convolutional variants are used in images \textcite{GoodfellowEtAl2016}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,height=0.36\textwidth,
      xlabel={$z_1$}, ylabel={$z_2$}, grid=both]
      \addplot[bookpurple,very thick,domain=-2:2]{1.0 - abs(x)}; % diamond-like L1 contour (schematic)
    \end{axis}
  \end{tikzpicture}
  \caption{Schematic illustrating $\ell_1$-induced sparsity geometry.}
  \label{fig:l1-geometry}
\end{figure}
