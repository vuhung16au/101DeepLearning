% Chapter 13, Section 4

\section{Sparse Coding \difficultyInline{intermediate}}
\label{sec:sparse-coding}

Sparse coding learns an overcomplete dictionary where data has sparse representation through the optimization problem
\begin{equation}
\min_{\mat{D},\, \vect{z}}\; \underbrace{\lVert \vect{x} - \mat{D}\vect{z} \rVert^2}_{\text{reconstruction error}} + \lambda\, \underbrace{\lVert \vect{z} \rVert_1}_{\text{sparsity penalty}}.
\end{equation}
\noindent\textbf{Explanation.} The objective trades off fidelity to the data (small reconstruction error) with sparsity of the code $\vect{z}$ (few nonzeros), controlled by $\lambda>0$. This encourages parts-based, interpretable representations.

\subsection{Optimization and interpretation}

The $\ell_1$ penalty promotes sparsity, yielding parts-based representations and robust denoising. Alternating minimization over dictionary $\mat{D}$ and codes $\vect{z}$ is common; convolutional variants are used in images \textcite{GoodfellowEtAl2016}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,height=0.36\textwidth,
      xlabel={$z_1$}, ylabel={$z_2$}, grid=both]
      \addplot[bookpurple,very thick,domain=-2:2]{1.0 - abs(x)}; % diamond-like L1 contour (schematic)
    \end{axis}
  \end{tikzpicture}
  \caption{Schematic illustrating $\ell_1$-induced sparsity geometry.}
  \label{fig:l1-geometry}
\end{figure}
