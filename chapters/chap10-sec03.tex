% Chapter 10, Section 3

\section{Long Short-Term Memory (LSTM) \difficultyInline{intermediate}}
\label{sec:lstm}

\subsection*{Intuition}

The LSTM adds a highway for information (the cell state) that can pass signals forward with minimal modification. Gates act like valves to forget unhelpful information, write new content, and reveal outputs, which preserves gradients over long spans \cite{Hochreiter1997,GoodfellowEtAl2016}.

\subsection*{Historical Context}

Introduced in the 1990s to address vanishing gradients \cite{Hochreiter1997}, LSTMs unlocked practical sequence learning across speech, language, and time-series tasks before attention-based Transformers became dominant \cite{Vaswani2017}.

% Index and glossary
\index{long short-term memory}
\glsadd{long-short-term-memory}

\subsection{Architecture}

LSTM uses \textbf{gating mechanisms} to control information flow and maintain a persistent cell state that supports long-range credit assignment \cite{Hochreiter1997,GoodfellowEtAl2016}:

\begin{align}
\vect{f}_t &= \sigma(\mat{W}_f [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_f) \quad \text{(forget gate)} \\
\vect{i}_t &= \sigma(\mat{W}_i [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_i) \quad \text{(input gate)} \\
\tilde{\vect{c}}_t &= \tanh(\mat{W}_c [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_c) \quad \text{(candidate)} \\
\vect{c}_t &= \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t \quad \text{(cell state)} \\
\vect{o}_t &= \sigma(\mat{W}_o [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_o) \quad \text{(output gate)} \\
\vect{h}_t &= \vect{o}_t \odot \tanh(\vect{c}_t) \quad \text{(hidden state)}
\end{align}

\subsection{Key Ideas}

\textbf{Cell state} $\vect{c}_t$: Long-term memory
\begin{itemize}
    \item Information flows with minimal transformation
    \item Gates control what to remember/forget
\end{itemize}

\textbf{Forget gate} $\vect{f}_t$: Decides what to discard from cell state

\textbf{Input gate} $\vect{i}_t$: Decides what new information to store

\textbf{Output gate} $\vect{o}_t$: Decides what to output

\subsection{Advantages}

\begin{itemize}
    \item Addresses vanishing gradient problem
    \item Can learn long-term dependencies
    \item Gradients flow more easily through cell state
    \item Widely used for sequential tasks

\paragraph{Visual aid.} A compact LSTM cell diagram:
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=stealth, node distance=1.8cm]
        \tikzstyle{gate}=[draw, rounded corners, minimum width=1.5cm, minimum height=0.9cm]
        \node[gate] (ft) {$\vect{f}_t$};
        \node[gate, below of=ft] (it) {$\vect{i}_t$};
        \node[gate, below of=it] (ot) {$\vect{o}_t$};
        \node[right of=it, xshift=2.2cm] (ct1) {$\vect{c}_{t-1}$};
        \node[right of=ct1, xshift=1.2cm] (sum) {$+$};
        \node[right of=sum, xshift=1.2cm] (ct) {$\vect{c}_t$};
        \draw[->] (ct1) -- node[midway, above] {$\odot\,\vect{f}_t$} (sum);
        \draw[->] (it) -- node[midway, above] {$\odot\,\tilde{\vect{c}}_t$} (sum);
        \draw[->] (sum) -- (ct);
        \draw[->] (ot) |- ++(1.2,1.8) node[pos=0.55, right] {$\odot\,\tanh(\vect{c}_t)$} -| ++(0,0);
    \end{tikzpicture}
    \caption{Illustrative LSTM flow with forget, input, and output gates.}
\end{figure}
\end{itemize}

