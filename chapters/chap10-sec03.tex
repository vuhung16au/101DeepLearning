% Chapter 10, Section 3

\section{Long Short-Term Memory (LSTM) \difficultyInline{intermediate}}
\label{sec:lstm}

\subsection*{Intuition}

The LSTM adds a highway for information (the cell state) that can pass signals forward with minimal modification. Gates act like valves to forget unhelpful information, write new content, and reveal outputs, which preserves gradients over long spans \cite{Hochreiter1997,GoodfellowEtAl2016}.

LSTM (Long Short-Term Memory) is a type of recurrent neural network architecture designed to solve the vanishing gradient problem by using gating mechanisms to control information flow and maintain long-term memory through a dedicated cell state.

\begin{remark}[Historical Context of LSTM]
Introduced in the 1990s to address vanishing gradients \cite{Hochreiter1997}, LSTMs unlocked practical sequence learning across speech, language, and time-series tasks before attention-based Transformers became dominant \cite{Vaswani2017}.
\end{remark}

% Index and glossary
\index{long short-term memory}
\glsadd{long-short-term-memory}

\subsection{Architecture}

\begin{remark}[Gates in LSTM]
Gates in LSTM are learnable mechanisms that control information flow by deciding what information to forget, what new information to store, and what to output, enabling the network to selectively maintain or discard information over long time spans.
\end{remark}

\begin{remark}[Recurrence and Hidden State - The Memory]
The hidden state in LSTM serves as the network's working memory, carrying forward processed information from previous time steps while the cell state acts as long-term memory that can store information across extended sequences, with gates controlling how information flows between these memory components.
\end{remark}

LSTM uses \textbf{gating mechanisms} to control information flow and maintain a persistent cell state that supports long-range credit assignment \cite{Hochreiter1997,GoodfellowEtAl2016}. Gating mechanisms are essential because they allow the network to learn when to remember, forget, or output information, solving the fundamental problem of how to maintain useful information over long sequences while discarding irrelevant details.

\begin{align}
\vect{f}_t &= \sigma(\mat{W}_f [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_f) \quad \text{(forget gate)} \\
\vect{i}_t &= \sigma(\mat{W}_i [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_i) \quad \text{(input gate)} \\
\tilde{\vect{c}}_t &= \tanh(\mat{W}_c [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_c) \quad \text{(candidate)} \\
\vect{c}_t &= \vect{f}_t \odot \vect{c}_{t-1} + \vect{i}_t \odot \tilde{\vect{c}}_t \quad \text{(cell state)} \\
\vect{o}_t &= \sigma(\mat{W}_o [\vect{h}_{t-1}, \vect{x}_t] + \vect{b}_o) \quad \text{(output gate)} \\
\vect{h}_t &= \vect{o}_t \odot \tanh(\vect{c}_t) \quad \text{(hidden state)}
\end{align}

\subsection{Key Ideas}

The LSTM's key innovations address fundamental limitations of traditional RNNs and feedforward networks. Unlike standard RNNs where information must pass through repeated nonlinear transformations that cause gradient decay, the LSTM introduces a dedicated \textbf{cell state} $\vect{c}_t$ that acts as a highway for information flow with minimal transformation, enabling gradients to flow directly across long time spans. The \textbf{gating mechanisms} (forget, input, and output gates) provide selective control over information flow, allowing the network to learn when to remember, forget, and output informationâ€”a capability that was impossible with fixed-weight feedforward networks or vanilla RNNs. This selective memory management solves the vanishing gradient problem by creating direct paths for gradient flow while maintaining the ability to learn complex temporal dependencies that exceed the capacity of traditional sequence models.

\textbf{Cell state} $\vect{c}_t$: Long-term memory
\begin{itemize}
    \item Information flows with minimal transformation
    \item Gates control what to remember/forget
\end{itemize}

\textbf{Forget gate} $\vect{f}_t$: Decides what to discard from cell state

\textbf{Input gate} $\vect{i}_t$: Decides what new information to store

\textbf{Output gate} $\vect{o}_t$: Decides what to output

\subsection{Advantages}

LSTMs address the vanishing gradient problem through their innovative cell state and gating mechanisms, where the cell state acts as a memory highway allowing gradients to flow relatively unimpeded across many time steps, preventing them from shrinking to zero. Unlike vanilla RNNs where gradients must pass through repeated nonlinear transformations, LSTMs provide direct paths for gradient flow, creating a more stable training environment. The architecture's ability to learn long-term dependencies stems from the forget and input gates that explicitly control what information is retained or discarded from the cell state, allowing LSTMs to store relevant information for extended periods and access it when needed, effectively capturing dependencies across hundreds of time steps without degradation. This selective memory mechanism, combined with the linear-like flow through the cell state path involving simple additions and multiplications by gate activations, has made LSTMs a de facto standard for various sequential data processing tasks, with robust performance across diverse applications from natural language processing to speech recognition.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=stealth, node distance=1.8cm]
        \tikzstyle{gate}=[draw, rounded corners, minimum width=1.5cm, minimum height=0.9cm]
        \node[gate] (ft) {$\vect{f}_t$};
        \node[gate, below of=ft] (it) {$\vect{i}_t$};
        \node[gate, below of=it] (ot) {$\vect{o}_t$};
        \node[right of=it, xshift=2.2cm] (ct1) {$\vect{c}_{t-1}$};
        \node[right of=ct1, xshift=1.2cm] (sum) {$+$};
        \node[right of=sum, xshift=1.2cm] (ct) {$\vect{c}_t$};
        \draw[->] (ct1) -- node[midway, above] {$\odot\,\vect{f}_t$} (sum);
        \draw[->] (it) -- node[midway, above] {$\odot\,\tilde{\vect{c}}_t$} (sum);
        \draw[->] (sum) -- (ct);
        \draw[->] (ot) |- ++(1.2,1.8) node[pos=0.55, right] {$\odot\,\tanh(\vect{c}_t)$} -| ++(0,0);
    \end{tikzpicture}
    \caption{Visual aid: A compact LSTM cell diagram.}
    \label{fig:lstm_cell_diagram}
\end{figure}

