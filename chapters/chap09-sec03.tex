% Chapter 9, Section 3

\section{CNN Architectures \difficultyInline{intermediate}}
\label{sec:cnn-architectures}

\subsection*{Historical Context}
CNNs evolved from early biologically inspired work to practical systems. \textbf{LeNet-5} established the template for digit recognition \cite{LeCun1989}. \textbf{AlexNet} showed large-scale training with ReLU, dropout, and data augmentation could dominate ImageNet \cite{Krizhevsky2012}. \textbf{VGG} emphasized simplicity via small filters, while \textbf{Inception} exploited multi-scale processing with $1\times1$ dimension reduction. \textbf{ResNet} enabled very deep networks via residual connections \cite{He2016}. Efficiency-driven families like \textbf{MobileNet} and \textbf{EfficientNet} target edge devices and compound scaling.
\subsection{LeNet-5 (1998)\index{LeNet-5}}
\label{subsec:lenet}

LeNet-5 demonstrated the viability of CNNs for handwritten digit recognition on the MNIST dataset, establishing the foundational architecture that would influence all subsequent convolutional networks. The network combined convolution, subsampling through pooling, and small fully connected layers in a carefully designed topology: \(\text{Conv}(6@5\times5)\to\text{Pool}(2\times2)\to\text{Conv}(16@5\times5)\to\text{Pool}(2\times2)\to\text{FC}(120)\to\text{FC}(84)\to\text{Softmax}(10)\).

The architecture utilized sigmoid and tanh activation functions, though later pedagogical treatments often retrofit ReLU activations to demonstrate modern practices. LeNet-5's key properties included local receptive fields that captured spatial patterns, parameter sharing that dramatically reduced the number of learnable parameters, and early evidence of translation invariance through pooling operations. The network's impact extended far beyond digit recognition, establishing the core conv-pool pattern and demonstrating end-to-end learning for computer vision tasks, fundamentally changing how researchers approached image classification problems.\cite{LeCun1989,GoodfellowEtAl2016}

\begin{remark}[LeNet-5's Historical Importance]
LeNet-5 was the first successful deep convolutional network that demonstrated the practical viability of CNNs for real-world applications, establishing the fundamental conv-pool-FC architecture pattern that became the foundation for all subsequent CNN designs. Its success on MNIST digit recognition proved that deep learning could solve complex pattern recognition tasks, directly inspiring the development of modern CNN architectures and marking the beginning of the deep learning revolution in computer vision.
\end{remark}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=0.2cm,y=0.2cm]
        % input
        \draw[fill=bookpurple!10,draw=bookpurple] (0,0) rectangle (12,12);
        \node at (6,13.5) {Input $32\times32$};
        % conv1
        \draw[->,thick] (12,6) -- (16,6);
        \draw[fill=bookpurple!20,draw=bookpurple] (16,1) rectangle (24,11);
        \node[align=center] at (20,12.8) {Conv1\\$6@5\times5$};
        % pool1
        \draw[->,thick] (24,6) -- (28,6);
        \draw[fill=bookpurple!20,draw=bookpurple] (28,3) rectangle (34,9);
        \node[align=center] at (31,10.8) {Pool $2\times2$};
        % conv2
        \draw[->,thick] (34,6) -- (38,6);
        \draw[fill=bookpurple!20,draw=bookpurple] (38,2) rectangle (46,10);
        \node[align=center] at (42,11.8) {Conv2\\$16@5\times5$};
        % pool2
        \draw[->,thick] (46,6) -- (50,6);
        \draw[fill=bookpurple!20,draw=bookpurple] (50,4) rectangle (55,8);
        \node[align=center] at (52.5,9.8) {Pool};
        % fc
        \draw[->,thick] (55,6) -- (59,6);
        \draw[fill=bookpurple!10,draw=bookpurple] (59,4) rectangle (62,8);
        \node at (60.5,9.8) {FC120};
        \draw[->,thick] (62,6) -- (66,6);
        \draw[fill=bookpurple!10,draw=bookpurple] (66,4) rectangle (69,8);
        \node at (67.5,9.8) {FC84};
        \draw[->,thick] (69,6) -- (73,6);
        \draw[fill=bookpurple!10,draw=bookpurple] (73,4) rectangle (76,8);
        \node at (74.5,9.8) {10};
    \end{tikzpicture}
    \caption{LeNet-5 architecture: alternating conv and pooling, followed by small fully connected layers.}
    \label{fig:lenet5}
\end{figure}

\subsection{AlexNet (2012)\index{AlexNet}}

AlexNet achieved a decisive victory at ILSVRC 2012, winning by a large margin and catalyzing the deep learning revolution in computer vision. The network's design featured 5 convolutional layers followed by 3 fully connected layers, incorporating local response normalization (LRN) and overlapping pooling to improve feature extraction. The optimization strategy proved equally important, with ReLU activations enabling significantly faster training compared to traditional sigmoid and tanh functions, while heavy data augmentation and dropout in the fully connected layers effectively reduced overfitting.

The systems engineering aspects of AlexNet were equally groundbreaking, as the network was trained on 2 GPUs using model parallelism to handle the computational demands. The architecture used large kernels in early layers combined with stride for rapid downsampling, creating an efficient pipeline for processing high-resolution images. AlexNet's impact extended far beyond its immediate performance gains, establishing large-scale supervised pretraining on ImageNet as the standard approach for computer vision research and demonstrating the practical viability of deep learning for real-world applications.\cite{Krizhevsky2012}

\subsection{VGG Networks (2014)\index{VGG}}

VGG networks emphasized architectural depth through a simple yet effective recipe: stacks of $3\times3$ convolutions with stride 1 combined with $2\times2$ max pooling for downsampling. The uniform block design represented a key innovation, where replacing large kernels with multiple $3\times3$ layers increased nonlinearity and receptive field while using fewer parameters than equivalent single large kernels. This approach demonstrated that depth could be achieved through careful architectural choices rather than simply adding more layers.

The VGG-16 and VGG-19 models achieved strong accuracy on ImageNet, though they came with significant computational costs due to very large parameter counts in the dense layers. The trade-offs inherent in VGG networks included strong classification accuracy balanced against heavy memory and computation requirements, making them particularly suitable as feature extractors for transfer learning applications. Despite their computational demands, VGG networks established the importance of systematic architectural design and demonstrated that simple, uniform building blocks could achieve competitive performance when properly scaled.\cite{GoodfellowEtAl2016}

\begin{remark}[VGG Networks' Impact on Deep Learning]
VGG networks revolutionized CNN design by proving that depth could be achieved through systematic use of small 3Ã—3 filters, establishing the principle that multiple small convolutions are more effective than single large filters. Their uniform architecture design and strong performance made VGG networks the standard feature extractor for transfer learning, directly influencing the development of modern CNN architectures and demonstrating that systematic architectural choices could achieve state-of-the-art performance.
\end{remark}

\subsection{ResNet (2015)\index{ResNet}}
\label{subsec:resnet}

ResNet introduced identity skip connections to learn residual functions, fundamentally changing how deep networks could be trained and optimized. The core equation $\vect{y} = \mathcal{F}(\vect{x}, \{\mat{W}_i\}) + \vect{x}$ represents a simple yet powerful idea: instead of learning the direct mapping from input to output, the network learns the residual function $\mathcal{F}$ that represents the difference between input and desired output, where $\mathcal{F}$ is typically a small stack of convolutions with normalization and activation.

This architectural innovation enabled unprecedented depth, allowing stable training of 50, 101, and 152-layer models that would have been impossible to train with traditional architectures. The gradient flow benefits were equally important, as the Jacobian includes an identity term that mitigates vanishing gradients by providing direct paths for gradient propagation. ResNet blocks come in two main variants: basic blocks with two $3\times3$ convolutions and bottleneck blocks with $1\times1$-$3\times3$-$1\times1$ convolutions, both incorporating projection shortcuts to handle dimension changes between layers.\cite{He2016}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=0.25cm,y=0.25cm]
        % input x
        \draw[fill=bookpurple!10,draw=bookpurple] (0,4) rectangle (6,10);
        \node at (3,11.7) {$\vect{x}$};
        % conv block
        \draw[->,thick] (6,7) -- (9,7);
        \draw[fill=bookpurple!20,draw=bookpurple] (9,5.5) rectangle (15,8.5);
        \node at (12,9.8) {$3\times3$};
        \draw[->,thick] (15,7) -- (18,7);
        \draw[fill=bookpurple!20,draw=bookpurple] (18,5.5) rectangle (24,8.5);
        \node at (21,9.8) {$3\times3$};
        % sum
        \draw[->,thick] (24,7) -- (27,7);
        % skip
        \draw[thick] (3,10) to[out=90,in=180] (15,13) to[out=0,in=90] (27,7);
        % output
        \draw[fill=bookpurple!10,draw=bookpurple] (27,4) rectangle (33,10);
        \node at (30,11.7) {$\vect{y}$};
    \end{tikzpicture}
    \caption{ResNet basic residual block with identity skip connection.}
    \label{fig:resnet-block}
\end{figure}

\subsection{Inception/GoogLeNet (2014)\index{Inception}\index{GoogLeNet}}
\label{subsec:inception}

GoogLeNet popularized Inception modules that revolutionized CNN design through parallel multi-scale processing and strategic use of $1\times1$ bottlenecks for computational efficiency. The Inception architecture featured parallel branches with $1\times1$, $3\times3$, and $5\times5$ convolutions alongside a pooled branch, all followed by channel-wise concatenation to combine multi-scale features. This design philosophy recognized that different scales of features are important for visual recognition and that processing them in parallel could capture more diverse patterns than sequential processing.

The efficiency gains came primarily from $1\times1$ convolutions that reduced channel dimensions before applying larger kernels, dramatically cutting floating-point operations while preserving network capacity. This approach proved particularly effective for reducing computational costs in deeper layers where channel counts are high. GoogLeNet's impact extended beyond its immediate performance, achieving competitive accuracy with significantly fewer parameters than VGG networks while influencing the design of later hybrid networks that incorporated multi-scale processing and efficient channel mixing techniques.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=0.25cm,y=0.25cm]
        % input
        \draw[fill=bookpurple!10,draw=bookpurple] (0,6) rectangle (6,12);
        \node at (3,13.7) {Input};
        % branches
        \draw[->,thick] (6,9) -- (9,9);
        % 1x1
        \draw[fill=bookpurple!20,draw=bookpurple] (9,10.5) rectangle (15,12.5);
        \node at (12,13.7) {$1\times1$};
        % 3x3
        \draw[fill=bookpurple!20,draw=bookpurple] (9,7.5) rectangle (15,9.5);
        \node at (12,10.7) {$3\times3$};
        % 5x5
        \draw[fill=bookpurple!20,draw=bookpurple] (9,4.5) rectangle (15,6.5);
        \node at (12,7.7) {$5\times5$};
        % pool
        \draw[fill=bookpurple!20,draw=bookpurple] (9,1.5) rectangle (15,3.5);
        \node at (12,4.7) {Pool $3\times3$};
        % concat
        \draw[->,thick] (15,11.5) -- (18,11.5);
        \draw[->,thick] (15,8.5) -- (18,8.5);
        \draw[->,thick] (15,5.5) -- (18,5.5);
        \draw[->,thick] (15,2.5) -- (18,2.5);
        \draw[fill=bookpurple!10,draw=bookpurple] (18,1) rectangle (24,13);
        \node at (21,14.7) {Concat};
    \end{tikzpicture}
    \caption{Inception module: parallel multi-scale branches with $1\times1$ bottlenecks.}
    \label{fig:inception-module}
\end{figure}

\subsection{MobileNet and EfficientNet\index{MobileNet}\index{EfficientNet}}

\textbf{MobileNet.} Prioritizes efficiency for edge devices using depthwise separable convolutions (depthwise $k\times k$ followed by pointwise $1\times1$), drastically reducing FLOPs and parameters while maintaining accuracy.

\textbf{EfficientNet.} Introduces compound scaling to jointly scale depth, width, and resolution with a principled coefficient, yielding strong accuracy/efficiency trade-offs. Variants (B0--B7) demonstrate near-optimal Pareto fronts.

For introductory treatment of these families, see \textit{D2L} \href{https://d2l.ai/chapter_convolutional-modern/index.html}{(modern CNNs)} and \textit{Deep Learning} \href{https://www.deeplearningbook.org/contents/convnets.html}{(convolutional networks)}.

