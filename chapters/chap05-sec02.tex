% Chapter 5, Section 02

\section{Logistic Regression \difficultyInline{intermediate}}
\label{sec:logistic-regression}

\textbf{Logistic regression} is a fundamental classification algorithm that models the probability of class membership using a logistic (sigmoid) function. Despite its name, it's actually a classification method, not a regression method.

\subsection{Intuition and Motivation}

Logistic regression extends linear regression to handle classification problems. Instead of predicting continuous values, it predicts probabilities that an input belongs to a particular class. The key insight is to use a sigmoid function to map linear combinations of features to probabilities between 0 and 1.

Think of logistic regression as answering: "Given these features, what's the probability that this example belongs to the positive class?" For example, given a patient's symptoms, what's the probability they have a particular disease?

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={Linear Score ($z = \vect{w}^\top \vect{x} + b$)},
    ylabel={Probability $P(y=1|\vect{x})$},
    title={Sigmoid Function: Mapping Linear Scores to Probabilities},
    grid=major,
    width=10cm,
    height=6cm,
    xmin=-6, xmax=6,
    ymin=0, ymax=1
]
\addplot[thick, color=bookpurple, domain=-6:6] {1/(1+exp(-x))};
\node at (axis cs:2,0.8) [anchor=west] {$\sigma(z) = \frac{1}{1 + e^{-z}}$};
\addplot[dashed, color=bookred] coordinates {(-6,0.5) (6,0.5)};
\node at (axis cs:0,0.6) [anchor=east] {Decision boundary at 0.5};
\end{axis}
\end{tikzpicture}
\caption{Sigmoid function maps real numbers to probabilities (0,1). Decision boundary at 0.5.}
\label{fig:sigmoid-function}
\end{figure}

\subsection{Binary Classification}

For binary classification with classes $\{0, 1\}$, logistic regression models the probability of the positive class using the sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$, which smoothly maps any real number to a probability between 0 and 1. The prediction probability is:
\begin{equation}
P(y=1|\vect{x}) = \sigma(\vect{w}^\top \vect{x} + b) = \frac{1}{1 + e^{-(\vect{w}^\top \vect{x} + b)}}
\end{equation}
where the linear combination of features and weights determines the input to the sigmoid function, and the output represents the probability that the input belongs to the positive class.

\begin{remark}
For categorical classification with more than 2 classes, we extend logistic regression to softmax regression, which uses the softmax function to compute probabilities for each class that sum to 1. This allows us to handle multi-class problems like image classification with 10 digit classes or natural language processing with hundreds of word categories.
\end{remark}

\subsubsection{Properties of the Sigmoid Function}

The sigmoid function has several important properties:
\begin{itemize}
    \item \textbf{Range:} $\sigma(z) \in (0, 1)$ for all $z \in \mathbb{R}$
    \item \textbf{Monotonic:} $\sigma'(z) = \sigma(z)(1-\sigma(z)) > 0$ for all $z$
    \item \textbf{Symmetric:} $\sigma(-z) = 1 - \sigma(z)$
    \item \textbf{Asymptotic:} $\lim_{z \to \infty} \sigma(z) = 1$ and $\lim_{z \to -\infty} \sigma(z) = 0$
\end{itemize}

\subsection{Cross-Entropy Loss}

Unlike linear regression, we can't use mean squared error for classification because the sigmoid function is non-linear. Instead, we use the cross-entropy loss (negative log-likelihood):

\begin{definition}[Cross-Entropy Loss]
The cross-entropy loss for binary classification is:

\begin{equation}
L(\vect{w}, b) = -\frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})\right]
\end{equation}
where $\hat{y}^{(i)} = P(y=1|\vect{x}^{(i)})$.
\end{definition}

\begin{remark}
Cross-entropy loss is the standard loss function for classification tasks in both classical machine learning and deep learning, providing better gradient properties than mean squared error for probability outputs. In deep neural networks, it's commonly used in the final layer for binary and multi-class classification, enabling efficient backpropagation and stable training across various architectures from simple logistic regression to complex convolutional and transformer networks.
\end{remark}

\begin{definition}[Derivation of Cross-Entropy Loss]
The cross-entropy loss comes from maximum likelihood estimation. For a single example, the likelihood is:
$$L_i = P(y^{(i)}|\vect{x}^{(i)}) = (\hat{y}^{(i)})^{y^{(i)}} (1-\hat{y}^{(i)})^{1-y^{(i)}}$$

Taking the negative log-likelihood:
$$-\log L_i = -y^{(i)} \log \hat{y}^{(i)} - (1-y^{(i)}) \log(1-\hat{y}^{(i)})$$
\end{definition}

\subsection{Gradient Descent for Logistic Regression}

The gradient of the cross-entropy loss with respect to the weights is:

\begin{equation}
\nabla_{\vect{w}} L = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)}) \vect{x}^{(i)}
\end{equation}

The weight update rule becomes:
\begin{equation}
\vect{w}_{t+1} = \vect{w}_t - \alpha \frac{1}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)}) \vect{x}^{(i)}
\end{equation}

\begin{remark}
The gradient has a simple form: it's the average of the prediction errors multiplied by the input features. This makes logistic regression particularly efficient to train.
\end{remark}

\subsection{Multiclass Classification}

For $K$ classes, we extend logistic regression to softmax regression (also called multinomial logistic regression), where instead of a single sigmoid function, we use the softmax function to compute the probability of each class:

\begin{equation}
P(y=k|\vect{x}) = \frac{\exp(\vect{w}_k^\top \vect{x} + b_k)}{\sum_{j=1}^{K} \exp(\vect{w}_j^\top \vect{x} + b_j)}
\end{equation}

The softmax function normalizes the exponential scores so that they sum to 1, creating a valid probability distribution over all possible classes.

\subsubsection{Properties of Softmax}

The softmax function has several key properties:
\begin{itemize}
    \item \textbf{Probability distribution:} $\sum_{k=1}^{K} P(y=k|\vect{x}) = 1$
    \item \textbf{Non-negative:} $P(y=k|\vect{x}) \geq 0$ for all $k$
    \item \textbf{Monotonic:} Higher scores lead to higher probabilities
    \item \textbf{Scale invariant:} Adding a constant to all scores doesn't change probabilities
\end{itemize}

\subsection{Categorical Cross-Entropy Loss}

For multiclass classification, we use the categorical cross-entropy loss:

\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log \hat{y}_k^{(i)}
\end{equation}

where $y_k^{(i)}$ is 1 if example $i$ belongs to class $k$, and 0 otherwise (one-hot encoding).

\subsection{Decision Boundaries}

Logistic regression creates linear decision boundaries. For binary classification, the decision boundary is the hyperplane where $P(y=1|\vect{x}) = 0.5$, which occurs when $\vect{w}^\top \vect{x} + b = 0$.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={Feature 1},
    ylabel={Feature 2},
    title={Logistic Regression Decision Boundary},
    grid=major,
    width=10cm,
    height=8cm
]
% Generate some sample data
\addplot[only marks, mark=*, mark size=3pt, color=bookpurple] coordinates {
    (1,1) (1.5,1.5) (2,1) (2.5,2) (3,1.5)
};
\addplot[only marks, mark=square*, mark size=3pt, color=bookred] coordinates {
    (2,3) (2.5,3.5) (3,3) (3.5,3.5) (4,3)
};
% Add decision boundary
\addplot[thick, color=bookblack, domain=0:5] {3-x};
\node at (axis cs:2.5,2.5) [anchor=west] {Decision boundary: $x_1 + x_2 = 3$};
\legend{Class 0, Class 1, Decision boundary}
\end{axis}
\end{tikzpicture}
\caption{Logistic regression finds linear decision boundary separating classes 0 and 1.}
\label{fig:logistic-decision-boundary}
\end{figure}

\subsection{Regularization in Logistic Regression}

Just like linear regression, logistic regression can benefit from regularization to prevent overfitting:

\subsubsection{L2 Regularization (Ridge)}

\begin{equation}
L(\vect{w}) = -\frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})\right] + \lambda \|\vect{w}\|^2
\end{equation}

\subsubsection{L1 Regularization (Lasso)}

\begin{equation}
L(\vect{w}) = -\frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})\right] + \lambda \|\vect{w}\|_1
\end{equation}

\begin{remark}
The key difference between L1 and L2 regularization lies in their penalty shapes: L2 uses the sum of squared weights $\|\vect{w}\|^2$ which creates smooth, continuous shrinkage of all weights, while L1 uses the sum of absolute weights $\|\vect{w}\|_1$ which can drive some weights to exactly zero, performing automatic feature selection. L2 is better for preventing overfitting when all features are relevant, while L1 is preferred when you want to identify and remove irrelevant features.
\end{remark}

\subsection{Advantages and Limitations}

Logistic regression has several advantages including being simple and interpretable with clear mathematical foundations, fast training and prediction due to efficient gradient computation, providing probability estimates that are useful for decision-making, working well with small datasets where complex models might overfit, and making no assumptions about feature distributions. However, it also has limitations including assuming a linear relationship between features and log-odds which may not hold for complex data, being sensitive to outliers that can significantly affect the decision boundary, potentially not working well with highly correlated features that can cause numerical instability, and being limited to linear decision boundaries which may not be sufficient for non-linearly separable data.

