% Chapter 9, Section 1

\section{The Convolution Operation\index{convolution}\index{cross-correlation}}
\label{sec:convolution}


\subsection*{Intuition}
Convolution extracts local patterns by sliding small filters across the input, producing feature maps that respond strongly where patterns occur. Parameter sharing means the same filter detects the same pattern anywhere, yielding translation equivariance \index{equivariance} and dramatic parameter efficiency \cite{GoodfellowEtAl2016,Prince2023}.

\subsection{Definition}

The \textbf{convolution} operation applies a filter (kernel) across an input:

For discrete 2D convolution:
\begin{equation}
S(i,j) = (I * K)(i,j) = \sum_m \sum_n I(i-m, j-n) K(m, n)
\end{equation}

where $I$ is the input and $K$ is the kernel. In deep learning libraries, the implemented operation is often cross-correlation (no kernel flip).

In practice, we often use \textbf{cross-correlation}:
\begin{equation}
S(i,j) = (I * K)(i,j) = \sum_m \sum_n I(i+m, j+n) K(m, n)
\end{equation}

\subsection{Properties}

\textbf{Parameter sharing:} same kernel applied across spatial locations\index{parameter sharing}. This dramatically reduces parameters compared to fully connected layers acting on flattened images. Parameter sharing yields \emph{translation equivariance}: if the input shifts, the feature map shifts in the same way \cite{GoodfellowEtAl2016,Prince2023}. Formally, letting \(T_\delta\) denote a spatial shift, we have \((T_\delta I) * K = T_\delta (I * K)\) under appropriate boundary conditions.

\textbf{Local connectivity:} each output depends on a local input region (\gls{receptive-field})\index{receptive field}. This local connectivity exploits spatial locality, recognising that nearby pixels are statistically dependent in natural images. Through \emph{compositionality}, stacking layers grows the effective receptive field, enabling detection of increasingly complex patternsâ€”from edges to corners to object parts \cite{GoodfellowEtAl2016}.

\textbf{Linearity and nonlinearity:} a single convolution is linear, so CNNs interleave it with pointwise nonlinearities (e.g., ReLU) to model complex functions. With stride and padding fixed, convolution is a sparse matrix multiplication with a Toeplitz structure \cite{GoodfellowEtAl2016}.

\textbf{Padding and boundary effects}\index{padding}: padding preserves spatial size and mitigates edge shrinkage. Without padding (``valid''), repeated convolutions rapidly reduce feature map size and bias features toward the center.

\textbf{Stride and downsampling}\index{stride}: stride \(s>1\) reduces spatial resolution. While efficient, aggressive striding early can remove fine detail; many designs delay downsampling to later stages (e.g., \textit{conv stem} then stride) \cite{Krizhevsky2012,He2016}.

\textbf{Dilation (atrous convolutions)}\index{dilation}: inserting \emph{holes} between kernel elements increases receptive field without increasing parameters, useful for dense prediction (e.g., segmentation) \cite{GoodfellowEtAl2016}.

\textbf{Invariance via pooling:} convolution is equivariant to translation; combining it with pooling or global aggregation introduces partial \emph{translation invariance} in the representation \cite{GoodfellowEtAl2016}.

\textbf{Multi-channel mixing:} kernels of shape \(k\times k\times C_{\text{in}}\) learn both spatial and cross-channel interactions, while $1\times1$ convolutions mix channels without spatial coupling (used for bottlenecks and dimension reduction in Inception/ResNet).

\subsection{Multi-Channel Convolution}

For input with $C_{\text{in}}$ channels and $C_{\text{out}}$ output channels:
\begin{equation}
S_{c_{\text{out}}}(i,j) = \sum_{c_{\text{in}}=1}^{C_{\text{in}}} (I_{c_{\text{in}}} * K_{c_{\text{out}}, c_{\text{in}}})(i,j) + b_{c_{\text{out}}}
\end{equation}

\paragraph{Understanding Multi-Channel Convolution and S(i,j)}
Multi-channel convolution is the fundamental operation that enables CNNs to process complex inputs like RGB images (3 channels) and produce rich feature representations. The notation $S_{c_{\text{out}}}(i,j)$ represents the output feature map value at spatial position $(i,j)$ for output channel $c_{\text{out}}$. This operation allows the network to learn both spatial patterns (through kernel sliding) and cross-channel interactions (by mixing information from different input channels). Each output channel $S_{c_{\text{out}}}(i,j)$ captures how strongly a particular learned pattern is detected at location $(i,j)$, where higher values indicate stronger pattern matches. This enables the network to detect complex features that span across multiple input channels while maintaining spatial locality and translation equivariance.

\subsection{Hyperparameters}

\textbf{Kernel size:} typical choices are $3 \times 3$ or $5 \times 5$, with stacked $3\times3$ filters often preferred over a single $5\times5$ as demonstrated in VGG \cite{GoodfellowEtAl2016}.

\textbf{Stride:} the step size for sliding the kernel (stride $s$)\index{stride} determines the output size according to:
\begin{equation}
\text{Output size} = \left\lfloor \frac{n - k}{s} \right\rfloor + 1
\end{equation}

\textbf{Padding:} adding zeros around the input\index{padding} offers different strategies. \textbf{Valid} padding means no padding is added. \textbf{Same} padding adds just enough padding to preserve spatial size. \textbf{Full} padding adds maximum padding. For "same" padding with stride 1, the required padding is:
\begin{equation}
p = \left\lfloor \frac{k-1}{2} \right\rfloor
\end{equation}

% \subsection{Visual Aid}
% \begin{figure}[h]
%     \centering
%     % illustrative kernel sliding grid using TikZ
%     \begin{tikzpicture}[scale=0.5]
%         % input grid 5x5
%         \foreach \i in {0,...,5} {\draw (0,\i) -- (5,\i);}
%         \foreach \j in {0,...,5} {\draw (\j,0) -- (\j,5);}
%         % kernel window 3x3 at (1,1)
%         \draw[bookred,very thick] (1,1) rectangle (4,4);
%     \end{tikzpicture}
%     \caption{A $3\times3$ kernel sliding over a $5\times5$ input highlights local pattern extraction.}
%     \label{fig:conv-sliding-window}
% \end{figure}

