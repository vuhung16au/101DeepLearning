% Chapter 6, Section 2

\section{Activation Functions \difficultyInline{intermediate}}
\label{sec:activation-functions}

Activation functions are nonlinear transformations applied to neuron outputs that introduce the essential non-linearity needed for neural networks to learn complex patterns and relationships in data.

\subsection{Intuition: Why Do We Need Activation Functions?}

Imagine you're building a house with only straight lines and right angles, where no matter how many rooms you add, you can only create rectangular spaces, but what if you want curved walls, arches, or domes? You need curved tools! Similarly, without activation functions, neural networks can only learn linear relationships, no matter how many layers you add, where activation functions are the "curved tools" that allow networks to learn non-linear patterns. Think of an activation function as a decision maker that takes a weighted sum of information from other neurons as input, decides how much this neuron should "fire" or contribute to the next layer, and outputs a transformed value that becomes input to the next layer. Activation functions introduce non-linearity, enabling networks to learn complex patterns that would be impossible with linear transformations alone.

\subsection{Sigmoid}

The sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$ is an S-shaped curve that maps any real number to a value between 0 and 1, making it useful for binary classification and probability estimation. However, the sigmoid function has several limitations including saturating for large absolute values of $z$ which causes vanishing gradients during backpropagation, not being zero-centered which can slow down learning, and being historically important but less common in hidden layers due to these issues, though it remains useful in output layers for binary classification tasks.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=0.7\textwidth,
    height=0.4\textwidth,
    axis lines=center,
    xlabel={$z$}, ylabel={$\sigma(z)$},
    xmin=-6, xmax=6, ymin=-0.1, ymax=1.1,
    grid=both, grid style={draw=bookpurple!10},
    major grid style={draw=bookpurple!40}
  ]
    \addplot[domain=-6:6, samples=300, very thick, bookpurple] {1/(1+exp(-x))};
  \end{axis}
\end{tikzpicture}
\caption{Sigmoid $\sigma(z)=1/(1+e^{-z})$: smooth S-shape $\mathbb{R}\to(0,1)$.}
\label{fig:sigmoid-plot}
\end{figure}

\subsection{Hyperbolic Tangent (tanh)}

The hyperbolic tangent function $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ is a zero-centered S-shaped curve that maps any real number to a value between -1 and 1, making it an improvement over sigmoid for hidden layers. The tanh function is zero-centered which helps with gradient flow and learning dynamics, though it still suffers from saturation at extreme values which can cause vanishing gradients in deep networks, making it better than sigmoid but still not ideal for very deep architectures.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=0.7\textwidth,
    height=0.4\textwidth,
    axis lines=center,
    xlabel={$z$}, ylabel={$\tanh(z)$},
    xmin=-6, xmax=6, ymin=-1.1, ymax=1.1,
    grid=both, grid style={draw=bookpurple!10},
    major grid style={draw=bookpurple!40}
  ]
    \addplot[domain=-6:6, samples=300, very thick, bookred] {tanh(x)};
  \end{axis}
\end{tikzpicture}
\caption{$\tanh(z)$: zero-centered S-shape mapping $\mathbb{R}\to(-1,1)$.}
\label{fig:tanh-plot}
\end{figure}

\subsection{Rectified Linear Unit (ReLU)}

The Rectified Linear Unit (ReLU) function $\text{ReLU}(z) = \max(0, z)$ is a simple piecewise linear function that outputs the input directly if it's positive, otherwise outputs zero. ReLU is simple and computationally efficient with no saturation for positive values, making it the most widely used activation function in modern deep learning. However, ReLU can cause "dead neurons" that always output 0 when the input is negative, which can slow down learning, though this issue is often mitigated by proper initialization and other techniques.

\subsection{Leaky ReLU and Variants}

\textbf{Leaky ReLU:}
\begin{equation}
\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha \ll 1
\end{equation}

\textbf{Parametric ReLU (PReLU):}
\begin{equation}
\text{PReLU}(z) = \max(\alpha z, z)
\end{equation}
where $\alpha$ is learned.

\textbf{Exponential Linear Unit (ELU):}
\begin{equation}
\text{ELU}(z) = \begin{cases}
z & \text{if } z > 0 \\
\alpha(e^z - 1) & \text{if } z \leq 0
\end{cases}
\end{equation}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.6]
    % Leaky ReLU plot
    \begin{scope}[xshift=0cm]
        \begin{axis}[
            width=0.3\textwidth,
            height=0.25\textwidth,
            axis lines=center,
            xlabel={$z$},
            ylabel={$\text{LeakyReLU}(z)$},
            xmin=-3, xmax=3,
            ymin=-0.5, ymax=3,
            grid=both,
            grid style={line width=.1pt, draw=bookpurple!10},
            major grid style={line width=.2pt, draw=bookpurple!50},
            title={Leaky ReLU ($\alpha=0.001$)}
        ]
        \addplot[domain=-3:0, samples=100, color=bookpurple, thick] {0.001*x};
        \addplot[domain=0:3, samples=100, color=bookpurple, thick] {x};
        \addplot[only marks, mark=*, color=bookpurple] coordinates {(0,0)};
        \node at (axis cs:2,2.5) [anchor=west] {$\max(0.001z, z)$};
        \end{axis}
    \end{scope}
    
    % PReLU plot
    \begin{scope}[xshift=4cm]
        \begin{axis}[
            width=0.3\textwidth,
            height=0.25\textwidth,
            axis lines=center,
            xlabel={$z$},
            ylabel={$\text{PReLU}(z)$},
            xmin=-3, xmax=3,
            ymin=-0.5, ymax=3,
            grid=both,
            grid style={line width=.1pt, draw=bookpurple!10},
            major grid style={line width=.2pt, draw=bookpurple!50},
            title={PReLU ($\alpha=0.001$)}
        ]
        \addplot[domain=-3:0, samples=100, color=bookred, thick] {0.001*x};
        \addplot[domain=0:3, samples=100, color=bookred, thick] {x};
        \addplot[only marks, mark=*, color=bookred] coordinates {(0,0)};
        \node at (axis cs:2,2.5) [anchor=west] {$\max(\alpha z, z)$};
        \end{axis}
    \end{scope}
    
    % ELU plot
    \begin{scope}[xshift=8cm]
        \begin{axis}[
            width=0.3\textwidth,
            height=0.25\textwidth,
            axis lines=center,
            xlabel={$z$},
            ylabel={$\text{ELU}(z)$},
            xmin=-3, xmax=3,
            ymin=-1, ymax=3,
            grid=both,
            grid style={line width=.1pt, draw=bookpurple!10},
            major grid style={line width=.2pt, draw=bookpurple!50},
            title={ELU ($\alpha=0.001$)}
        ]
        \addplot[domain=-3:0, samples=100, color=bookpurple!70, thick] {0.001*(exp(x)-1)};
        \addplot[domain=0:3, samples=100, color=bookpurple!70, thick] {x};
        \addplot[only marks, mark=*, color=bookpurple!70] coordinates {(0,0)};
        \node at (axis cs:2,2.5) [anchor=west] {$\alpha(e^z-1)$ for $z\leq0$};
        \end{axis}
    \end{scope}
\end{tikzpicture}
\caption{Leaky ReLU, PReLU, and ELU activation functions with $\alpha=0.001$.}
\label{fig:leaky-relu-variants}
\end{figure}

\subsection{Swish and GELU}

The Swish activation function $\text{Swish}(z) = z \cdot \sigma(z)$ is a smooth, non-monotonic function that combines the benefits of ReLU with smooth gradients, often outperforming ReLU in many tasks. The Gaussian Error Linear Unit (GELU) $\text{GELU}(z) = z \cdot \Phi(z)$ where $\Phi$ is the Gaussian CDF, is used in modern transformers because it provides smooth gradients and better performance in transformer architectures, particularly in the attention mechanisms and feedforward layers of models like BERT, GPT, and other large language models where the smooth activation helps with training stability and convergence.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.7]
    \begin{axis}[
        axis lines = center,
        xlabel = $z$,
        ylabel = $\sigma(z)$,
        xmin = -4, xmax = 4,
        ymin = -1.5, ymax = 2,
        legend pos = north west,
        grid = both,
        grid style = {line width=.1pt, draw=bookpurple!10},
        major grid style = {line width=.2pt, draw=bookpurple!50},
    ]
    
    % Sigmoid
    \addplot[domain=-4:4, samples=100, color=bookpurple, thick] {1/(1+exp(-x))};
    \addlegendentry{Sigmoid}
    
    % Tanh
    \addplot[domain=-4:4, samples=100, color=bookred, thick] {tanh(x)};
    \addlegendentry{Tanh}
    
    % ReLU
    \addplot[domain=-4:4, samples=100, color=bookpurple!70, thick] {max(0,x)};
    \addlegendentry{ReLU}
    
    % Leaky ReLU
    \addplot[domain=-4:4, samples=100, color=bookred!70, thick] {max(0.1*x, x)};
    \addlegendentry{Leaky ReLU}
    
    % Swish
    \addplot[domain=-4:4, samples=100, color=bookpurple!50, thick] {x/(1+exp(-x))};
    \addlegendentry{Swish}
    
    \end{axis}
\end{tikzpicture}
\caption{Common activations: sigmoid/tanh saturate, ReLU can "die", Swish smooth.}
\label{fig:activation-functions}
\end{figure}

% Index entries
\index{activation function!sigmoid}
\index{activation function!tanh}
\index{activation function!ReLU}
\index{activation function!Leaky ReLU}
\index{activation function!Swish}
\index{activation function!GELU}
\index{non-linearity!activation functions}

