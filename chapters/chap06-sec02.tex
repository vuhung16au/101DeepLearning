% Chapter 6, Section 2

\section{Activation Functions \difficultyInline{intermediate}}
\label{sec:activation-functions}

\subsection{Intuition: Why Do We Need Activation Functions?}

Imagine you're building a house with only straight lines and right angles. No matter how many rooms you add, you can only create rectangular spaces. But what if you want curved walls, arches, or domes? You need curved tools!

Similarly, without activation functions, neural networks can only learn linear relationships, no matter how many layers you add. Activation functions are the "curved tools" that allow networks to learn non-linear patterns.

Think of an activation function as a decision maker:
\begin{itemize}
    \item \textbf{Input:} A weighted sum of information from other neurons
    \item \textbf{Decision:} How much should this neuron "fire" or contribute to the next layer?
    \item \textbf{Output:} A transformed value that becomes input to the next layer
\end{itemize}

Activation functions introduce non-linearity, enabling networks to learn complex patterns.

\subsection{Sigmoid}

\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}

Properties:
\begin{itemize}
    \item Range: $(0, 1)$
    \item Saturates for large $|z|$ (vanishing gradients)
    \item Not zero-centered
    \item Historically important but less common in hidden layers
\end{itemize}

\subsection{Hyperbolic Tangent (tanh)}

\begin{equation}
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation}

Properties:
\begin{itemize}
    \item Range: $(-1, 1)$
    \item Zero-centered
    \item Still suffers from saturation
\end{itemize}

\subsection{Rectified Linear Unit (ReLU)}

\begin{equation}
\text{ReLU}(z) = \max(0, z)
\end{equation}

Properties:
\begin{itemize}
    \item Simple and computationally efficient
    \item No saturation for positive values
    \item Can cause "dead neurons" (always output 0)
    \item Most widely used activation
\end{itemize}

\subsection{Leaky ReLU and Variants}

\textbf{Leaky ReLU:}
\begin{equation}
\text{LeakyReLU}(z) = \max(\alpha z, z), \quad \alpha \ll 1
\end{equation}

\textbf{Parametric ReLU (PReLU):}
\begin{equation}
\text{PReLU}(z) = \max(\alpha z, z)
\end{equation}
where $\alpha$ is learned.

\textbf{Exponential Linear Unit (ELU):}
\begin{equation}
\text{ELU}(z) = \begin{cases}
z & \text{if } z > 0 \\
\alpha(e^z - 1) & \text{if } z \leq 0
\end{cases}
\end{equation}

\subsection{Swish and GELU}

\textbf{Swish:}
\begin{equation}
\text{Swish}(z) = z \cdot \sigma(z)
\end{equation}

\textbf{Gaussian Error Linear Unit (GELU):}
\begin{equation}
\text{GELU}(z) = z \cdot \Phi(z)
\end{equation}
where $\Phi$ is the Gaussian CDF. Used in modern transformers.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.7]
    \begin{axis}[
        axis lines = center,
        xlabel = $z$,
        ylabel = $\sigma(z)$,
        xmin = -4, xmax = 4,
        ymin = -1.5, ymax = 2,
        legend pos = north west,
        grid = both,
        grid style = {line width=.1pt, draw=bookpurple!10},
        major grid style = {line width=.2pt, draw=bookpurple!50},
    ]
    
    % Sigmoid
    \addplot[domain=-4:4, samples=100, color=bookpurple, thick] {1/(1+exp(-x))};
    \addlegendentry{Sigmoid}
    
    % Tanh
    \addplot[domain=-4:4, samples=100, color=bookred, thick] {tanh(x)};
    \addlegendentry{Tanh}
    
    % ReLU
    \addplot[domain=-4:4, samples=100, color=bookpurple!70, thick] {max(0,x)};
    \addlegendentry{ReLU}
    
    % Leaky ReLU
    \addplot[domain=-4:4, samples=100, color=bookred!70, thick] {max(0.1*x, x)};
    \addlegendentry{Leaky ReLU}
    
    % Swish
    \addplot[domain=-4:4, samples=100, color=bookpurple!50, thick] {x/(1+exp(-x))};
    \addlegendentry{Swish}
    
    \end{axis}
\end{tikzpicture}
\caption{Comparison of common activation functions. Each function has different properties: sigmoid and tanh saturate at extreme values, ReLU is simple but can "die", while Swish provides smooth gradients.}
\label{fig:activation-functions}
\end{figure}

% Index entries
\index{activation function!sigmoid}
\index{activation function!tanh}
\index{activation function!ReLU}
\index{activation function!Leaky ReLU}
\index{activation function!Swish}
\index{activation function!GELU}
\index{non-linearity!activation functions}

