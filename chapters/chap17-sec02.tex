% Chapter 17, Section 2

\section{Markov Chain Monte Carlo \difficultyInline{advanced}}
\label{sec:mcmc}

\subsection{Markov Chains}

Sequence where $p(x_t|x_{t-1}, \ldots, x_1) = p(x_t|x_{t-1})$.

\textbf{Stationary distribution:} $\pi(x)$ such that if $x_t \sim \pi$, then $x_{t+1} \sim \pi$.

\subsection{Metropolis-Hastings Algorithm}

Sample from target distribution $p(x)$:
\begin{enumerate}
    \item Propose: $x' \sim q(x'|x_t)$
    \item Accept with probability:
    \begin{equation}
    A(x', x_t) = \min\left(1, \frac{p(x')q(x_t|x')}{p(x_t)q(x'|x_t)}\right)
    \end{equation}
    \item If accepted, $x_{t+1} = x'$; otherwise $x_{t+1} = x_t$
\end{enumerate}

\subsection{Gibbs Sampling}

Special case where each variable updated conditionally:
\begin{equation}
x_i^{(t+1)} \sim p(x_i | x_{-i}^{(t)})
\end{equation}

Simple when conditional distributions are tractable.

\subsection{Hamiltonian Monte Carlo}

Uses gradient information for efficient exploration:
\begin{itemize}
    \item Treats parameters as position in physics simulation
    \item Uses momentum for faster mixing
    \item More efficient than random walk methods
\end{itemize}


% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (MCMC)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Iteration}, ylabel={Autocorrelation}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(0,1.0) (1,0.8) (2,0.64) (3,0.51) (4,0.41) (5,0.33)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Autocorrelation decay in an MCMC chain (illustrative).}
%   \label{fig:mcmc-acf}
% \end{figure}

% \subsection{Notes and references}

% Background on MCMC algorithms and diagnostics can be found in \textcite{Bishop2006,GoodfellowEtAl2016,Prince2023}.

