% Chapter 15, Section 4

\section{Contrastive Learning \difficultyInline{intermediate}}
\label{sec:contrastive-learning}

Learn representations by contrasting positive and negative pairs.

\subsection{Core Idea}

Maximize agreement between different views of same data (positive pairs), minimize agreement with other data (negative pairs).

\subsection{SimCLR Framework}

\begin{enumerate}
    \item Apply two random augmentations to each image
    \item Encode both views: $\vect{z}_i = f(\vect{x}_i)$, $\vect{z}_j = f(\vect{x}_j)$
    \item Minimize contrastive loss (NT-Xent):
\end{enumerate}

\begin{equation}
\ell_{i,j} = -\log \frac{\exp(\text{sim}(\vect{z}_i, \vect{z}_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{I}_{[k \neq i]} \exp(\text{sim}(\vect{z}_i, \vect{z}_k)/\tau)}
\end{equation}

where $\text{sim}(\cdot, \cdot)$ is cosine similarity and $\tau$ is temperature.

\subsection{MoCo (Momentum Contrast)}

Uses momentum encoder and queue of negative samples for efficiency.

\subsection{BYOL (Bootstrap Your Own Latent)}

Surprisingly, can work without negative samples using:
\begin{itemize}
    \item Online network (updated by gradients)
    \item Target network (momentum update)
    \item Prediction head on online network
\end{itemize}

\subsection{Applications}

State-of-the-art results in:
\begin{itemize}
    \item Image classification
    \item Object detection
    \item Segmentation
    \item Medical imaging with limited labels
\end{itemize}


% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (contrastive)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Epoch}, ylabel={NT-Xent loss}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1,6.0) (10,3.2) (20,2.1) (40,1.4) (80,1.0)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Contrastive loss decreasing during training (illustrative).}
%   \label{fig:ntxent}
% \end{figure}

% \subsection{References}

% Contrastive learning frameworks (SimCLR, MoCo, BYOL) underpin modern representation learning \textcite{Chen2020,Prince2023}.
