% Chapter 15, Section 4

\section{Contrastive Learning \difficultyInline{intermediate}}
\label{sec:contrastive-learning}

Contrastive learning learns representations by contrasting positive and negative pairs, where the model learns to pull similar examples together and push dissimilar examples apart in the representation space, enabling the discovery of meaningful semantic structure without explicit supervision.

\subsection{Core Idea}

The core idea of contrastive learning is to maximize agreement between different views of the same data (positive pairs) while minimizing agreement with other data (negative pairs), where this approach forces the model to learn representations that are invariant to irrelevant transformations while being sensitive to semantic differences. This is achieved by training the model to distinguish between positive pairs that should have similar representations and negative pairs that should have different representations, where the contrastive loss encourages the model to learn a representation space where similar examples are close together and dissimilar examples are far apart. The key insight is that by learning to distinguish between positive and negative pairs, the model naturally learns to capture the underlying semantic structure of the data without requiring explicit labels.

\subsection{SimCLR Framework}

The SimCLR framework applies two random augmentations to each image, encodes both views using the same encoder network $\vect{z}_i = f(\vect{x}_i)$ and $\vect{z}_j = f(\vect{x}_j)$, and minimizes the contrastive loss (NT-Xent) to learn invariances.
\begin{equation}
\ell_{i,j} = -\log \frac{\exp(\mathrm{sim}(\vect{z}_i, \vect{z}_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{I}[k \neq i] \, \exp(\mathrm{sim}(\vect{z}_i, \vect{z}_k)/\tau)}.
\end{equation}
\noindent\textbf{Variables.}
\begin{itemize}[leftmargin=1.5em]
  \item $\vect{z}_i,\vect{z}_j$: embeddings of two augmented views of the same image.
  \item $\mathrm{sim}(\cdot,\cdot)$: cosine similarity between normalized embeddings.
  \item $\tau>0$: temperature scaling the softness of the distribution.
  \item $2N$: number of views in a batch (two per original sample); negatives are the other $2N-2$ views.
  \item $\mathbb{I}[\cdot]$: indicator that excludes the anchor itself from the denominator.
\end{itemize}

\subsection{MoCo (Momentum Contrast)}

MoCo (Momentum Contrast) uses a momentum encoder and a queue of negative samples for efficiency, where the momentum encoder is updated using exponential moving average of the main encoder parameters to maintain consistency in the representation space. The queue stores negative samples from previous batches to provide a large and diverse set of negative examples without requiring large batch sizes, where this approach enables efficient contrastive learning with limited computational resources while maintaining high-quality representations.

\subsection{BYOL (Bootstrap Your Own Latent)}

BYOL (Bootstrap Your Own Latent) surprisingly works without negative samples by using an online network that is updated by gradients, a target network that is updated using momentum, and a prediction head on the online network that learns to predict the target network's output. This approach avoids the need for negative samples by learning to predict the target representation from the online representation, where the target network provides a moving target that encourages the online network to learn consistent representations across different augmentations.

\subsection{Applications}

Contrastive learning has achieved state-of-the-art results in image classification where the learned representations transfer well to downstream classification tasks, object detection where the learned features enable accurate localization and classification of objects, and segmentation where the learned representations capture fine-grained spatial information needed for pixel-level predictions. The approach is particularly valuable for medical imaging with limited labels, where the learned representations can be fine-tuned with minimal labeled data while maintaining high performance on diagnostic tasks.


% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (contrastive)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Epoch}, ylabel={NT-Xent loss}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1,6.0) (10,3.2) (20,2.1) (40,1.4) (80,1.0)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Contrastive loss decreasing during training (illustrative).}
%   \label{fig:ntxent}
% \end{figure}

% \subsection{References}

% Contrastive learning frameworks (SimCLR, MoCo, BYOL) underpin modern representation learning \textcite{Chen2020,Prince2023}.
