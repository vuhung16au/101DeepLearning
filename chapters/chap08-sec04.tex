% Chapter 8, Section 4

\section{Second-Order Methods \difficultyInline{intermediate}}
\label{sec:second-order}

Second-order methods leverage curvature information from the Hessian or Fisher matrix to take more informed steps, though computational costs often limit their applicability to smaller models.

\subsection{Intuition: Curvature-Aware Steps}

First-order methods follow the slope; second-order methods also consider the \textit{curvature} of the landscape to choose better-scaled steps. If the valley is sharply curved in one direction and flat in another, curvature-aware steps shorten strides along the sharp direction and lengthen them along the flat one.\index{second-order methods}

Historical note: Classical optimization popularized Newton and quasi-Newton methods; in deep learning, memory and compute constraints motivated approximations like L-BFGS and natural gradient \cite{LiuNocedal1989,Amari1998,GoodfellowEtAl2016,Bishop2006}.

\subsection{Newton's Method}

Uses second-order Taylor expansion:
\begin{equation}
\vect{\theta}_{t+1} = \vect{\theta}_t - \mat{H}^{-1} \nabla_{\vect{\theta}} L(\vect{\theta}_t)
\end{equation}

where $\mat{H}$ is the Hessian matrix. Intuitively, the Hessian rescales the gradient by local curvature, yielding steps invariant to axis scaling in quadratic bowls (see Figure~\ref{fig:newton-bowl}).

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
  % Quadratic bowl as concentric ellipses
  \foreach \a/\b in {2.2/1.3, 1.8/1.05, 1.4/0.85, 1.0/0.65, 0.6/0.45} {
    \draw[bookpurple!60] (0,0) ellipse ({\a} and {\b});
  }
  % Current point
  \fill[bookpurple] (-1.2,0.7) circle (1.5pt);
  % Gradient step (steepest descent, unscaled)
  \draw[->, thick, bookpurple!60] (-1.2,0.7) -- ++(0.35,-0.85) node[below right, xshift=1pt, yshift=-2pt] {\small gradient step};
  % Newton step (curvature scaled)
  \draw[->, thick, bookred!70] (-1.2,0.7) -- ++(0.95,-0.35) node[right, xshift=2pt] {\small Newton step};
  % Optimum
  \fill[bookblack] (0,0) circle (1.2pt) node[below right] {\small $\theta^*$};
\end{tikzpicture}
\caption{Newton's method rescales the gradient by curvature, taking longer steps along shallow directions and shorter steps along steep directions.}
\label{fig:newton-bowl}
\end{figure}

Newton's method faces significant computational challenges.\index{Newton's method} Computing the Hessian is $O(n^2)$ in the number of parameters, and inverting it is $O(n^3)$. These costs render Newton's method infeasible for large neural networks with millions or billions of parameters.

\subsection{Quasi-Newton Methods}\index{L-BFGS}

Approximate the Hessian inverse:

\textbf{L-BFGS} (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) maintains a low-rank approximation of the Hessian inverse, making it more efficient than full Newton's method. However, L-BFGS remains expensive for very large models and is primarily used for smaller networks or specific applications where its convergence properties justify the computational cost.

Historical note: Quasi-Newton methods, notably BFGS and its limited-memory variant L-BFGS \cite{LiuNocedal1989}, performed well on moderate-sized networks and remain valuable for fine-tuning smaller models or optimizing differentiable components inside larger systems \cite{GoodfellowEtAl2016,Bishop2006}.

\subsection{Natural Gradient}\index{natural gradient}

Uses Fisher information matrix instead of Hessian:
\begin{equation}
\vect{\theta}_{t+1} = \vect{\theta}_t - \alpha \mat{F}^{-1} \nabla_{\vect{\theta}} L(\vect{\theta}_t)
\end{equation}

Provides parameter updates invariant to reparameterization. In probabilistic models, \(\mat{F}\) is the Fisher information, defining a Riemannian metric on the parameter manifold; stepping along \(\mat{F}^{-1}\nabla L\) follows the steepest descent in information geometry \cite{Amari1998}. Approximations (e.g., K-FAC) make natural gradient practical in deep nets by exploiting layer structure (see Figure~\ref{fig:natural-gradient}).

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
  % Anisotropic contours (level sets)
  \foreach \a/\b in {2.2/0.9, 1.8/0.75, 1.4/0.62, 1.0/0.5, 0.6/0.38} {
    \draw[bookpurple!60, rotate=25] (0,0) ellipse ({\a} and {\b});
  }
  % Current point
  \fill[bookpurple] (-1.0,0.3) circle (1.5pt);
  % Euclidean gradient step (blue)
  \draw[->, thick, bookpurple!70] (-1.0,0.3) -- ++(0.25,-0.55) node[below right, xshift=1pt, yshift=-1pt] {\small Euclidean};
  % Natural gradient step (green), rotates/scales toward true steepest path under metric
  \draw[->, thick, bookred!80] (-1.0,0.3) -- ++(0.6,-0.25) node[right, xshift=2pt] {\small Natural};
  % Optimum
  \fill[bookblack] (0,0) circle (1.2pt) node[below right] {\small $\theta^*$};
\end{tikzpicture}
\caption{Natural gradient accounts for the local geometry (Fisher metric), often directing updates more orthogonally to level sets than Euclidean gradient.}
\label{fig:natural-gradient}
\end{figure}

