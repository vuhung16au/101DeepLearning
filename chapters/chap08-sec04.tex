% Chapter 8, Section 4

\section{Second-Order Methods \difficultyInline{intermediate}}
\label{sec:second-order}

Second-order methods utilize curvature information from the Hessian matrix to make more informed optimization steps, providing faster convergence than first-order methods but requiring significant computational resources for large neural networks.

\subsection{Intuition: Curvature-Aware Steps}

First-order methods follow the slope; second-order methods also consider the \textit{curvature} of the landscape to choose better-scaled steps. If the valley is sharply curved in one direction and flat in another, curvature-aware steps shorten strides along the sharp direction and lengthen them along the flat one.\index{second-order methods}

Historical note: Classical optimization popularized Newton and quasi-Newton methods; in deep learning, memory and compute constraints motivated approximations like L-BFGS and natural gradient \cite{LiuNocedal1989,Amari1998,GoodfellowEtAl2016,Bishop2006}.

\subsection{Newton's Method}

Newton's method uses second-order Taylor expansion to make curvature-aware optimization steps, providing theoretically optimal convergence rates for quadratic functions. The mathematical formulation incorporates the Hessian matrix to rescale gradients according to local curvature, yielding steps that are invariant to parameter scaling.

The update rule incorporates second-order information:
\begin{equation}
\vect{\theta}_{t+1} = \vect{\theta}_t - \mat{H}^{-1} \nabla_{\vect{\theta}} L(\vect{\theta}_t)
\end{equation}

where $\mat{H}$ is the Hessian matrix containing second-order derivatives. The Hessian rescales the gradient by local curvature, yielding steps that naturally adapt to the geometry of the loss landscape. In quadratic bowls, this approach takes longer steps along shallow directions and shorter steps along steep directions, providing optimal convergence properties.

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
  % Quadratic bowl as concentric ellipses
  \foreach \a/\b in {2.2/1.3, 1.8/1.05, 1.4/0.85, 1.0/0.65, 0.6/0.45} {
    \draw[bookpurple!60] (0,0) ellipse ({\a} and {\b});
  }
  % Current point
  \fill[bookpurple] (-1.2,0.7) circle (1.5pt);
  % Gradient step (steepest descent, unscaled)
  \draw[->, thick, bookpurple!60] (-1.2,0.7) -- ++(0.35,-0.85) node[below right, xshift=1pt, yshift=-2pt] {\small gradient step};
  % Newton step (curvature scaled)
  \draw[->, thick, bookred!70] (-1.2,0.7) -- ++(0.95,-0.35) node[right, xshift=2pt] {\small Newton step};
  % Optimum
  \fill[bookblack] (0,0) circle (1.2pt) node[below right] {\small $\theta^*$};
\end{tikzpicture}
\caption{Newton's method rescales gradient by curvature: longer steps in shallow, shorter in steep.}
\label{fig:newton-bowl}
\end{figure}

Despite its theoretical advantages, Newton's method faces significant computational challenges that limit its practical application in deep learning. Computing the Hessian matrix requires $O(n^2)$ operations in the number of parameters, while inverting the Hessian demands $O(n^3)$ operations, making the approach computationally infeasible for large neural networks with millions of parameters. These computational requirements grow quadratically and cubically with the number of parameters, respectively, creating prohibitive memory and time costs for modern deep learning architectures.\index{Newton's method}

\subsection{Quasi-Newton Methods}\index{L-BFGS}

Quasi-Newton methods approximate the Hessian inverse using iterative updates that avoid the computational burden of computing and inverting the full Hessian matrix. These approaches maintain low-rank approximations of the Hessian inverse, providing a practical compromise between computational efficiency and second-order optimization benefits.

The L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) algorithm represents the most widely used quasi-Newton method in deep learning, maintaining a low-rank approximation of the Hessian inverse that is significantly more efficient than full Newton's method. While still computationally expensive for very large models, L-BFGS remains valuable for smaller networks or specific applications where second-order information provides substantial benefits. The limited-memory approach stores only recent gradient differences, making it feasible for moderate-sized neural networks while preserving much of the convergence advantages of second-order methods.

Historical note: Quasi-Newton methods, notably BFGS and its limited-memory variant L-BFGS \cite{LiuNocedal1989}, performed well on moderate-sized networks and remain valuable for fine-tuning smaller models or optimizing differentiable components inside larger systems \cite{GoodfellowEtAl2016,Bishop2006}.

\subsection{Natural Gradient}\index{natural gradient}

Uses Fisher information matrix instead of Hessian:
\begin{equation}
\vect{\theta}_{t+1} = \vect{\theta}_t - \alpha \mat{F}^{-1} \nabla_{\vect{\theta}} L(\vect{\theta}_t)
\end{equation}

Provides parameter updates invariant to reparameterization. In probabilistic models, \(\mat{F}\) is the Fisher information, defining a Riemannian metric on the parameter manifold; stepping along \(\mat{F}^{-1}\nabla L\) follows the steepest descent in information geometry \cite{Amari1998}. Approximations (e.g., K-FAC) make natural gradient practical in deep nets by exploiting layer structure (see Figure~\ref{fig:natural-gradient}).

\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.0]
  % Anisotropic contours (level sets)
  \foreach \a/\b in {2.2/0.9, 1.8/0.75, 1.4/0.62, 1.0/0.5, 0.6/0.38} {
    \draw[bookpurple!60, rotate=25] (0,0) ellipse ({\a} and {\b});
  }
  % Current point
  \fill[bookpurple] (-1.0,0.3) circle (1.5pt);
  % Euclidean gradient step (blue)
  \draw[->, thick, bookpurple!70] (-1.0,0.3) -- ++(0.25,-0.55) node[below right, xshift=1pt, yshift=-1pt] {\small Euclidean};
  % Natural gradient step (green), rotates/scales toward true steepest path under metric
  \draw[->, thick, bookred!80] (-1.0,0.3) -- ++(0.6,-0.25) node[right, xshift=2pt] {\small Natural};
  % Optimum
  \fill[bookblack] (0,0) circle (1.2pt) node[below right] {\small $\theta^*$};
\end{tikzpicture}
\caption{Natural gradient accounts for local geometry (Fisher metric), directing updates orthogonally.}
\label{fig:natural-gradient}
\end{figure}

