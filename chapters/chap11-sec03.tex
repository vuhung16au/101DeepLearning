% Chapter 11, Section 3

\section{Hyperparameter Tuning \difficultyInline{intermediate}}
\label{sec:hyperparameter-tuning}

\textbf{Hyperparameter tuning} is the process of selecting optimal configuration settings that control how a machine learning model learns, rather than the parameters the model learns itself. Unlike classical ML algorithms like linear regression or decision trees that have few, well-understood hyperparameters, deep learning models have dozens of hyperparameters that interact in complex ways, making tuning much more challenging and critical for success.

\paragraph{Metaphor.} Think of hyperparameter tuning like tuning a musical instrument—classical ML is like tuning a simple guitar with just a few strings, where each adjustment has a clear, predictable effect. Deep learning is like tuning a complex orchestra with dozens of instruments, where changing one instrument's tuning affects how all the others sound together, and the perfect harmony requires careful coordination of many interdependent settings.

\index{classical ML}\index{deep learning}

\subsection{Key Hyperparameters (Priority Order)}

Effective tuning prioritizes learning rate, regularization, and capacity before fine details \index{hyperparameter tuning}. This systematic approach prevents wasting time on minor optimizations when fundamental issues remain unresolved—for example, tuning dropout rates while using a learning rate that's 10x too high will yield poor results regardless of regularization choices. Treat the validation set as your instrumentation layer and control randomness via fixed seeds \textcite{GoodfellowEtAl2016,Prince2023,D2LChapterOptimization}.

\begin{table}[h]
\centering
\begin{tabular}{|c|p{0.25\textwidth}|p{0.55\textwidth}|}
\hline
\textbf{Priority} & \textbf{Hyperparameter} & \textbf{Key Considerations} \\
\hline
1 & Learning rate & Most critical; consider warmup and cosine decay \\
\hline
2 & Network architecture & Depth/width, normalization, residuals \\
\hline
3 & Batch size & Affects noise scale and generalization \\
\hline
4 & Regularization & Weight decay, dropout, label smoothing \\
\hline
5 & Optimizer parameters & Momentum, $\beta$ values in Adam \\
\hline
\end{tabular}
\caption{Priority order for hyperparameter tuning in deep learning.}
\label{tab:hyperparameter-priority}
\end{table}

\subsection{Search Strategies}

\textbf{Manual Search} involves human-guided exploration of hyperparameter space based on domain knowledge and intuition. Practitioners typically start with educated guesses drawn from hyperparameter values that have worked well in similar problems or are recommended in literature—for example, starting with a learning rate of 0.001 for Adam optimizer or using 0.5 dropout rate for fully connected layers, as these are commonly successful starting points across many deep learning tasks. The process then involves systematically modifying hyperparameters based on validation performance, typically changing one parameter at a time to understand its individual effect: if validation loss plateaus, try increasing learning rate; if overfitting occurs, increase regularization strength or reduce model capacity. While manual search requires significant human effort and can take days or weeks, it provides deep understanding of how different hyperparameters affect model behaviour, building intuition that proves valuable for future projects and helping identify the most promising regions of hyperparameter space.

\textbf{Grid Search} systematically evaluates all combinations of hyperparameters from predefined discrete sets. The approach involves defining a grid of possible values for each hyperparameter—for example, learning rates [0.001, 0.01, 0.1] and batch sizes [32, 64, 128]—and training a model for every possible combination, ensuring comprehensive coverage of the specified search space without missing any potential configurations. However, grid search guarantees finding the best combination within the defined grid at the cost of exponentially growing computational requirements: for 3 hyperparameters with 5 values each, you need 125 training runs, making it computationally prohibitive for large models or extensive search spaces. Consequently, grid search works best when you have few hyperparameters to tune (2-3), as the search space remains manageable; with more than 3-4 hyperparameters, the curse of dimensionality makes grid search impractical, and most combinations will likely be suboptimal anyway.

\textbf{Random Search} samples hyperparameters from probability distributions rather than evaluating all combinations. Instead of testing every combination, practitioners randomly sample hyperparameter values from appropriate distributions—for example, learning rate from log-uniform distribution, dropout from uniform distribution—exploring the search space more efficiently by avoiding the rigid structure of grid search. Random search often finds good hyperparameters with fewer trials than grid search because it doesn't waste time on systematically poor regions of the search space; studies show that random search can achieve similar performance to grid search with 10-100x fewer evaluations, making it much more practical for expensive training procedures. As the number of hyperparameters increases, random search becomes increasingly advantageous over grid search, particularly because in high-dimensional spaces most of the volume lies near the boundaries, and random sampling naturally explores these regions more effectively than the structured approach of grid search.

\textbf{Bayesian Optimization} uses probabilistic models to guide hyperparameter search intelligently. The approach builds a probabilistic model (typically Gaussian Process) that predicts the performance of untested hyperparameter configurations based on previous evaluations, capturing both the expected performance and uncertainty to enable informed decisions about where to search next. Instead of random sampling, Bayesian optimization uses acquisition functions (like Expected Improvement or Upper Confidence Bound) to select the most promising hyperparameter configurations to evaluate next, balancing exploration of uncertain regions with exploitation of areas likely to contain good solutions. This intelligent guidance typically requires far fewer evaluations than random or grid search to find good hyperparameters, especially when each evaluation is expensive, making it particularly valuable for neural architecture search or when training large models where each hyperparameter trial might take hours or days to complete.

\subsection{Best Practices}

Effective hyperparameter tuning requires systematic approaches that balance thoroughness with computational efficiency while maintaining scientific rigour. These best practices help practitioners avoid common pitfalls and maximise the value of their tuning efforts.

When searching for optimal learning rates, use logarithmic scale and sweep over the range $[10^{-5},10^{-1}]$ because learning rates span several orders of magnitude, and linear spacing would miss critical regions where small changes have dramatic effects. For example, the difference between 0.001 and 0.01 can mean the difference between convergence and divergence, while the difference between 0.1 and 0.11 is usually negligible; logarithmic sampling ensures equal attention to each order of magnitude, capturing the full range of potentially useful learning rates.

When varying batch size, adjust learning rate proportionally, as larger batch sizes provide more stable gradients but require higher learning rates to maintain the same effective step size. When doubling batch size from 32 to 64, increase learning rate by approximately 2x to maintain similar convergence dynamics—this relationship stems from the fact that larger batches reduce gradient noise, allowing for more aggressive updates without destabilising training.

Track results with a consistent random seed and multiple repeats, as deep learning results can vary significantly due to random initialisation and data shuffling, making single runs unreliable for hyperparameter comparison. Use fixed seeds for reproducibility and run multiple trials (3-5) to estimate the variance and ensure that performance differences are statistically significant rather than due to random chance.

Early-stop poor runs and allocate budget adaptively rather than running every hyperparameter configuration to completion. Monitor training progress and terminate clearly failing experiments early: if a configuration shows no improvement after 20\% of the planned training time, stop it and redirect computational resources to more promising candidates. This adaptive allocation can reduce total tuning time by 50-70\% while focusing effort on the most promising regions of hyperparameter space.

Use a fixed validation protocol to avoid leakage by establishing a single, immutable validation split before beginning any hyperparameter tuning. Changing validation splits during tuning can lead to overly optimistic estimates and poor generalisation, so the validation set should remain completely untouched until the final evaluation, with all hyperparameter decisions based on this consistent benchmark.

Finally, retrain with the best setting on train+val and report on held-out test: after identifying the best hyperparameters, retrain the model using both training and validation data to maximise the information available for learning, then report final performance on a completely held-out test set that was never used for any hyperparameter decisions. This two-stage approach ensures that the final model uses all available training data whilst maintaining an unbiased estimate of true generalisation performance.

\index{manual search}\index{grid search}\index{random search}\index{Bayesian optimization}\index{batch size}\index{learning rate}\index{early stopping}\index{two-stage training}

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (tuning)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,
%       height=0.36\textwidth,
%       xmode=log, log basis x=10,
%       xlabel={Learning rate}, ylabel={Val. loss}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1e-5,0.90) (3e-5,0.80) (1e-4,0.70) (3e-4,0.62) (1e-3,0.60) (3e-3,0.95) (1e-2,2.0)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Learning-rate sweep identifies a stable operating region.}
%   \label{fig:lr-range-test}
% \end{figure}

\subsection{Historical notes}

The scaling of deep learning models and search spaces necessitated a move beyond rudimentary optimization methods, driving the evolution from simple grid search to more sophisticated hyperparameter tuning strategies. The challenge of efficiently finding optimal hyperparameters in high-dimensional spaces led to the rise of Random Search and Bayesian Optimization, offering superior coverage and effectiveness compared to exhaustive grid searches that became computationally prohibitive. For the stability of model weights during the immense computation required for large language models and Transformers, learning-rate schedules became a standard component of modern training pipelines. Specifically, practices like Warmup ensure that training begins with a small learning rate to stabilize early-stage gradients before gradually increasing it, while techniques such as Step Decay or Cosine Annealing then regulate the descent toward the optimum in later phases. These scheduling mechanisms are essential for preventing gradient explosion and achieving reliable convergence across large batches and deep network architectures. The historical progression from manual tuning to automated optimization reflects the broader trend toward systematic, data-driven approaches in deep learning methodology \textcite{GoodfellowEtAl2016,Prince2023,D2LChapterOptimization}.

