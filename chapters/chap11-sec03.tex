% Chapter 11, Section 3

\section{Hyperparameter Tuning \difficultyInline{intermediate}}
\label{sec:hyperparameter-tuning}

\subsection{Key Hyperparameters (Priority Order)}

Effective tuning prioritizes learning rate, regularization, and capacity before fine details \index{hyperparameter tuning}. Treat the validation set as your instrumentation layer and control randomness via fixed seeds \textcite{GoodfellowEtAl2016,Prince2023,D2LChapterOptimization}.

\begin{enumerate}
    \item \textbf{Learning rate:} Most critical; consider warmup and cosine decay
    \item \textbf{Network architecture:} Depth/width, normalization, residuals
    \item \textbf{Batch size:} Affects noise scale and generalization
    \item \textbf{Regularization:} Weight decay, dropout, label smoothing
    \item \textbf{Optimizer parameters:} Momentum, $\beta$ values in Adam
\end{enumerate}

\subsection{Search Strategies}

\textbf{Manual Search:}
\begin{itemize}
    \item Start with educated guesses
    \item Adjust based on results
    \item Time-consuming but insightful
\end{itemize}

\textbf{Grid Search:}
\begin{itemize}
    \item Try all combinations from predefined values
    \item Exhaustive but expensive
    \item Better for 2-3 hyperparameters
\end{itemize}

\textbf{Random Search:}
\begin{itemize}
    \item Sample hyperparameters randomly
    \item More efficient than grid search
    \item Better for high-dimensional spaces
\end{itemize}

\textbf{Bayesian Optimization:}
\begin{itemize}
    \item Model hyperparameter performance
    \item Choose next trials intelligently
    \item More sample-efficient
\end{itemize}

\subsection{Best Practices}

\begin{itemize}
    \item Use logarithmic scale for learning rate; sweep $[10^{-5},10^{-1}]$
    \item Vary batch size and adjust learning rate proportionally
    \item Track results with a consistent random seed and multiple repeats
    \item Early-stop poor runs; allocate budget adaptively
    \item Use a fixed validation protocol to avoid leakage
    \item Retrain with best setting on train+val and report on held-out test
\end{itemize}

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (tuning)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,
%       height=0.36\textwidth,
%       xmode=log, log basis x=10,
%       xlabel={Learning rate}, ylabel={Val. loss}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1e-5,0.90) (3e-5,0.80) (1e-4,0.70) (3e-4,0.62) (1e-3,0.60) (3e-3,0.95) (1e-2,2.0)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Learning-rate sweep identifies a stable operating region.}
%   \label{fig:lr-range-test}
% \end{figure}

\subsection{Historical notes}

Random search and Bayesian optimization rose to prominence as models and search spaces grew large; they offer better coverage than grid for high-dimensional spaces. Practical DL texts emphasize learning-rate schedules (step, cosine) and warmup for stability \textcite{GoodfellowEtAl2016,Prince2023,D2LChapterOptimization}.

