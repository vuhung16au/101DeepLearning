% Chapter 11, Section 3

\section{Hyperparameter Tuning \difficultyInline{intermediate}}
\label{sec:hyperparameter-tuning}

\textbf{Hyperparameter tuning} is the process of selecting optimal configuration settings that control how a machine learning model learns, rather than the parameters the model learns itself. Unlike classical ML algorithms like linear regression or decision trees that have few, well-understood hyperparameters, deep learning models have dozens of hyperparameters that interact in complex ways, making tuning much more challenging and critical for success.

\paragraph{Metaphor.} Think of hyperparameter tuning like tuning a musical instrument—classical ML is like tuning a simple guitar with just a few strings, where each adjustment has a clear, predictable effect. Deep learning is like tuning a complex orchestra with dozens of instruments, where changing one instrument's tuning affects how all the others sound together, and the perfect harmony requires careful coordination of many interdependent settings.

\index{classical ML}\index{deep learning}

\subsection{Key Hyperparameters (Priority Order)}

Effective tuning prioritizes learning rate, regularization, and capacity before fine details \index{hyperparameter tuning}. This systematic approach prevents wasting time on minor optimizations when fundamental issues remain unresolved—for example, tuning dropout rates while using a learning rate that's 10x too high will yield poor results regardless of regularization choices. Treat the validation set as your instrumentation layer and control randomness via fixed seeds \textcite{GoodfellowEtAl2016,Prince2023,D2LChapterOptimization}.

\begin{table}[h]
\centering
\begin{tabular}{|c|p{0.6\textwidth}|p{0.2\textwidth}|}
\hline
\textbf{Priority} & \textbf{Hyperparameter} & \textbf{Key Considerations} \\
\hline
1 & Learning rate & Most critical; consider warmup and cosine decay \\
\hline
2 & Network architecture & Depth/width, normalization, residuals \\
\hline
3 & Batch size & Affects noise scale and generalization \\
\hline
4 & Regularization & Weight decay, dropout, label smoothing \\
\hline
5 & Optimizer parameters & Momentum, $\beta$ values in Adam \\
\hline
\end{tabular}
\caption{Priority order for hyperparameter tuning in deep learning.}
\label{tab:hyperparameter-priority}
\end{table}

\subsection{Search Strategies}

Manual search involves human-guided exploration of hyperparameter space based on domain knowledge and intuition, beginning with hyperparameter values that have worked well in similar problems or are recommended in literature, such as starting with a learning rate of 0.001 for Adam optimizer or using 0.5 dropout rate for fully connected layers, as these are commonly successful starting points across many deep learning tasks. The approach systematically modifies hyperparameters based on validation performance, typically changing one parameter at a time to understand its individual effect—if validation loss plateaus, try increasing learning rate; if overfitting occurs, increase regularization strength or reduce model capacity. While manual search requires significant human effort and can take days or weeks, it provides deep understanding of how different hyperparameters affect model behavior, building intuition that proves valuable for future projects and helping identify the most promising regions of hyperparameter space.

Grid search systematically evaluates all combinations of hyperparameters from predefined discrete sets, defining a grid of possible values for each hyperparameter (e.g., learning rates [0.001, 0.01, 0.1] and batch sizes [32, 64, 128]) and training a model for every possible combination, ensuring comprehensive coverage of the specified search space without missing any potential configurations. While grid search guarantees finding the best combination within the defined grid, computational cost grows exponentially with the number of hyperparameters—for 3 hyperparameters with 5 values each, you need 125 training runs, making it computationally prohibitive for large models or extensive search spaces. Grid search works well when you have few hyperparameters to tune, as the search space remains manageable, but with more than 3-4 hyperparameters, the curse of dimensionality makes grid search impractical, and most combinations will likely be suboptimal anyway.

Random search samples hyperparameters from probability distributions rather than evaluating all combinations, randomly sampling hyperparameter values from appropriate distributions (e.g., learning rate from log-uniform distribution, dropout from uniform distribution) to explore the search space more efficiently by avoiding the rigid structure of grid search. Random search often finds good hyperparameters with fewer trials than grid search because it doesn't waste time on systematically poor regions of the search space, with studies showing that random search can achieve similar performance to grid search with 10-100x fewer evaluations, making it much more practical for expensive training procedures. As the number of hyperparameters increases, random search becomes increasingly advantageous over grid search, since in high-dimensional spaces, most of the volume lies near the boundaries, and random sampling naturally explores these regions more effectively than the structured approach of grid search.

Bayesian optimization uses probabilistic models to guide hyperparameter search intelligently, building a probabilistic model (typically Gaussian Process) that predicts the performance of untested hyperparameter configurations based on previous evaluations, capturing both the expected performance and uncertainty to allow informed decisions about where to search next. Instead of random sampling, Bayesian optimization uses acquisition functions (like Expected Improvement or Upper Confidence Bound) to select the most promising hyperparameter configurations to evaluate next, balancing exploration of uncertain regions with exploitation of areas likely to contain good solutions. Bayesian optimization typically requires far fewer evaluations than random or grid search to find good hyperparameters, especially when each evaluation is expensive, making it particularly valuable for neural architecture search or when training large models, where each hyperparameter trial might take hours or days to complete.

\subsection{Best Practices}

Effective hyperparameter tuning requires systematic approaches that balance thoroughness with computational efficiency while maintaining scientific rigor, helping practitioners avoid common pitfalls and maximize the value of their tuning efforts. Use logarithmic scale for learning rate and sweep $[10^{-5},10^{-1}]$ because learning rates span several orders of magnitude, and linear spacing would miss critical regions where small changes have dramatic effects—for example, the difference between 0.001 and 0.01 can mean the difference between convergence and divergence, while the difference between 0.1 and 0.11 is usually negligible, with logarithmic sampling ensuring equal attention to each order of magnitude and capturing the full range of potentially useful learning rates.

Vary batch size and adjust learning rate proportionally since larger batch sizes provide more stable gradients but require higher learning rates to maintain the same effective step size, where doubling batch size from 32 to 64 requires increasing learning rate by approximately 2x to maintain similar convergence dynamics, a relationship that stems from the fact that larger batches reduce gradient noise, allowing for more aggressive updates without destabilizing training. Track results with a consistent random seed and multiple repeats because deep learning results can vary significantly due to random initialization and data shuffling, making single runs unreliable for hyperparameter comparison—use fixed seeds for reproducibility and run multiple trials (3-5) to estimate the variance and ensure that performance differences are statistically significant rather than due to random chance.

Early-stop poor runs and allocate budget adaptively by monitoring training progress and terminating clearly failing experiments early, where if a configuration shows no improvement after 20\% of the planned training time, stop it and redirect computational resources to more promising candidates, as this adaptive allocation can reduce total tuning time by 50-70\% while focusing effort on the most promising regions of hyperparameter space. Use a fixed validation protocol to avoid leakage by establishing a single, immutable validation split before beginning any hyperparameter tuning to prevent data leakage and overfitting to the validation set, since changing validation splits during tuning can lead to overly optimistic estimates and poor generalization, with the validation set remaining completely untouched until the final evaluation and all hyperparameter decisions based on this consistent benchmark.

Retrain with best setting on train+val and report on held-out test by identifying the best hyperparameters, then retraining the model using both training and validation data to maximize the information available for learning, and reporting final performance on a completely held-out test set that was never used for any hyperparameter decisions. This two-stage approach ensures that the final model uses all available training data while maintaining an unbiased estimate of true generalization performance.

\index{manual search}\index{grid search}\index{random search}\index{Bayesian optimization}\index{batch size}\index{learning rate}\index{early stopping}\index{two-stage training}

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (tuning)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,
%       height=0.36\textwidth,
%       xmode=log, log basis x=10,
%       xlabel={Learning rate}, ylabel={Val. loss}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1e-5,0.90) (3e-5,0.80) (1e-4,0.70) (3e-4,0.62) (1e-3,0.60) (3e-3,0.95) (1e-2,2.0)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Learning-rate sweep identifies a stable operating region.}
%   \label{fig:lr-range-test}
% \end{figure}

\subsection{Historical notes}

The scaling of deep learning models and search spaces necessitated a move beyond rudimentary optimization methods, driving the evolution from simple grid search to more sophisticated hyperparameter tuning strategies. The challenge of efficiently finding optimal hyperparameters in high-dimensional spaces led to the rise of Random Search and Bayesian Optimization, offering superior coverage and effectiveness compared to exhaustive grid searches that became computationally prohibitive. For the stability of model weights during the immense computation required for large language models and Transformers, learning-rate schedules became a standard component of modern training pipelines. Specifically, practices like Warmup ensure that training begins with a small learning rate to stabilize early-stage gradients before gradually increasing it, while techniques such as Step Decay or Cosine Annealing then regulate the descent toward the optimum in later phases. These scheduling mechanisms are essential for preventing gradient explosion and achieving reliable convergence across large batches and deep network architectures. The historical progression from manual tuning to automated optimization reflects the broader trend toward systematic, data-driven approaches in deep learning methodology \textcite{GoodfellowEtAl2016,Prince2023,D2LChapterOptimization}.

