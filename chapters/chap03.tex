% Chapter 3: Probability and Information Theory

\chapter{Probability and Information Theory}
\label{chap:probability}

This chapter introduces fundamental concepts from probability theory and information theory that are essential for understanding machine learning and deep learning. Topics include probability distributions, conditional probability, expectation, variance, entropy, and mutual information.

\begin{learningobjectives}
\objective{Probability foundations and intuitive meaning of discrete and continuous distributions}
\objective{Conditional probability and Bayes' theorem in machine learning algorithms}
\objective{Statistical measures: expectation, variance, and covariance of random variables}
\objective{Common probability distributions and their use in machine learning}
\objective{Information content using entropy, cross-entropy, and KL divergence}
\objective{Information theory concepts in loss functions and representation learning}
\end{learningobjectives}

\input{chapters/chap03-sec01}
\input{chapters/chap03-sec02}
\input{chapters/chap03-sec03}
\input{chapters/chap03-sec04}
\input{chapters/chap03-sec05}
% \input{chapters/chap03-sec06}

% Chapter summary and problems
\input{chapters/chap03-key-takeaways}
\input{chapters/chap03-exercises}
