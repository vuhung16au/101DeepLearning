% Chapter 3: Probability and Information Theory

\chapter{Probability and Information Theory}
\label{chap:probability}

This chapter introduces fundamental concepts from probability theory and information theory that are essential for understanding machine learning and deep learning. Topics include probability distributions, conditional probability, expectation, variance, entropy, and mutual information.

\begin{learningobjectives}
\objective{Probability foundations and grasp the intuitive meaning of probability distributions, both discrete and continuous, and how they model uncertainty in real-world scenarios}
\objective{Conditional probability and use Bayes' theorem to update beliefs with new evidence and understand its central role in machine learning algorithms}
\objective{Statistical measures and compute expectation, variance, and covariance to characterize the behavior of random variables and their relationships}
\objective{Common distributions and recognize when to use Bernoulli, Gaussian, and other probability distributions in machine learning contexts}
\objective{Information content and use entropy, cross-entropy, and KL divergence to measure uncertainty and information in data and models}
\objective{Information theory and connect information-theoretic concepts to loss functions, model selection, and representation learning in deep neural networks}
\end{learningobjectives}

\input{chapters/chap03-sec01}
\input{chapters/chap03-sec02}
\input{chapters/chap03-sec03}
\input{chapters/chap03-sec04}
\input{chapters/chap03-sec05}
% \input{chapters/chap03-sec06}

% Chapter summary and problems
\input{chapters/chap03-key-takeaways}
\input{chapters/chap03-exercises}
