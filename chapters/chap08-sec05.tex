% Chapter 8, Section 5

\section{Optimization Challenges \difficultyInline{intermediate}}
\label{sec:challenges}

\subsection{Intuition: Why Training Gets Stuck}

Deep networks combine nonlinearity and depth, creating landscapes with flat plateaus, narrow valleys, and saddle points. Noise (SGD), momentum, and schedules act like navigational aids to keep moving and avoid getting stuck.\index{optimization!challenges}

\subsection{Vanishing and Exploding Gradients}

In deep networks, gradients can become exponentially small or large. See \gls{vanishing-gradient} and \gls{exploding-gradient}.

\textbf{Vanishing gradients:}
\begin{itemize}
    \item Common with sigmoid/tanh activations
    \item Mitigated by ReLU, batch normalization, residual connections
\end{itemize}

\textbf{Exploding gradients:}
\begin{itemize}
    \item Common in RNNs
    \item Mitigated by gradient clipping, careful initialization
\end{itemize}

\textbf{Gradient clipping:}\index{gradient clipping}
\begin{equation}
\vect{g} \leftarrow \frac{\vect{g}}{\max(1, \|\vect{g}\| / \theta)}
\end{equation}

\subsection{Local Minima and Saddle Points}\index{saddle point}

In high dimensions, saddle points are more common than local minima.

Saddle points have:
\begin{itemize}
    \item Zero gradient
    \item Mixed curvature (positive and negative eigenvalues)
\end{itemize}

Momentum and noise help escape saddle points.

\subsection{Plateaus}\index{plateau}

Flat regions with small gradients slow convergence. Adaptive methods and learning rate schedules help navigate plateaus.

\subsection{Practical Optimization Strategy}

\textbf{Recommended approach:}
\begin{enumerate}
    \item Start with Adam \cite{Kingma2014} for rapid progress; tune $\alpha$ in $\{10^{-3},3\cdot10^{-4},10^{-4}\}$.
    \item Use cosine decay with warmup for transformer-like models; step decay for CNNs with SGD+momentum \cite{WebOptimizationDLBook,D2LChapterOptimization,He2016}.
    \item If validation accuracy saturates, consider switching from Adam to SGD+Nesterov with tuned $\alpha$ and $\beta$ to improve generalization.
    \item Apply gradient clipping in recurrent models and when training becomes unstable.\index{gradient clipping}
    \item Monitor training/validation loss, accuracy, and learning-rate schedule. Use early stopping when needed.\index{early stopping}
\end{enumerate}

Applications and heuristics:
\begin{itemize}
    \item Vision: SGD+momentum or Nesterov often yields state-of-the-art with careful schedules and augmentations \cite{He2016}.
    \item NLP/Transformers: Adam/AdamW with warmup+cosine is a strong default; clip global norm in seq2seq models.
    \item Reinforcement learning: Adam with small $\alpha$ stabilizes non-stationary objectives.
\end{itemize}

Common failure modes:
\begin{itemize}
    \item Divergence at start: reduce $\alpha$, add warmup, or increase \(\epsilon\) for Adam.
    \item Plateau: try larger batch with warmup, use cosine schedule, or add momentum.
    \item Overfitting: increase regularization (weight decay, dropout), add data augmentation.
\end{itemize}
