% Chapter 8, Section 5

\section{Optimization Challenges \difficultyInline{intermediate}}
\label{sec:challenges}

Deep neural networks present unique optimisation challenges including vanishing or exploding gradients, saddle points, and plateaus, each requiring specific mitigation strategies.

\subsection{Intuition: Why Training Gets Stuck}

Deep networks combine nonlinearity and depth, creating landscapes with flat plateaus, narrow valleys, and saddle points. Noise (SGD), momentum, and schedules act like navigational aids to keep moving and avoid getting stuck.\index{optimization!challenges}

\subsection{Vanishing and Exploding Gradients}

In deep networks, gradients can become exponentially small or large. See \gls{vanishing-gradient} and \gls{exploding-gradient}.

\textbf{Vanishing gradients} are particularly common with sigmoid or tanh activation functions and can be mitigated through the use of ReLU activations, batch normalisation, and residual connections.

\textbf{Exploding gradients} occur frequently in recurrent neural networks and can be addressed through gradient clipping and careful weight initialisation strategies.

\textbf{Gradient clipping:}\index{gradient clipping}
\begin{equation}
\vect{g} \leftarrow \frac{\vect{g}}{\max(1, \|\vect{g}\| / \theta)}
\end{equation}

\subsection{Local Minima and Saddle Points}\index{saddle point}

In high dimensions, saddle points are more common than local minima.

Saddle points are characterised by zero gradients and mixed curvature (with both positive and negative eigenvalues of the Hessian). Momentum and stochastic noise from SGD help the optimiser escape these saddle points effectively.

\subsubsection{Example: Critical Points in Two Dimensions}

Consider the function $f(x,y) = x^4 - 2x^2 + y^2$ to illustrate different types of critical points:

\textbf{Local minimum:} Bottom of a bowl - you can't go lower in any direction
\begin{itemize}
    \item At $(0,0)$: $f(0,0) = 0$ is a local minimum
    \item All nearby points have higher function values
    \item Gradient is zero: $\nabla f = (4x^3 - 4x, 2y) = (0,0)$
    \item Hessian has positive eigenvalues (concave up in all directions)
\end{itemize}

\textbf{Local maximum:} Top of a hill - you can't go higher in any direction
\begin{itemize}
    \item At $(0,0)$ for $f(x,y) = -x^4 + 2x^2 - y^2$: would be a local maximum
    \item All nearby points have lower function values
    \item Hessian has negative eigenvalues (concave down in all directions)
\end{itemize}

\textbf{Saddle point:} Mountain pass - you can go down in some directions, up in others
\begin{itemize}
    \item At $(1,0)$ and $(-1,0)$: $f(\pm 1,0) = -1$ are saddle points
    \item Gradient is zero: $\nabla f = (4x^3 - 4x, 2y) = (0,0)$
    \item Hessian has mixed eigenvalues (concave up in one direction, down in another)
    \item Like sitting on a horse saddle: you can slide down the sides but the saddle curves up in front/back
\end{itemize}

In high-dimensional optimization, saddle points are much more common than local minima, making them the primary challenge for gradient-based methods.

\subsection{Plateaus}\index{plateau}

Flat regions with small gradients slow convergence. Adaptive methods and learning rate schedules help navigate plateaus.

\subsection{Practical Optimization Strategy}

\textbf{Recommended approach:} Begin with Adam \cite{Kingma2014} for rapid progress, tuning the learning rate $\alpha$ within $\{10^{-3},3\cdot10^{-4},10^{-4}\}$. For transformer-like models, use cosine decay with warmup; for CNNs trained with SGD and momentum, step decay is often more effective \cite{WebOptimizationDLBook,D2LChapterOptimization,He2016}. If validation accuracy saturates, consider switching from Adam to SGD with Nesterov momentum, carefully tuning both $\alpha$ and $\beta$ to improve generalisation. Apply gradient clipping in recurrent models and whenever training becomes unstable.\index{gradient clipping} Throughout training, monitor both training and validation loss, accuracy, and the learning-rate schedule, employing early stopping when appropriate.\index{early stopping}

Different domains benefit from different optimisation strategies. In computer vision, SGD with momentum or Nesterov acceleration often yields state-of-the-art results when paired with careful schedules and data augmentations \cite{He2016}. For NLP and transformer models, Adam or AdamW with warmup and cosine annealing is a strong default choice, with global norm clipping recommended for sequence-to-sequence models. In reinforcement learning, Adam with a small learning rate $\alpha$ helps stabilise non-stationary objectives.

Several common failure modes have well-established remedies. Divergence at the start of training can often be addressed by reducing $\alpha$, adding warmup, or increasing the numerical stability parameter \(\epsilon\) for Adam. When training reaches a plateau, try using a larger batch size with warmup, switching to a cosine schedule, or adding momentum if not already present. Overfitting can be mitigated by increasing regularisation through weight decay or dropout, or by incorporating additional data augmentation.
