% Chapter 5, Section 04

\section{Decision Trees and Ensemble Methods \difficultyInline{intermediate}}
\label{sec:decision-trees}

\textbf{Decision trees} are intuitive, interpretable models that make predictions by asking a series of yes/no questions about the input features. While individual trees can be prone to overfitting, combining multiple trees through ensemble methods often leads to much better performance.

\subsection{Intuition and Motivation}

Think of a decision tree as a flowchart for making decisions, where to classify whether someone will buy a product, you might ask "Is their income > \$50k?" and if yes, ask "Are they under 30?" or if no, ask "Do they have children?" Each question splits the data into smaller, more homogeneous groups that become easier to classify. The key advantages of decision trees include interpretability where they are easy to understand and explain, no feature scaling requirements since they work with mixed data types, handling missing values by being able to deal with incomplete data, and being non-parametric with no assumptions about data distribution, making them flexible and robust across different problem domains.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.7]
\node[draw, rectangle, fill=bookpurple!20] (root) at (0,0) {Income > \$50k?};
\node[draw, rectangle, fill=bookpurple!20] (left) at (-3,-2) {Age < 30?};
\node[draw, rectangle, fill=bookpurple!20] (right) at (3,-2) {Has children?};
\node[draw, rectangle, fill=bookred!20] (ll) at (-4.5,-4) {Buy};
\node[draw, rectangle, fill=bookred!20] (lr) at (-1.5,-4) {Don't buy};
\node[draw, rectangle, fill=bookred!20] (rl) at (1.5,-4) {Buy};
\node[draw, rectangle, fill=bookred!20] (rr) at (4.5,-4) {Don't buy};

\draw[->] (root) -- (left) node[midway, left] {Yes};
\draw[->] (root) -- (right) node[midway, right] {No};
\draw[->] (left) -- (ll) node[midway, left] {Yes};
\draw[->] (left) -- (lr) node[midway, right] {No};
\draw[->] (right) -- (rl) node[midway, left] {Yes};
\draw[->] (right) -- (rr) node[midway, right] {No};
\end{tikzpicture}
\caption{A simple decision tree for predicting product purchases. Each internal node represents a decision, and leaf nodes represent the final prediction.}
\label{fig:decision-tree-example}
\end{figure}

\subsection{Decision Trees}

A decision tree recursively partitions the input space based on feature values, where the algorithm works by starting with all training examples at the root, finding the best feature and threshold to split on that maximizes information gain, creating child nodes for each split that represent the different outcomes, repeating recursively until a stopping criterion is met such as maximum depth or minimum samples per leaf, and assigning a prediction to each leaf node based on the majority class or mean value of the examples that reach that node.

\begin{example}[Decision Tree: "Will we go out?"]
Consider the decision of whether to go out based on four features:
\begin{itemize}
    \item \textbf{Is it raining?} (Yes/No)
    \item \textbf{Temperature} (Hot/Mild/Cold)
    \item \textbf{Traffic OK?} (Yes/No)
    \item \textbf{Anyone goes with me?} (Yes/No)
\end{itemize}

A decision tree might look like:
\begin{enumerate}
    \item \textbf{Is it raining?}
    \begin{itemize}
        \item \textbf{Yes} → \textbf{Don't go out} (regardless of other factors)
        \item \textbf{No} → Continue to next question
    \end{itemize}
    \item \textbf{Temperature?}
    \begin{itemize}
        \item \textbf{Hot} → \textbf{Don't go out} (too hot)
        \item \textbf{Mild/Cold} → Continue to next question
    \end{itemize}
    \item \textbf{Traffic OK?}
    \begin{itemize}
        \item \textbf{No} → \textbf{Don't go out} (traffic is bad)
        \item \textbf{Yes} → Continue to next question
    \end{itemize}
    \item \textbf{Anyone goes with me?}
    \begin{itemize}
        \item \textbf{Yes} → \textbf{Go out} (company makes it worthwhile)
        \item \textbf{No} → \textbf{Don't go out} (not worth going alone)
    \end{itemize}
\end{enumerate}

This tree shows how decision trees make sequential decisions, where each question splits the data based on the most important feature at that point, leading to a final prediction.
\end{example}

\subsubsection{Splitting Criteria}

The key question is: "Which feature and threshold should we use to split the data?" We want splits that create the most homogeneous child nodes.

\textbf{For classification:}

\textbf{Gini impurity:}
\begin{equation}
\text{Gini} = 1 - \sum_{k=1}^{K} p_k^2
\end{equation}

\textbf{Entropy:}
\begin{equation}
\text{Entropy} = -\sum_{k=1}^{K} p_k \log p_k
\end{equation}

\textbf{For regression:}
\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - \bar{y})^2
\end{equation}

where $p_k$ is the proportion of class $k$ examples in a node, and $\bar{y}$ is the mean target value.

\subsubsection{Information Gain}

The \textbf{information gain} measures how much a split reduces impurity:

\begin{equation}
\text{IG} = \text{Impurity(parent)} - \sum_{j} \frac{n_j}{n} \text{Impurity(child}_j\text{)}
\end{equation}

We choose the split that maximizes information gain.

\subsection{Random Forests}

Random forests address the overfitting problem of individual trees by combining multiple trees trained on different subsets of the data, where each tree is trained on a bootstrap sample of the original data and considers only a random subset of features at each split, creating diversity among the trees that reduces overfitting and improves generalization performance.

\subsubsection{Bootstrap Aggregating (Bagging)}

Random forests inject diversity through two complementary mechanisms. First, each tree is trained on a \emph{bootstrap sample}—a dataset created by sampling with replacement from the original training set—so trees see different subsets of examples and learn different decision boundaries. Second, at each split within a tree, the algorithm considers only a \emph{random subset of features} rather than all features, forcing trees to rely on different signals and preventing a few strong predictors from dominating every split. At inference time, the forest aggregates individual tree predictions—by averaging for regression or majority voting for classification—so the ensemble reduces variance relative to any single overfit tree while keeping low bias.

\begin{equation}
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} f_b(\vect{x})
\end{equation}

where $B$ is the number of trees.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
% Draw multiple trees
\foreach \x in {0,2,4,6} {
    \node[draw, rectangle, fill=bookpurple!20] (root\x) at (\x,0) {Tree \pgfmathparse{int(\x/2+1)}\pgfmathresult};
    \node[draw, rectangle, fill=bookred!20] (leaf1\x) at (\x-0.5,-1.5) {Leaf 1};
    \node[draw, rectangle, fill=bookred!20] (leaf2\x) at (\x+0.5,-1.5) {Leaf 2};
    \draw[->] (root\x) -- (leaf1\x);
    \draw[->] (root\x) -- (leaf2\x);
}

% Add ensemble prediction
\node[draw, rectangle, fill=bookred!40] (ensemble) at (3,-3) {Final Prediction};
\draw[->] (leaf10) -- (ensemble);
\draw[->] (leaf12) -- (ensemble);
\draw[->] (leaf14) -- (ensemble);
\draw[->] (leaf16) -- (ensemble);

\node at (3,-4) {Average/Vote};
\end{tikzpicture}
\caption{Random forests combine multiple decision trees. Each tree is trained on a different bootstrap sample and makes its own prediction. The final prediction is the average (regression) or majority vote (classification) of all trees.}
\label{fig:random-forest}
\end{figure}

\subsubsection{Advantages of Random Forests}

Random forests substantially reduce overfitting by averaging many de-correlated trees, which lowers variance without greatly increasing bias. They provide built-in \emph{feature importance} estimates that highlight which inputs most influence predictions, helping interpretation and feature selection. Because each tree is trained on a bootstrap sample, the ensemble is naturally robust to outliers and noisy examples that might mislead a single tree. Training is trivially parallelizable since trees are independent, and, like decision trees, random forests require no feature scaling and handle mixed data types gracefully.

\subsection{Gradient Boosting}

Gradient boosting builds an ensemble sequentially, where each new tree corrects the errors of the previous ensemble by learning to predict the residuals of the current model, unlike random forests where trees are trained independently. This sequential approach allows the ensemble to focus on the most difficult examples and gradually improve its performance through iterative refinement.

\subsubsection{Algorithm}

\begin{algorithm}[htbp]
\caption{Gradient Boosting Algorithm}
\label{alg:gradient-boosting}
\begin{algorithmic}[1]
\State Initialize $\hat{y}^{(0)} = \frac{1}{n} \sum_{i=1}^{n} y^{(i)}$ \Comment{Start with mean for regression}
\For{$m = 1$ to $M$}
    \State Compute residuals: $r_i^{(m)} = y^{(i)} - \hat{y}^{(m-1)}(\vect{x}^{(i)})$ for $i = 1, \ldots, n$
    \State Fit tree $f_m$ to residuals $\{(\vect{x}^{(i)}, r_i^{(m)})\}_{i=1}^{n}$
    \State Update: $\hat{y}^{(m)} = \hat{y}^{(m-1)} + \nu f_m(\vect{x}^{(i)})$ for all $i$
\EndFor
\State \textbf{Return} final ensemble $\hat{y}^{(M)}$
\end{algorithmic}
\end{algorithm}

where $\nu$ is the learning rate (shrinkage parameter) and $M$ is the number of boosting iterations.

\subsubsection{Intuition}

Gradient boosting works by starting with a simple model like the mean for regression, calculating the errors (residuals) of the current model to identify where it fails, training a new model to predict these errors and correct the mistakes, adding the new model to the ensemble with a small weight to avoid overfitting, and repeating this process until convergence where the ensemble can no longer be improved significantly.

\begin{example}
For regression with target values $[10, 20, 30, 40]$:

\begin{algorithmic}[1]
\State Initial prediction: $\hat{y}^{(0)} = 25$ \Comment{mean of targets}
\State Residuals: $[-15, -5, 5, 15]$ \Comment{$r_i = y^{(i)} - \hat{y}^{(0)}$}
\State Fit a small tree to residuals $\{(\vect{x}^{(i)}, r_i)\}$
\State Update ensemble: $\hat{y}^{(1)}(\vect{x}) = 25 + \nu\, f_1(\vect{x})$
\State Recompute residuals and repeat for $m=2,\dots,M$
\end{algorithmic}
\end{example}

\subsection{Advanced Ensemble Methods}

Advanced ensemble methods extend the basic boosting and bagging approaches with sophisticated techniques that improve performance and efficiency. AdaBoost (Adaptive Boosting) is an early boosting algorithm that assigns higher weights to misclassified examples, combines weak learners with weights based on their performance, and focuses on the hardest examples to improve the overall ensemble. Modern gradient boosting implementations like XGBoost and LightGBM add regularization with L1 and L2 penalties to prevent overfitting, pruning to remove splits that don't improve performance, feature subsampling with random feature selection at each split, and efficient implementation optimized for speed and memory usage.

\subsection{Comparison of Ensemble Methods}

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
Method & Bias & Variance & Interpretability \\
\midrule
Single Tree & Low & High & High \\
Random Forest & Low & Medium & Medium \\
Gradient Boosting & Low & Low & Low \\
\bottomrule
\end{tabular}
\caption{Tree-based methods comparison.}
\label{tab:ensemble-comparison}
\end{table}

\subsection{Advantages and Limitations}

Decision trees and ensemble methods have several advantages including being interpretable and easy to understand and explain, being flexible by handling mixed data types and missing values, making no assumptions about data distribution, providing feature importance measures that can identify important features, and being robust with less sensitivity to outliers than linear methods. However, they also have limitations including overfitting where individual trees can overfit easily, instability where small changes in data can lead to very different trees, computational cost where ensemble methods can be slow to train, memory usage where storing many trees requires significant memory, and being less effective with high-dimensional data where performance can degrade with many features.

