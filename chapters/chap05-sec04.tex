% Chapter 5, Section 04

\section{Decision Trees and Ensemble Methods \difficultyInline{intermediate}}
\label{sec:decision-trees}

\textbf{Decision trees} are intuitive, interpretable models that make predictions by asking a series of yes/no questions about the input features. While individual trees can be prone to overfitting, combining multiple trees through ensemble methods often leads to much better performance.

\subsection{Intuition and Motivation}

Think of a decision tree as a flowchart for making decisions, where to classify whether someone will buy a product, you might ask "Is their income > \$50k?" and if yes, ask "Are they under 30?" or if no, ask "Do they have children?" Each question splits the data into smaller, more homogeneous groups that become easier to classify. The key advantages of decision trees include interpretability where they are easy to understand and explain, no feature scaling requirements since they work with mixed data types, handling missing values by being able to deal with incomplete data, and being non-parametric with no assumptions about data distribution, making them flexible and robust across different problem domains.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.7]
\node[draw, rectangle, fill=bookpurple!20] (root) at (0,0) {Income > \$50k?};
\node[draw, rectangle, fill=bookpurple!20] (left) at (-3,-2) {Age < 30?};
\node[draw, rectangle, fill=bookpurple!20] (right) at (3,-2) {Has children?};
\node[draw, rectangle, fill=bookred!20] (ll) at (-4.5,-4) {Buy};
\node[draw, rectangle, fill=bookred!20] (lr) at (-1.5,-4) {Don't buy};
\node[draw, rectangle, fill=bookred!20] (rl) at (1.5,-4) {Buy};
\node[draw, rectangle, fill=bookred!20] (rr) at (4.5,-4) {Don't buy};

\draw[->] (root) -- (left) node[midway, left] {Yes};
\draw[->] (root) -- (right) node[midway, right] {No};
\draw[->] (left) -- (ll) node[midway, left] {Yes};
\draw[->] (left) -- (lr) node[midway, right] {No};
\draw[->] (right) -- (rl) node[midway, left] {Yes};
\draw[->] (right) -- (rr) node[midway, right] {No};
\end{tikzpicture}
\caption{A simple decision tree for predicting product purchases. Each internal node represents a decision, and leaf nodes represent the final prediction.}
\label{fig:decision-tree-example}
\end{figure}

\subsection{Decision Trees}

A decision tree recursively partitions the input space based on feature values, where the algorithm works by starting with all training examples at the root, finding the best feature and threshold to split on that maximizes information gain, creating child nodes for each split that represent the different outcomes, repeating recursively until a stopping criterion is met such as maximum depth or minimum samples per leaf, and assigning a prediction to each leaf node based on the majority class or mean value of the examples that reach that node.

\subsubsection{Splitting Criteria}

The key question is: "Which feature and threshold should we use to split the data?" We want splits that create the most homogeneous child nodes.

\textbf{For classification:}

\textbf{Gini impurity:}
\begin{equation}
\text{Gini} = 1 - \sum_{k=1}^{K} p_k^2
\end{equation}

\textbf{Entropy:}
\begin{equation}
\text{Entropy} = -\sum_{k=1}^{K} p_k \log p_k
\end{equation}

\textbf{For regression:}
\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - \bar{y})^2
\end{equation}

where $p_k$ is the proportion of class $k$ examples in a node, and $\bar{y}$ is the mean target value.

\subsubsection{Information Gain}

The \textbf{information gain} measures how much a split reduces impurity:

\begin{equation}
\text{IG} = \text{Impurity(parent)} - \sum_{j} \frac{n_j}{n} \text{Impurity(child}_j\text{)}
\end{equation}

We choose the split that maximizes information gain.

\subsection{Random Forests}

Random forests address the overfitting problem of individual trees by combining multiple trees trained on different subsets of the data, where each tree is trained on a bootstrap sample of the original data and considers only a random subset of features at each split, creating diversity among the trees that reduces overfitting and improves generalization performance.

\subsubsection{Bootstrap Aggregating (Bagging)}

Random forests use two sources of randomness:

\begin{enumerate}
    \item \textbf{Bootstrap sampling:} Each tree is trained on a random sample (with replacement) of the training data
    \item \textbf{Random feature selection:} At each split, only a random subset of features is considered
\end{enumerate}

Prediction is made by averaging (regression) or voting (classification):

\begin{equation}
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} f_b(\vect{x})
\end{equation}

where $B$ is the number of trees.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
% Draw multiple trees
\foreach \x in {0,2,4,6} {
    \node[draw, rectangle, fill=bookpurple!20] (root\x) at (\x,0) {Tree \pgfmathparse{int(\x/2+1)}\pgfmathresult};
    \node[draw, rectangle, fill=bookred!20] (leaf1\x) at (\x-0.5,-1.5) {Leaf 1};
    \node[draw, rectangle, fill=bookred!20] (leaf2\x) at (\x+0.5,-1.5) {Leaf 2};
    \draw[->] (root\x) -- (leaf1\x);
    \draw[->] (root\x) -- (leaf2\x);
}

% Add ensemble prediction
\node[draw, rectangle, fill=bookred!40] (ensemble) at (3,-3) {Final Prediction};
\draw[->] (leaf10) -- (ensemble);
\draw[->] (leaf12) -- (ensemble);
\draw[->] (leaf14) -- (ensemble);
\draw[->] (leaf16) -- (ensemble);

\node at (3,-4) {Average/Vote};
\end{tikzpicture}
\caption{Random forests combine multiple decision trees. Each tree is trained on a different bootstrap sample and makes its own prediction. The final prediction is the average (regression) or majority vote (classification) of all trees.}
\label{fig:random-forest}
\end{figure}

\subsubsection{Advantages of Random Forests}

\begin{itemize}
    \item \textbf{Reduced overfitting:} Multiple trees reduce variance
    \item \textbf{Feature importance:} Can measure which features are most important
    \item \textbf{Robust to outliers:} Bootstrap sampling reduces outlier impact
    \item \textbf{Parallelizable:} Trees can be trained independently
    \item \textbf{No feature scaling needed:} Works with mixed data types
\end{itemize}

\subsection{Gradient Boosting}

Gradient boosting builds an ensemble sequentially, where each new tree corrects the errors of the previous ensemble by learning to predict the residuals of the current model, unlike random forests where trees are trained independently. This sequential approach allows the ensemble to focus on the most difficult examples and gradually improve its performance through iterative refinement.

\subsubsection{Algorithm}

\begin{algorithm}[htbp]
\caption{Gradient Boosting Algorithm}
\label{alg:gradient-boosting}
\begin{algorithmic}[1]
\State Initialize $\hat{y}^{(0)} = \frac{1}{n} \sum_{i=1}^{n} y^{(i)}$ \Comment{Start with mean for regression}
\For{$m = 1$ to $M$}
    \State Compute residuals: $r_i^{(m)} = y^{(i)} - \hat{y}^{(m-1)}(\vect{x}^{(i)})$ for $i = 1, \ldots, n$
    \State Fit tree $f_m$ to residuals $\{(\vect{x}^{(i)}, r_i^{(m)})\}_{i=1}^{n}$
    \State Update: $\hat{y}^{(m)} = \hat{y}^{(m-1)} + \nu f_m(\vect{x}^{(i)})$ for all $i$
\EndFor
\State \textbf{Return} final ensemble $\hat{y}^{(M)}$
\end{algorithmic}
\end{algorithm}

where $\nu$ is the learning rate (shrinkage parameter) and $M$ is the number of boosting iterations.

\subsubsection{Intuition}

Gradient boosting works by starting with a simple model like the mean for regression, calculating the errors (residuals) of the current model to identify where it fails, training a new model to predict these errors and correct the mistakes, adding the new model to the ensemble with a small weight to avoid overfitting, and repeating this process until convergence where the ensemble can no longer be improved significantly.

\begin{example}
For regression with target values $[10, 20, 30, 40]$:

\begin{algorithm}[htbp]
\caption{Gradient Boosting Example}
\label{alg:gradient-boosting-example}
\begin{algorithmic}[1]
\State Initial prediction: $\hat{y}^{(0)} = 25$ \Comment{mean of target values}
\State Residuals: $[-15, -5, 5, 15]$ \Comment{computed as $y^{(i)} - \hat{y}^{(0)}$}
\State Train tree on residuals $\{(\vect{x}^{(i)}, r_i^{(1)})\}_{i=1}^{4}$
\State Add tree to ensemble: $\hat{y}^{(1)} = 25 + \nu \cdot \text{tree}_1(\vect{x})$
\State Repeat with new residuals for next iteration
\end{algorithmic}
\end{algorithm}
\end{example}

\subsection{Advanced Ensemble Methods}

Advanced ensemble methods extend the basic boosting and bagging approaches with sophisticated techniques that improve performance and efficiency. AdaBoost (Adaptive Boosting) is an early boosting algorithm that assigns higher weights to misclassified examples, combines weak learners with weights based on their performance, and focuses on the hardest examples to improve the overall ensemble. Modern gradient boosting implementations like XGBoost and LightGBM add regularization with L1 and L2 penalties to prevent overfitting, pruning to remove splits that don't improve performance, feature subsampling with random feature selection at each split, and efficient implementation optimized for speed and memory usage.

\subsection{Comparison of Ensemble Methods}

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
Method & Bias & Variance & Interpretability \\
\midrule
Single Tree & Low & High & High \\
Random Forest & Low & Medium & Medium \\
Gradient Boosting & Low & Low & Low \\
\bottomrule
\end{tabular}
\caption{Comparison of tree-based methods. Random forests reduce variance through averaging, while gradient boosting reduces both bias and variance through sequential learning.}
\label{tab:ensemble-comparison}
\end{table}

\subsection{Advantages and Limitations}

Decision trees and ensemble methods have several advantages including being interpretable and easy to understand and explain, being flexible by handling mixed data types and missing values, making no assumptions about data distribution, providing feature importance measures that can identify important features, and being robust with less sensitivity to outliers than linear methods. However, they also have limitations including overfitting where individual trees can overfit easily, instability where small changes in data can lead to very different trees, computational cost where ensemble methods can be slow to train, memory usage where storing many trees requires significant memory, and being less effective with high-dimensional data where performance can degrade with many features.

