% Glossary definitions for Deep Learning 101

% Basic Terms
\newglossaryentry{accuracy}{
    name={accuracy},
    description={Proportion of correct predictions over all predictions in a classification task.}
}
\newglossaryentry{precision}{
    name={precision},
    description={Fraction of predicted positives that are true positives.}
}
\newglossaryentry{recall}{
    name={recall},
    description={Fraction of actual positives that are correctly identified.}
}
\newglossaryentry{bleu}{
    name={BLEU},
    description={Bilingual Evaluation Understudy; an n-gram overlap metric for machine translation quality.}
}
\newglossaryentry{rouge}{
    name={ROUGE},
    description={Recall-Oriented Understudy for Gisting Evaluation; a set of metrics for automatic summarization evaluation.}
}
\newglossaryentry{confusion-matrix}{
    name={confusion matrix},
    description={A table summarizing counts of true/false positives/negatives for a classifier.}
}
\newglossaryentry{calibration}{
    name={calibration},
    description={Agreement between predicted probabilities and observed frequencies.}
}
\newglossaryentry{activation-function}{
    name={activation function},
    description={A function that determines the output of a neuron based on its input. Common examples include ReLU, sigmoid, and tanh.}
}

\newglossaryentry{backpropagation}{
    name={backpropagation},
    description={An algorithm for training neural networks that computes gradients by propagating errors backward through the network.}
}

\newglossaryentry{batch-normalization}{
    name={batch normalization},
    description={A technique that normalizes the inputs to each layer by adjusting and scaling the activations.}
}

\newglossaryentry{convolutional-neural-network}{
    name={convolutional neural network},
    description={A type of neural network designed for processing grid-like data such as images, using convolutional layers.}
}

% Chapter 9 terms
\newglossaryentry{receptive-field}{
    name={receptive field},
    description={The spatial extent in the input that influences a unit's activation in a later layer.}
}
\newglossaryentry{stride}{
    name={stride},
    description={The step size with which a convolution or pooling window moves across the input.}
}
\newglossaryentry{padding}{
    name={padding},
    description={Values (typically zeros) added around the input's border to control output size.}
}
\newglossaryentry{depthwise-separable-convolution}{
    name={depthwise separable convolution},
    description={A factorized convolution consisting of depthwise spatial convolution followed by pointwise $1\times1$ convolution.}
}

\newglossaryentry{dropout}{
    name={dropout},
    description={A regularization technique that randomly sets a fraction of input units to 0 during training.}
}

\newglossaryentry{gradient-descent}{
    name={gradient descent},
    description={An optimization algorithm that iteratively adjusts parameters in the direction of steepest descent of the loss function.}
}
\newglossaryentry{stochastic-gradient-descent}{
    name={stochastic gradient descent},
    description={A variant of gradient descent that updates parameters using the gradient from a single example at a time.}
}

\newglossaryentry{momentum-term}{
    name={momentum (optimization)},
    description={An optimization technique that accumulates a velocity vector of past gradients to accelerate convergence and reduce oscillations.}
}

\newglossaryentry{adagrad}{
    name={AdaGrad},
    description={An adaptive optimization method that scales learning rates per-parameter by the inverse root of accumulated squared gradients.}
}

\newglossaryentry{rmsprop}{
    name={RMSProp},
    description={An adaptive optimization method using an exponential moving average of squared gradients to stabilize step sizes.}
}

\newglossaryentry{adam}{
    name={Adam},
    description={An adaptive optimization method combining momentum and RMSProp-style variance adaptation with bias correction of moments.}
}

\newglossaryentry{learning-rate-schedule}{
    name={learning rate schedule},
    description={A strategy for adjusting the learning rate over training, such as step decay, exponential decay, or cosine annealing.}
}


\newglossaryentry{loss-function}{
    name={loss function},
    description={A function that measures the difference between predicted and actual values, used to guide the learning process.}
}

\newglossaryentry{neural-network}{
    name={neural network},
    description={A computing system inspired by biological neural networks, consisting of interconnected nodes (neurons).}
}

\newglossaryentry{overfitting}{
    name={overfitting},
    description={A phenomenon where a model learns the training data too well, including noise, and performs poorly on new data.}
}

\newglossaryentry{regularization}{
    name={regularization},
    description={Techniques used to prevent overfitting by adding constraints or penalties to the model.}
}

% Chapter 7 related terms
\newglossaryentry{weight-decay}{
    name={weight decay},
    description={Another term for L2 regularization where weights are multiplicatively shrunk during updates.}
}

\newglossaryentry{elastic-net}{
    name={elastic net},
    description={A regularization method combining L1 and L2 penalties to promote sparsity and stability.}
}

\newglossaryentry{data-augmentation}{
    name={data augmentation},
    description={The practice of generating additional training examples by applying label-preserving transformations.}
}

\newglossaryentry{early-stopping}{
    name={early stopping},
    description={A regularization technique that halts training when validation performance stops improving.}
}

% duplicate removed: batch-normalization already defined above

\newglossaryentry{label-smoothing}{
    name={label smoothing},
    description={Replacing hard targets with a smoothed distribution to prevent overconfident predictions.}
}

\newglossaryentry{gradient-clipping}{
    name={gradient clipping},
    description={Rescaling or capping gradients to control exploding gradients during training.}
}

\newglossaryentry{stochastic-depth}{
    name={stochastic depth},
    description={Randomly dropping entire layers during training to improve optimization of very deep networks.}
}

\newglossaryentry{adversarial-training}{
    name={adversarial training},
    description={Training with adversarially perturbed inputs to improve robustness to worst-case perturbations.}
}

\newglossaryentry{mixup}{
    name={mixup},
    description={Regularization that trains on convex combinations of inputs and labels, encouraging linear behavior between classes.}
}

\newglossaryentry{recurrent-neural-network}{
    name={recurrent neural network},
    description={A type of neural network designed for sequential data, where connections form directed cycles.}
}

% Chapter 10 terms
\newglossaryentry{backpropagation-through-time}{
    name={backpropagation through time},
    description={Training method for RNNs that backpropagates gradients through the unrolled computation graph across time steps.}
}
\newglossaryentry{gated-recurrent-unit}{
    name={gated recurrent unit},
    description={A simplified gated RNN architecture that combines state and gating, often competitive with fewer parameters.}
}
\newglossaryentry{sequence-to-sequence}{
    name={sequence-to-sequence},
    description={An encoder–decoder framework mapping an input sequence to an output sequence, often with attention.}
}
\newglossaryentry{teacher-forcing}{
    name={teacher forcing},
    description={A training strategy for autoregressive decoders that feeds ground-truth tokens as inputs instead of model predictions.}
}
\newglossaryentry{beam-search}{
    name={beam search},
    description={A heuristic decoding algorithm that keeps the top-$k$ partial sequences at each step to balance quality and speed.}
}
\newglossaryentry{bidirectional-rnn}{
    name={bidirectional RNN},
    description={An architecture that processes sequences in both forward and backward directions and concatenates the states.}
}

\newglossaryentry{supervised-learning}{
    name={supervised learning},
    description={A machine learning approach where models learn from labeled training data.}
}

\newglossaryentry{unsupervised-learning}{
    name={unsupervised learning},
    description={A machine learning approach where models learn patterns from unlabeled data.}
}

\newglossaryentry{validation-set}{
    name={validation set},
    description={A subset of data used to evaluate model performance during training and tune hyperparameters.}
}

% Advanced Terms
\newglossaryentry{attention-mechanism}{
    name={attention mechanism},
    description={A technique that allows models to focus on relevant parts of the input when making predictions.}
}

\newglossaryentry{autoencoder}{
    name={autoencoder},
    description={A neural network architecture that learns to encode data into a lower-dimensional representation and then decode it back.}
}

\newglossaryentry{generative-adversarial-network}{
    name={generative adversarial network},
    description={A framework consisting of two neural networks competing against each other: a generator and a discriminator.}
}

\newglossaryentry{long-short-term-memory}{
    name={long short-term memory},
    description={A type of recurrent neural network architecture designed to overcome the vanishing gradient problem.}
}

\newglossaryentry{transformer}{
    name={transformer},
    description={A neural network architecture based on attention mechanisms, particularly effective for sequence modeling.}
}

\newglossaryentry{variational-autoencoder}{
    name={variational autoencoder},
    description={A generative model that learns to encode data into a probabilistic latent space and generate new samples.}
}

% Mathematical Terms
\newglossaryentry{gradient}{
    name={gradient},
    description={A vector of partial derivatives that points in the direction of steepest increase of a function.}
}

\newglossaryentry{hessian}{
    name={Hessian matrix},
    description={A square matrix of second-order partial derivatives of a scalar-valued function.}
}

\newglossaryentry{jacobian}{
    name={Jacobian matrix},
    description={A matrix of first-order partial derivatives of a vector-valued function.}
}

\newglossaryentry{optimization}{
    name={optimization},
    description={The process of finding the best parameters that minimize or maximize an objective function.}
}

% Application Terms
\newglossaryentry{computer-vision}{
    name={computer vision},
    description={A field of artificial intelligence that enables computers to interpret and understand visual information.}
}

\newglossaryentry{natural-language-processing}{
    name={natural language processing},
    description={A field of artificial intelligence that focuses on the interaction between computers and human language.}
}

\newglossaryentry{reinforcement-learning}{
    name={reinforcement learning},
    description={A machine learning paradigm where agents learn to make decisions through interaction with an environment.}
}

% Classical Machine Learning Terms
\newglossaryentry{linear-regression}{
    name={linear regression},
    description={A statistical method that models the relationship between a dependent variable and one or more independent variables using a linear function.}
}

\newglossaryentry{logistic-regression}{
    name={logistic regression},
    description={A classification algorithm that uses the logistic function to model the probability of class membership.}
}

\newglossaryentry{support-vector-machine}{
    name={support vector machine},
    description={A classification algorithm that finds the optimal hyperplane that maximally separates different classes.}
}

\newglossaryentry{decision-tree}{
    name={decision tree},
    description={A tree-like model that makes decisions by asking a series of yes/no questions about the input features.}
}

\newglossaryentry{random-forest}{
    name={random forest},
    description={An ensemble method that combines multiple decision trees trained on different subsets of the data.}
}

\newglossaryentry{gradient-boosting}{
    name={gradient boosting},
    description={An ensemble method that builds models sequentially, where each new model corrects the errors of the previous ensemble.}
}

\newglossaryentry{k-nearest-neighbors}{
    name={k-nearest neighbors},
    description={A non-parametric algorithm that makes predictions based on the k most similar training examples.}
}

\newglossaryentry{cross-entropy-loss}{
    name={cross-entropy loss},
    description={A loss function commonly used in classification that measures the difference between predicted and actual probability distributions.}
}

\newglossaryentry{regularization-l1}{
    name={L1 regularization},
    description={A regularization technique that adds the sum of absolute values of parameters to the loss function, promoting sparsity.}
}

\newglossaryentry{regularization-l2}{
    name={L2 regularization},
    description={A regularization technique that adds the sum of squared parameters to the loss function, promoting smaller parameter values.}
}

\newglossaryentry{kernel-trick}{
    name={kernel trick},
    description={A technique that allows linear algorithms to work in non-linear spaces by using kernel functions.}
}

\newglossaryentry{support-vector}{
    name={support vector},
    description={Training examples that are closest to the decision boundary in support vector machines.}
}

\newglossaryentry{margin}{
    name={margin},
    description={The distance between the decision boundary and the nearest support vectors in support vector machines.}
}

\newglossaryentry{gini-impurity}{
    name={Gini impurity},
    description={A measure of node impurity in decision trees, calculated as $1 - \sum_{k} p_k^2$ where $p_k$ is the proportion of class $k$.}
}

\newglossaryentry{information-gain}{
    name={information gain},
    description={A measure of how much a split reduces impurity in decision trees, calculated as the difference between parent and weighted child impurities.}
}

\newglossaryentry{bootstrap-sampling}{
    name={bootstrap sampling},
    description={A sampling technique that creates multiple training sets by sampling with replacement from the original dataset.}
}

\newglossaryentry{bagging}{
    name={bagging},
    description={Bootstrap aggregating, an ensemble method that trains multiple models on different bootstrap samples and averages their predictions.}
}

\newglossaryentry{boosting}{
    name={boosting},
    description={An ensemble method that builds models sequentially, where each new model focuses on examples that previous models got wrong.}
}

\newglossaryentry{curse-of-dimensionality}{
    name={curse of dimensionality},
    description={The phenomenon where the performance of many algorithms degrades as the number of dimensions increases.}
}

\newglossaryentry{feature-engineering}{
    name={feature engineering},
    description={The process of creating new features or transforming existing ones to improve model performance.}
}

\newglossaryentry{hyperparameter}{
    name={hyperparameter},
    description={Parameters that control the learning process and must be set before training, such as learning rate or regularization strength.}
}

\newglossaryentry{cross-validation}{
    name={cross-validation},
    description={A technique for assessing model performance by splitting the data into multiple folds and training/testing on different combinations.}
}

\newglossaryentry{overfitting-classical}{
    name={overfitting (classical)},
    description={A phenomenon where a model learns the training data too well, including noise, and performs poorly on new data.}
}

\newglossaryentry{underfitting}{
    name={underfitting},
    description={A phenomenon where a model is too simple to capture the underlying patterns in the data.}
}

\newglossaryentry{bias-variance-tradeoff}{
    name={bias-variance tradeoff},
    description={The fundamental tradeoff in machine learning between model bias (underfitting) and variance (overfitting).}
}

% Feedforward Network Terms
\newglossaryentry{feedforward-network}{
    name={feedforward network},
    description={A neural network architecture where information flows in one direction from input to output, with no cycles or feedback connections.}
}

\newglossaryentry{multilayer-perceptron}{
    name={multilayer perceptron},
    description={A type of feedforward neural network with multiple layers of neurons, also known as MLP.}
}

\newglossaryentry{hidden-layer}{
    name={hidden layer},
    description={An intermediate layer in a neural network that is not directly connected to the input or output.}
}

\newglossaryentry{forward-propagation}{
    name={forward propagation},
    description={The process of computing the output of a neural network by passing input data through each layer sequentially.}
}

\newglossaryentry{backpropagation-algorithm}{
    name={backpropagation algorithm},
    description={An algorithm for training neural networks that computes gradients by propagating errors backward through the network using the chain rule.}
}

\newglossaryentry{universal-approximation}{
    name={universal approximation theorem},
    description={A theorem stating that a feedforward network with a single hidden layer can approximate any continuous function given appropriate activation functions.}
}

\newglossaryentry{relu}{
    name={ReLU},
    description={Rectified Linear Unit, an activation function defined as $\text{ReLU}(x) = \max(0, x)$, widely used in modern neural networks.}
}

\newglossaryentry{sigmoid-activation}{
    name={sigmoid activation},
    description={An activation function defined as $\sigma(x) = \frac{1}{1 + e^{-x}}$, which maps inputs to the range (0,1).}
}

\newglossaryentry{tanh-activation}{
    name={tanh activation},
    description={Hyperbolic tangent activation function defined as $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, which maps inputs to the range (-1,1).}
}

\newglossaryentry{leaky-relu}{
    name={Leaky ReLU},
    description={A variant of ReLU that allows small negative values, defined as $\text{LeakyReLU}(x) = \max(\alpha x, x)$ where $\alpha$ is a small positive constant.}
}

\newglossaryentry{swish-activation}{
    name={Swish activation},
    description={A smooth activation function defined as $\text{Swish}(x) = x \cdot \sigma(x)$, where $\sigma$ is the sigmoid function.}
}

\newglossaryentry{gelu}{
    name={GELU},
    description={Gaussian Error Linear Unit, an activation function defined as $\text{GELU}(x) = x \cdot \Phi(x)$ where $\Phi$ is the Gaussian CDF.}
}

\newglossaryentry{softmax}{
    name={softmax},
    description={An activation function that converts a vector of real numbers into a probability distribution, defined as $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$.}
}

\newglossaryentry{mean-squared-error}{
    name={mean squared error},
    description={A loss function for regression tasks that measures the average squared difference between predicted and actual values.}
}

\newglossaryentry{vanishing-gradient}{
    name={vanishing gradient problem},
    description={A problem in deep networks where gradients become exponentially small as they propagate backward, making training difficult.}
}

\newglossaryentry{exploding-gradient}{
    name={exploding gradient problem},
    description={A problem in deep networks where gradients become exponentially large as they propagate backward, causing unstable training.}
}

\newglossaryentry{xavier-initialization}{
    name={Xavier initialization},
    description={A weight initialization method that sets weights according to a normal distribution with variance $\frac{2}{n_{\text{in}} + n_{\text{out}}}$.}
}

\newglossaryentry{he-initialization}{
    name={He initialization},
    description={A weight initialization method designed for ReLU networks, setting weights according to a normal distribution with variance $\frac{2}{n_{\text{in}}}$.}
}

\newglossaryentry{mini-batch}{
    name={mini-batch},
    description={A small subset of the training data used in each iteration of gradient descent, typically containing 32-256 examples.}
}

\newglossaryentry{computational-graph}{
    name={computational graph},
    description={A directed graph representing the flow of computations in a neural network, used for automatic differentiation.}
}

% Advanced Mathematical Terms
\newglossaryentry{spectral-decomposition}{
    name={spectral decomposition},
    description={The factorization of a matrix into eigenvalues and eigenvectors, also known as eigendecomposition.}
}

\newglossaryentry{eigendecomposition}{
    name={eigendecomposition},
    description={The decomposition of a matrix into its eigenvalues and eigenvectors, fundamental in linear algebra.}
}

\newglossaryentry{singular-value-decomposition}{
    name={singular value decomposition},
    description={A factorization of a matrix into three matrices: U, Σ, and V, where Σ contains the singular values.}
}

\newglossaryentry{manifold}{
    name={manifold},
    description={A mathematical space that locally resembles Euclidean space, important in representation learning.}
}

\newglossaryentry{kl-divergence}{
    name={KL divergence},
    description={Kullback-Leibler divergence, a measure of difference between two probability distributions.}
}

\newglossaryentry{mutual-information}{
    name={mutual information},
    description={A measure of the mutual dependence between two random variables, used in information theory.}
}

\newglossaryentry{entropy}{
    name={entropy},
    description={A measure of uncertainty or information content in a probability distribution.}
}

\newglossaryentry{cross-entropy}{
    name={cross-entropy},
    description={A measure of the difference between two probability distributions, commonly used as a loss function.}
}

% Optimization Terms
\newglossaryentry{nesterov-momentum}{
    name={Nesterov momentum},
    description={An improved version of momentum that looks ahead to the gradient at the predicted position.}
}

\newglossaryentry{adamw}{
    name={AdamW},
    description={A variant of Adam that decouples weight decay from gradient updates, often providing better generalization.}
}

\newglossaryentry{nadam}{
    name={Nadam},
    description={Nesterov-accelerated Adam, combining Nesterov momentum with Adam's adaptive learning rates.}
}

\newglossaryentry{cosine-annealing}{
    name={cosine annealing},
    description={A learning rate schedule that follows a cosine curve, often used with warmup for transformer training.}
}

\newglossaryentry{gradient-clipping}{
    name={gradient clipping},
    description={A technique to prevent exploding gradients by capping gradient magnitudes during training.}
}

% Advanced Neural Network Terms
\newglossaryentry{residual-connection}{
    name={residual connection},
    description={A skip connection that adds the input to the output of a layer, enabling training of very deep networks.}
}

\newglossaryentry{attention-mechanism-advanced}{
    name={attention mechanism (advanced)},
    description={A mechanism that allows models to focus on different parts of the input sequence when making predictions.}
}

\newglossaryentry{transformer-architecture}{
    name={transformer architecture},
    description={A neural network architecture based entirely on attention mechanisms, without recurrent or convolutional layers.}
}

\newglossaryentry{self-attention}{
    name={self-attention},
    description={An attention mechanism where the query, key, and value all come from the same sequence.}
}

\newglossaryentry{multi-head-attention}{
    name={multi-head attention},
    description={An attention mechanism that runs multiple attention operations in parallel and concatenates the results.}
}

\newglossaryentry{positional-encoding}{
    name={positional encoding},
    description={A technique to inject information about the position of tokens in a sequence into transformer models.}
}

% Generative Model Terms
\newglossaryentry{normalizing-flow}{
    name={normalizing flow},
    description={A generative model that learns invertible transformations to map between simple and complex distributions.}
}

\newglossaryentry{diffusion-model}{
    name={diffusion model},
    description={A generative model that learns to reverse a noise corruption process to generate new samples.}
}

\newglossaryentry{score-function}{
    name={score function},
    description={The gradient of the log-probability density function, used in score-based generative models.}
}

\newglossaryentry{energy-based-model}{
    name={energy-based model},
    description={A generative model that defines an energy function over the data space rather than a probability distribution.}
}

% Advanced Regularization Terms
\newglossaryentry{spectral-normalization}{
    name={spectral normalization},
    description={A regularization technique that constrains the spectral norm of weight matrices to stabilize training.}
}

\newglossaryentry{weight-clipping}{
    name={weight clipping},
    description={A regularization technique that constrains the magnitude of weights to prevent them from becoming too large.}
}

\newglossaryentry{gradient-penalty}{
    name={gradient penalty},
    description={A regularization term added to the loss function to enforce Lipschitz constraints, commonly used in GANs.}
}

% Information Theory Terms
\newglossaryentry{information-gain-advanced}{
    name={information gain (advanced)},
    description={The reduction in entropy achieved by observing a feature, used in decision trees and feature selection.}
}

\newglossaryentry{conditional-entropy}{
    name={conditional entropy},
    description={The entropy of a random variable given the value of another random variable.}
}

\newglossaryentry{joint-entropy}{
    name={joint entropy},
    description={The entropy of the joint distribution of two or more random variables.}
}

% Advanced Optimization Terms
\newglossaryentry{natural-gradient}{
    name={natural gradient},
    description={A gradient descent method that accounts for the geometry of the parameter space using the Fisher information matrix.}
}

\newglossaryentry{second-order-optimization}{
    name={second-order optimization},
    description={Optimization methods that use second-order derivatives (Hessian) to improve convergence.}
}

\newglossaryentry{quasi-newton}{
    name={quasi-Newton methods},
    description={Optimization methods that approximate the Hessian matrix to achieve faster convergence than first-order methods.}
}

\newglossaryentry{bfgs}{
    name={BFGS},
    description={Broyden-Fletcher-Goldfarb-Shanno algorithm, a quasi-Newton method for unconstrained optimization.}
}

\newglossaryentry{l-bfgs}{
    name={L-BFGS},
    description={Limited-memory BFGS, a memory-efficient version of BFGS suitable for large-scale optimization.}
}
