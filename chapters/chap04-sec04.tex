% Chapter 4, Section 4: Numerical Stability and Conditioning

\section{Numerical Stability and Conditioning \difficultyInline{beginner}}
\label{sec:numerical-stability}

Numerical stability and conditioning are crucial considerations in deep learning that determine how sensitive computations are to small errors, affecting the reliability and accuracy of neural network training and inference in real-world applications.

\subsection{Intuition: The Butterfly Effect in Computation}

Imagine a house of cards where a tiny breeze can cause the entire structure to collapse. In numerical computation, we have a similar problem where small errors can grow into large ones, especially problematic in deep learning because deep networks have many layers where errors compound, matrix operations can amplify small errors, and gradient computation requires precise derivatives. The condition number tells us how "sensitive" a computation is to small changes, where a high condition number means small input errors become large output errors, making the computation unstable and unreliable.

\subsection{Condition Number}

The \textbf{condition number} of matrix $\mat{A}$ is:

\begin{equation}
\kappa(\mat{A}) = \|\mat{A}\| \|\mat{A}^{-1}\|
\end{equation}

For symmetric matrices with eigenvalues $\lambda_i$:

\begin{equation}
\kappa(\mat{A}) = \frac{\max_i |\lambda_i|}{\min_i |\lambda_i|}
\end{equation}

High condition numbers indicate numerical instability: small changes in input lead to large changes in output.

\subsubsection{Example: Well-Conditioned vs Ill-Conditioned Matrices}

Consider two matrices:
\begin{align}
\mat{A}_1 &= \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} \quad \text{(well-conditioned)} \\
\mat{A}_2 &= \begin{bmatrix} 1 & 0.99 \\ 0.99 & 1 \end{bmatrix} \quad \text{(ill-conditioned)}
\end{align}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={Input Error},
    ylabel={Output Error},
    width=10cm,
    height=6cm,
    legend pos=north west
]
\addplot[bookpurple, thick] {x};
\addplot[bookred, thick] {10*x};
\legend{Well-conditioned, Ill-conditioned}
\end{axis}
\end{tikzpicture}
\caption{Error amplification for well-conditioned vs ill-conditioned matrices}
\label{fig:condition-number}
\end{figure}

\subsection{Ill-Conditioned Matrices}

In deep learning, ill-conditioned Hessians can make optimization difficult, motivating techniques like batch normalization to stabilize activations, careful weight initialization to avoid poor starting points, adaptive learning rate methods to adjust step sizes dynamically, and preconditioning to improve the conditioning of optimization problems.

\subsection{Gradient Checking}

To verify gradient computations, we use \textbf{finite differences}:

\begin{equation}
\frac{\partial f}{\partial \theta_i} \approx \frac{f(\theta_i + \epsilon) - f(\theta_i - \epsilon)}{2\epsilon}
\end{equation}

This is computationally expensive but useful for debugging.

\subsection{Numerical Precision Trade-offs}

Mixed precision training involves storing weights in FP32 for numerical stability while computing activations and gradients in FP16 for computational efficiency, using loss scaling to prevent underflow in the lower precision computations, and achieving 2-3x speedup with minimal accuracy loss by carefully managing the precision trade-offs between numerical stability and computational performance.

\subsection{Practical Tips}

Practical tips for maintaining numerical stability include monitoring gradient norms during training to detect potential instability, using gradient clipping for RNNs to prevent exploding gradients, preferring numerically stable implementations like log-space computations that avoid overflow and underflow, and being aware of precision limits in very deep networks where accumulated errors can become significant.
