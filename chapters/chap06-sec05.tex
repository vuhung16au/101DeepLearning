% Chapter 6, Section 5

\section{Design Choices \difficultyInline{intermediate}}
\label{sec:design-choices}

Design choices in neural networks involve critical decisions about architecture, initialization, and training procedures that significantly impact the network's ability to learn and generalize to new data.

\subsection{Intuition: Building the Right Network}

Designing a neural network is like designing a building, where depth (layers) is like floors in a building where more floors can house more complex functions, but they're harder to build and maintain, width (neurons per layer) is like rooms per floor where more rooms give more space, but you need to fill them efficiently, initialization is like the foundation where if it's wrong, the whole building might collapse, and training is like the construction process where you need the right tools and techniques. The key is finding the right balance for your specific task and data, where each design choice affects the network's capacity, trainability, and performance.

\subsection{Network Depth and Width}

The choice of network depth and width involves important trade-offs, where deeper networks can learn more complex functions but can be harder to optimize due to vanishing or exploding gradients, though modern techniques enable very deep networks with 100+ layers. Wider networks have more capacity but there's a trade-off between width and depth, where very wide shallow networks compete with narrow deep networks in terms of representational power and computational efficiency, with the optimal choice depending on the specific task and available computational resources.

\subsection{Weight Initialization}

Poor initialization can prevent learning. Common strategies:

\textbf{Xavier/Glorot initialization:} balances variance across layers by matching forward and backward signal scales for approximately linear activations (tanh/sigmoid in their linear regime). In the popular uniform variant:
\begin{equation}
W_{ij} \sim \mathcal{U}\Big(-\sqrt{\tfrac{6}{n_{\text{in}} + n_{\text{out}}}},\; \sqrt{\tfrac{6}{n_{\text{in}} + n_{\text{out}}}}\Big)
\end{equation}
and in the normal variant:
\begin{equation}
W_{ij} \sim \mathcal{N}\!\left(0, \; \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\end{equation}
The scaling arises by setting $\mathrm{Var}[\text{preact}]$ and $\mathrm{Var}[\text{grad}]$ approximately constant layer-to-layer under independence assumptions.

\textbf{He initialization} (for ReLU-family): uses a larger variance to compensate for half of activations being zeroed by $\max(0,\cdot)$. Common forms are
\begin{equation}
W_{ij} \sim \mathcal{N}\!\left(0, \; \frac{2}{n_{\text{in}}}\right)
\end{equation}
or the uniform analogue
\begin{equation}
W_{ij} \sim \mathcal{U}\Big(-\sqrt{\tfrac{6}{n_{\text{in}}}},\; \sqrt{\tfrac{6}{n_{\text{in}}}}\Big)
\end{equation}
which maintains stable activation and gradient variances at initialization for ReLU/LeakyReLU networks.

\subsection{Batch Training}

Mini-batch gradient descent computes gradients on small batches of typically 32-256 examples, providing regularization through noise and enabling efficient parallel computation. This approach balances between stochastic gradient descent and full-batch gradient descent, offering a compromise between computational efficiency and gradient stability that is well-suited for modern deep learning applications.

% Index entries
\index{network design!depth}
\index{network design!width}
\index{weight initialization!Xavier}
\index{weight initialization!He}
\index{mini-batch!training}
\index{gradient descent!mini-batch}
