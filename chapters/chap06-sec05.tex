% Chapter 6, Section 5

\section{Design Choices \difficultyInline{intermediate}}
\label{sec:design-choices}

\subsection{Intuition: Building the Right Network}

Designing a neural network is like designing a building:
\begin{itemize}
    \item \textbf{Depth (layers):} Like floors in a building - more floors can house more complex functions, but they're harder to build and maintain
    \item \textbf{Width (neurons per layer):} Like rooms per floor - more rooms give more space, but you need to fill them efficiently
    \item \textbf{Initialization:} Like the foundation - if it's wrong, the whole building might collapse
    \item \textbf{Training:} Like the construction process - you need the right tools and techniques
\end{itemize}

The key is finding the right balance for your specific task and data.

\subsection{Network Depth and Width}

\textbf{Depth} (number of layers):
\begin{itemize}
    \item Deeper networks can learn more complex functions
    \item Can be harder to optimize (vanishing/exploding gradients)
    \item Modern techniques enable very deep networks (100+ layers)
\end{itemize}

\textbf{Width} (neurons per layer):
\begin{itemize}
    \item Wider networks have more capacity
    \item Trade-off between width and depth
    \item Very wide shallow networks vs. narrow deep networks
\end{itemize}

\subsection{Weight Initialization}

Poor initialization can prevent learning. Common strategies:

\textbf{Xavier/Glorot initialization:}
\begin{equation}
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
\end{equation}

\textbf{He initialization} (for ReLU):
\begin{equation}
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
\end{equation}

\subsection{Batch Training}

\textbf{Mini-batch gradient descent:}
\begin{itemize}
    \item Compute gradients on small batches (typically 32-256 examples)
    \item Provides regularization through noise
    \item Enables efficient parallel computation
    \item Balances between SGD and full-batch GD
\end{itemize}

% Index entries
\index{network design!depth}
\index{network design!width}
\index{weight initialization!Xavier}
\index{weight initialization!He}
\index{mini-batch!training}
\index{gradient descent!mini-batch}
