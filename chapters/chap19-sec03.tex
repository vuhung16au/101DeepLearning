% Chapter 19, Section 3

\section{Loopy Belief Propagation \difficultyInline{advanced}}
\label{sec:loopy-bp}

Loopy belief propagation extends the exact message passing algorithm from trees to general graphs with cycles, providing an approximate inference method that often works well despite lacking convergence guarantees.

\subsection{Message Passing}

Message passing algorithms compute marginal probabilities by iteratively passing messages between nodes in a graphical model. The message from node $i$ to node $j$ is computed as:

\begin{equation}
m_{i \to j}(x_j) = \sum_{x_i} \psi(x_i, x_j) \psi(x_i) \prod_{k \in N(i) \setminus j} m_{k \to i}(x_i)
\end{equation}

This equation shows that the message $m_{i \to j}(x_j)$ combines the local potential $\psi(x_i, x_j)$ between nodes $i$ and $j$, the node potential $\psi(x_i)$ at node $i$, and all incoming messages from other neighbors $k \in N(i) \setminus j$. The algorithm iteratively updates these messages until convergence, providing an efficient way to compute approximate marginals in complex graphical models.

\subsection{Beliefs}

The beliefs represent the approximate marginal probabilities for each variable, computed from the incoming messages:

\begin{equation}
b_i(x_i) \propto \psi(x_i) \prod_{j \in N(i)} m_{j \to i}(x_i)
\end{equation}

This equation shows that the belief $b_i(x_i)$ at node $i$ is proportional to the product of the local node potential $\psi(x_i)$ and all incoming messages $m_{j \to i}(x_i)$ from neighboring nodes $j \in N(i)$. The beliefs provide the final approximation to the marginal probabilities after the message passing algorithm has converged, serving as the output of the loopy belief propagation algorithm.

\subsection{Exact on Trees}

For tree-structured graphs, belief propagation converges to exact marginals in a finite number of iterations. The key insight is that trees have no cycles, so each message is computed exactly once during the forward and backward passes of the algorithm. The mathematical foundation relies on the fact that the joint distribution factors according to the tree structure, and the message passing equations correspond exactly to the marginalization operations needed to compute the true marginals. This makes belief propagation on trees both exact and efficient, with computational complexity linear in the number of nodes.

\subsection{Loopy Graphs}

When applied to graphs with cycles, loopy belief propagation faces several challenges that distinguish it from the exact algorithm on trees. The presence of cycles means that messages can circulate indefinitely, potentially preventing convergence to a fixed point. Despite this theoretical limitation, the algorithm often provides surprisingly good approximations in practice, making it a valuable tool for approximate inference in complex graphical models.

The success of loopy belief propagation in applications like error-correcting codes and computer vision demonstrates its practical utility, even when theoretical convergence guarantees are absent. The algorithm's ability to capture local dependencies and propagate information through the graph structure makes it particularly effective for problems where the true posterior has complex dependencies that would be difficult to capture with simpler approximation methods. This practical success has made loopy belief propagation one of the most widely used approximate inference methods in machine learning and computer vision applications.



% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (loopy BP)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Iteration}, ylabel={Residual}, ymode=log, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(1,1.0) (2,0.5) (3,0.3) (4,0.2) (5,0.18)};
%       \addplot[bookred,very thick,dashed] coordinates{(1,1.0) (2,1.2) (3,1.4) (4,1.7) (5,2.0)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Loopy BP: a convergent case (solid) vs. a divergent case (dashed), illustrative.}
%   \label{fig:lbp-conv}
% \end{figure}

% \subsection{Notes and references}

% For background and practical considerations, see \textcite{Bishop2006,GoodfellowEtAl2016,Prince2023}.

