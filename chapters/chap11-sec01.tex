% Chapter 11, Section 1

\section{Performance Metrics \difficultyInline{intermediate}}
\label{sec:performance-metrics}

Performance metrics provide quantitative measures to evaluate model effectiveness, guide model selection, and ensure models meet business requirements across different machine learning tasks.

\subsection{Classification Metrics}

Robust model evaluation depends on selecting metrics aligned with task requirements and operational costs \index{metrics}. Accuracy alone can be misleading under class imbalance \index{class imbalance}; prefer precision/recall, AUC, PR-AUC, calibration, and cost-sensitive metrics when appropriate \textcite{GoodfellowEtAl2016,Prince2023}.

\begin{definition}[Confusion Matrix]
For binary classification with positive/negative classes, define true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). The confusion matrix \index{confusion matrix} summarizes counts:

\begin{center}
\begin{tabular}{@{}lcc@{}}\toprule
 & \textbf{Predicted +} & \textbf{Predicted --} \\
\midrule
\textbf{Actual +} & TP & FN \\
\textbf{Actual --} & FP & TN \\
\bottomrule
\end{tabular}
\end{center}
\end{definition}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[x=1.8cm,y=1.8cm]
    % Axes labels
    \node at (1,2.3) {Predicted};
    \node[rotate=90] at (-0.3,1) {Actual};
    % Grid and cells (values: TP=100, FP=15, FN=10, TN=875)
    % Normalize colors roughly by mapping value/ max
    \definecolor{cellA}{RGB}{60,16,83}    % TP - bookpurple
    \definecolor{cellB}{RGB}{242,18,12}   % FP - bookred
    \definecolor{cellC}{RGB}{242,18,12}   % FN - bookred
    \definecolor{cellD}{RGB}{90,25,120}   % TN - lighter purple
    % Draw borders
    \draw[bookblack] (0,0) rectangle (2,2);
    \draw[bookblack] (1,0) -- (1,2);
    \draw[bookblack] (0,1) -- (2,1);
    % Fill cells
    \fill[cellA] (0,1) rectangle (1,2);
    \fill[cellB] (1,1) rectangle (2,2);
    \fill[cellC] (0,0) rectangle (1,1);
    \fill[cellD] (1,0) rectangle (2,1);
    % Ticks and labels
    \node at (0.5,-0.25) {$-$};
    \node at (1.5,-0.25) {$+$};
    \node at (-0.25,0.5) {$-$};
    \node at (-0.25,1.5) {$+$};
    % Counts
    \node[bookwhite] at (0.5,1.5) {100};
    \node[bookwhite] at (1.5,1.5) {15};
    \node[bookwhite] at (0.5,0.5) {10};
    \node[bookwhite] at (1.5,0.5) {875};
  \end{tikzpicture}
  \caption{Confusion matrix heatmap (example counts). High diagonal values indicate good performance.}
  \label{fig:confusion-heatmap}
\end{figure}

\begin{definition}[Accuracy]
\gls{accuracy} \index{accuracy} measures overall correctness but can obscure minority-class performance:
\begin{equation}
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{TN} + \text{FN}}.
\end{equation}
\end{definition}

\begin{definition}[Precision and Recall]
\gls{precision} and \gls{recall} quantify quality on the positive class:
\begin{align}
\text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}}, \\
\text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}}.
\end{align}
\end{definition}

\begin{definition}[F1 Score]
The harmonic mean balances precision and recall:
\begin{equation}
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\end{equation}
\end{definition}

\begin{definition}[ROC and AUC]
The ROC curve \index{ROC curve} plots TPR vs. FPR as the decision threshold varies; AUC summarizes ranking quality and is threshold-independent.
\begin{align}
\text{TPR} &= \frac{\text{TP}}{\text{TP} + \text{FN}}, & \text{FPR} &= \frac{\text{FP}}{\text{FP} + \text{TN}}.
\end{align}
\end{definition}

\begin{definition}[Precision-Recall (PR) Curve]
Under heavy class imbalance, the PR curve \index{precision--recall curve} and average precision (AP) are often more informative than ROC \textcite{Prince2023}.
\end{definition}

\begin{definition}[Calibration]
A calibrated classifier's predicted probabilities match observed frequencies. Use reliability diagrams and expected calibration error (ECE) \index{calibration}. Calibration matters in risk-sensitive applications \textcite{GoodfellowEtAl2016}.
\end{definition}

% \subsubsection*{Visual aids}

\addcontentsline{toc}{subsubsection}{Visual aids (classification)}

\paragraph{ROC and PR curves.} The following figures illustrate key classification metrics:

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.45\textwidth,
      height=0.32\textwidth,
      xlabel={False Positive Rate}, 
      ylabel={True Positive Rate},
      xmin=0,xmax=1,ymin=0,ymax=1,
      legend pos=south east, 
      grid=both,
      axis lines=left]
      \addplot[very thick,bookpurple] coordinates {(0,0) (0.1,0.6) (0.2,0.78) (0.4,0.9) (1,1)};\addlegendentry{Model A}
      \addplot[very thick,bookred,dashed] coordinates {(0,0) (0.2,0.55) (0.5,0.8) (0.8,0.9) (1,1)};\addlegendentry{Model B}
      \addplot[bookpurple!40] coordinates {(0,0) (1,1)};\addlegendentry{Random}
    \end{axis}
  \end{tikzpicture}
  \caption{ROC curves for two models. Higher AUC indicates better ranking quality.}
  \label{fig:roc-curves}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.45\textwidth,
      height=0.32\textwidth,
      xlabel={Recall}, 
      ylabel={Precision},
      xmin=0,xmax=1,ymin=0,ymax=1,
      legend pos=south west, 
      grid=both,
      axis lines=left]
      \addplot[very thick,bookpurple] coordinates {(0.0,1.0) (0.2,0.92) (0.4,0.88) (0.6,0.80) (0.8,0.65) (1.0,0.45)};\addlegendentry{Model A}
      \addplot[very thick,bookred,dashed] coordinates {(0.0,1.0) (0.2,0.85) (0.4,0.78) (0.6,0.70) (0.8,0.55) (1.0,0.40)};\addlegendentry{Model B}
    \end{axis}
  \end{tikzpicture}
  \caption{Precision--recall curves emphasize performance on the positive class under imbalance.}
  \label{fig:pr-curves}
\end{figure}

\paragraph{Calibration diagram.} Shows how well predicted probabilities match observed frequencies:

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.45\textwidth,
      height=0.32\textwidth,
      xlabel={Predicted probability}, 
      ylabel={Observed frequency},
      xmin=0,xmax=1,ymin=0,ymax=1, 
      grid=both,
      axis lines=left]
      \addplot[bookpurple,very thick] coordinates{(0.0,0.0) (0.1,0.05) (0.2,0.10) (0.3,0.18) (0.4,0.28) (0.5,0.40) (0.6,0.55) (0.7,0.68) (0.8,0.80) (0.9,0.90) (1.0,1.0)};
      \addplot[bookpurple!50] coordinates{(0,0) (1,1)};
    \end{axis}
  \end{tikzpicture}
  \caption{Reliability diagram illustrating calibration. The diagonal is perfect calibration.}
  \label{fig:calibration}
\end{figure}

\subsection{Regression Metrics}

Choose metrics that reflect business loss and robustness to outliers \index{regression metrics}. Mean squared error (MSE) penalizes large errors more heavily than mean absolute error (MAE). Root mean squared error (RMSE) is in the original units. Coefficient of determination $R^2$ measures variance explained.

\begin{align}
\text{MSE} &= \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2, &
\text{MAE} &= \frac{1}{n} \sum_{i=1}^{n} \lvert y_i - \hat{y}_i \rvert, &
\text{RMSE} &= \sqrt{\text{MSE}}.
\end{align}

\begin{remark}[Regression Metrics for Robustness]
For heavy-tailed noise, consider Huber loss and quantile losses for pinball objectives. Huber loss is a robust loss function that combines the benefits of squared loss (for small errors) and absolute loss (for large errors), making it less sensitive to outliers than mean squared error while still being differentiable everywhere.
\end{remark}

\begin{definition}[Huber Loss]
Huber loss is defined as:
\begin{equation}
L_\delta(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta|y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}
\end{equation}
where $\delta$ is a threshold parameter that determines the transition point between squared and absolute loss.
\end{definition}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=0.48\textwidth,
      height=0.36\textwidth,
      xlabel={$\hat y - y$}, ylabel={Loss}, grid=both, legend pos=north west]
      \addplot[bookpurple,very thick,domain=-3:3,samples=200]{x^2};\addlegendentry{Squared}
      \addplot[bookred,very thick,domain=-3:3,samples=200]{abs(x)};\addlegendentry{Absolute}
      \addplot[black,very thick,domain=-3:3,samples=200]{(abs(x)<=1)*0.5*x^2 + (abs(x)>1)*(abs(x)-0.5)};\addlegendentry{Huber($\delta{=}1$)}
    \end{axis}
  \end{tikzpicture}
  \caption{Comparison of squared, absolute, and Huber losses.}
  \label{fig:huber}
\end{figure}

\subsection{NLP and Sequence Metrics}

Sequence generation quality is commonly measured by \gls{bleu} \index{BLEU} and \gls{rouge} \index{ROUGE} (n-gram overlap), while language models use \emph{perplexity} (negative log-likelihood in exponential form) \textcite{GoodfellowEtAl2016,D2LChapterRNN}:
\begin{equation}
\text{PPL} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(x_i)\right).
\end{equation}

\begin{remark}[Mean Average Precision (mAP)]
Mean Average Precision (mAP) measures how well a system ranks relevant items by computing the average precision across different recall levels, making it crucial for information retrieval where ranking quality matters more than binary classification.
\end{remark}

\begin{remark}[Normalized Discounted Cumulative Gain (nDCG)]
Normalized Discounted Cumulative Gain (nDCG) evaluates ranking quality by considering both relevance and position, giving higher weight to items ranked earlier in the list, which reflects real-world user behavior where top results are most important.
\end{remark}

\begin{remark}[Recall@k]
Recall@k measures the proportion of relevant items found in the top-k results, providing a practical metric for applications where users only examine the first few results.
\end{remark}

For retrieval and ranking, report mean average precision (mAP), normalized discounted cumulative gain (nDCG), and recall@k. These metrics are essential because they capture the nuanced performance of ranking systems where the order of results significantly impacts user experience, unlike traditional classification metrics that treat all predictions equally regardless of their position in a ranked list.

\index{mAP}\index{nDCG}\index{recall@k}

\subsection{Worked examples}

\begin{example}[Imbalanced Disease Detection]
In a 1\% prevalence setting, a classifier with 99\% accuracy can be worthless. Reporting PR-AUC and calibration surfaces early detection quality and absolute risk estimates valued by clinicians \textcite{Ronneberger2015}.
\end{example}

\begin{example}[Threshold Selection]
Optimize thresholds against a cost matrix or utility function (e.g., false negative cost \(\gg\) false positive). Plot utility vs. threshold to choose operating points.
\end{example}

\begin{example}[Macro vs. Micro Averaging]
For multi-class, macro-averaged F1 treats classes equally; micro-averaged F1 weights by support. Choose based on fairness vs. prevalence alignment \textcite{Prince2023}.
\end{example}

