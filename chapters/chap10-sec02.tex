% Chapter 10, Section 2

\section{Backpropagation Through Time (BTTT)\difficultyInline{intermediate}}
\label{sec:bptt}

\subsection*{Intuition}

BPTT treats the unrolled RNN as a deep network across time and applies backpropagation through each time slice. Gradients flow backward along temporal edges, accumulating effects from future steps. Truncation limits how far signals propagate to balance cost and dependency length \cite{GoodfellowEtAl2016}.

\subsection*{Historical Context}

The backpropagation algorithm \cite{Rumelhart1986} enabled efficient training of deep networks; applying it to unrolled RNNs became known as BPTT. Awareness of vanishing/exploding gradients led to clipping and gated architectures \cite{GoodfellowEtAl2016,Hochreiter1997}.

% Index and glossary
\index{backpropagation through time}
\glsadd{vanishing-gradient}
\glsadd{exploding-gradient}

\subsection{BPTT Algorithm}

Gradients are computed by unrolling the network and applying backpropagation through the temporal graph. Let $L=\sum_{t=1}^{T} L_t$ and $\vect{h}_t = f(\vect{h}_{t-1}, \vect{x}_t; \vect{\theta})$, $\vect{y}_t = g(\vect{h}_t; \vect{\theta}_y)$. The total derivative w.r.t. hidden states satisfies the recurrence:

\begin{equation}
\frac{\partial L}{\partial \vect{h}_t} = \frac{\partial L}{\partial \vect{y}_t} \frac{\partial \vect{y}_t}{\partial \vect{h}_t} + \frac{\partial L}{\partial \vect{h}_{t+1}} \frac{\partial \vect{h}_{t+1}}{\partial \vect{h}_t}
\end{equation}

For any parameter block $\mat{W} \in \vect{\theta}$ appearing at each time step:
\begin{equation}
\frac{\partial L}{\partial \mat{W}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial \mat{W}}
\end{equation}

\begin{algorithm}[h]
\caption{Backpropagation Through Time (BPTT)}
\label{alg:bptt}
\begin{algorithmic}[1]
\State \textbf{Input:} Sequence $\{\vect{x}_1, \ldots, \vect{x}_T\}$, parameters $\vect{\theta}$
\State \textbf{Output:} Gradients $\frac{\partial L}{\partial \vect{\theta}}$
\State
\State \Comment{Forward pass}
\For{$t = 1$ \textbf{to} $T$}
    \State Compute $\vect{h}_t = f(\vect{h}_{t-1}, \vect{x}_t; \vect{\theta})$
    \State Compute $\vect{y}_t = g(\vect{h}_t; \vect{\theta}_y)$
    \State Compute $L_t = \text{loss}(\vect{y}_t, \vect{y}_t^{\text{target}})$
\EndFor
\State
\State \Comment{Initialize temporal gradients}
\State $\frac{\partial L}{\partial \vect{h}_{T+1}} \leftarrow \vect{0}$
\State
\State \Comment{Backward pass}
\For{$t = T$ \textbf{downto} $1$}
    \State $\delta_t \leftarrow \frac{\partial L}{\partial \vect{y}_t} \frac{\partial \vect{y}_t}{\partial \vect{h}_t} + \frac{\partial L}{\partial \vect{h}_{t+1}} \frac{\partial \vect{h}_{t+1}}{\partial \vect{h}_t}$
    \State $\frac{\partial L}{\partial \mat{W}} \mathrel{+}= \delta_t \frac{\partial \vect{h}_t}{\partial \mat{W}}$ \Comment{Accumulate gradients for all parameters at time $t$}
\EndFor
\State
\State \Comment{Apply gradient clipping if needed and update parameters}
\State Apply gradient clipping if $\norm{\frac{\partial L}{\partial \vect{\theta}}} > \text{threshold}$
\State Update parameters: $\vect{\theta} \leftarrow \vect{\theta} - \alpha \frac{\partial L}{\partial \vect{\theta}}$
\end{algorithmic}
\end{algorithm}

This view aligns with the computational-graph treatment in \cite{GoodfellowEtAl2016} and standard expositions \cite{WebDLBRNN,D2LChapterRNN}.

\subsection{Vanishing and Exploding Gradients}

Gradients can vanish or explode exponentially due to the chain rule applied across time steps. The gradient of the loss with respect to an earlier hidden state $\vect{h}_k$ involves a product of Jacobian matrices:

\begin{equation}
\frac{\partial \vect{h}_t}{\partial \vect{h}_k} = \prod_{i=k+1}^{t} \frac{\partial \vect{h}_i}{\partial \vect{h}_{i-1}} = \prod_{i=k+1}^{t} \mat{W}^\top \text{diag}(\sigma'(\vect{z}_i))
\end{equation}

The behavior depends on the eigenvalues of the weight matrix $\mat{W}$:
\begin{itemize}
    \item \textbf{Eigenvalues $< 1$:} Each multiplication by $\mat{W}^\top$ shrinks the gradient magnitude, causing exponential decay as the product accumulates over time steps. This leads to vanishing gradients where early time steps receive negligible gradient signals.
    \item \textbf{Eigenvalues $> 1$:} Each multiplication amplifies the gradient, causing exponential growth that can lead to numerical instability and training divergence.
\end{itemize}

\textbf{Solutions:}

\textbf{Gradient clipping} prevents exploding gradients by scaling down gradients when their norm exceeds a threshold, maintaining training stability while preserving gradient direction. \textbf{Careful initialization} techniques like Xavier/He initialization set initial weights to have appropriate variance, reducing the likelihood of extreme eigenvalues that cause gradient problems. \textbf{ReLU activation} helps because its derivative is either 0 or 1, avoiding the multiplicative shrinking effect of sigmoid/tanh derivatives that compound vanishing gradients. \textbf{LSTM/GRU architectures} introduce gating mechanisms that create direct paths for gradient flow, bypassing the problematic multiplicative chains and enabling learning of long-range dependencies.

\subsection{Truncated BPTT}

For very long sequences, truncate gradient computation by limiting backpropagation to a sliding window of $k$ steps \cite{GoodfellowEtAl2016}. This approach processes inputs in segments of length $k$, possibly overlapping with stride $s$, allowing the model to handle arbitrarily long sequences while maintaining computational efficiency. The key insight is that gradients are backpropagated only within each segment to reduce memory and computational time, making it feasible to train on very long sequences that would otherwise be computationally prohibitive. The hidden state is carried forward between segments but treated as a constant during the truncated backward step, creating a balance between maintaining temporal continuity and computational tractability.

\begin{algorithm}[h]
\caption{Truncated Backpropagation Through Time (Truncated BPTT)}
\label{alg:truncated-bptt}
\begin{algorithmic}[1]
\State \textbf{Input:} Sequence $\{\vect{x}_1, \ldots, \vect{x}_T\}$, parameters $\vect{\theta}$, chunk size $k$, stride $s$
\State \textbf{Output:} Gradients $\frac{\partial L}{\partial \vect{\theta}}$
\State
\State Initialize $\vect{h}_0$ \Comment{Initial hidden state}
\State
\For{$t = 1$ \textbf{to} $T$ \textbf{step} $s$}
    \State \Comment{Take chunk $[t, t+k-1]$}
    \State $t_{\text{end}} \leftarrow \min(t + k - 1, T)$
    \State
    \State \Comment{Forward pass over the chunk}
    \For{$i = t$ \textbf{to} $t_{\text{end}}$}
        \State Compute $\vect{h}_i = f(\vect{h}_{i-1}, \vect{x}_i; \vect{\theta})$
        \State Compute $\vect{y}_i = g(\vect{h}_i; \vect{\theta}_y)$
        \State Compute $L_i = \text{loss}(\vect{y}_i, \vect{y}_i^{\text{target}})$
    \EndFor
    \State
    \State \Comment{Backpropagate losses only within the chunk}
    \State Initialize $\frac{\partial L}{\partial \vect{h}_{t_{\text{end}}+1}} \leftarrow \vect{0}$
    \For{$i = t_{\text{end}}$ \textbf{downto} $t$}
        \State $\delta_i \leftarrow \frac{\partial L}{\partial \vect{y}_i} \frac{\partial \vect{y}_i}{\partial \vect{h}_i} + \frac{\partial L}{\partial \vect{h}_{i+1}} \frac{\partial \vect{h}_{i+1}}{\partial \vect{h}_i}$
        \State $\frac{\partial L}{\partial \mat{W}} \mathrel{+}= \delta_i \frac{\partial \vect{h}_i}{\partial \mat{W}}$ \Comment{Accumulate gradients}
    \EndFor
    \State
    \State \Comment{Optionally detach hidden state to bound gradient length}
    \State $\vect{h}_t \leftarrow \text{detach}(\vect{h}_{t_{\text{end}}})$ \Comment{Prevent gradients from flowing beyond chunk}
\EndFor
\end{algorithmic}
\end{algorithm}

\textbf{Trade-offs:} Lower memory and latency versus potentially missing very long-range dependencies. Increasing $k$ improves dependency length coverage but increases cost. Hybrids with dilated RNNs or attention can mitigate the trade-off.

