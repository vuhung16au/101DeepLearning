% Chapter 10, Section 2

\section{Backpropagation Through Time \difficultyInline{intermediate}}
\label{sec:bptt}

\subsection*{Intuition}

BPTT treats the unrolled RNN as a deep network across time and applies backpropagation through each time slice. Gradients flow backward along temporal edges, accumulating effects from future steps. Truncation limits how far signals propagate to balance cost and dependency length \cite{GoodfellowEtAl2016}.

\subsection*{Historical Context}

The backpropagation algorithm \cite{Rumelhart1986} enabled efficient training of deep networks; applying it to unrolled RNNs became known as BPTT. Awareness of vanishing/exploding gradients led to clipping and gated architectures \cite{GoodfellowEtAl2016,Hochreiter1997}.

% Index and glossary
\index{backpropagation through time}
\glsadd{vanishing-gradient}
\glsadd{exploding-gradient}

\subsection{BPTT Algorithm}

Gradients are computed by unrolling the network and applying backpropagation through the temporal graph. Let $L=\sum_{t=1}^{T} L_t$ and $\vect{h}_t = f(\vect{h}_{t-1}, \vect{x}_t; \vect{\theta})$, $\vect{y}_t = g(\vect{h}_t; \vect{\theta}_y)$. The total derivative w.r.t. hidden states satisfies the recurrence:

\begin{equation}
\frac{\partial L}{\partial \vect{h}_t} = \frac{\partial L}{\partial \vect{y}_t} \frac{\partial \vect{y}_t}{\partial \vect{h}_t} + \frac{\partial L}{\partial \vect{h}_{t+1}} \frac{\partial \vect{h}_{t+1}}{\partial \vect{h}_t}
\end{equation}

For any parameter block $\mat{W} \in \vect{\theta}$ appearing at each time step:
\begin{equation}
\frac{\partial L}{\partial \mat{W}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial \mat{W}}
\end{equation}

\paragraph{Algorithm (BPTT).}
\begin{enumerate}[nosep]
    \item Forward pass: for $t=1,\dots,T$, compute $\vect{h}_t$, $\vect{y}_t$, and $L_t$.
    \item Initialize temporal gradients: $\frac{\partial L}{\partial \vect{h}_{T+1}}=\vect{0}$.
    \item Backward pass: for $t=T,\dots,1$:
    \begin{align*}
        \delta_t &\leftarrow \frac{\partial L}{\partial \vect{y}_t} \frac{\partial \vect{y}_t}{\partial \vect{h}_t} + \left(\frac{\partial L}{\partial \vect{h}_{t+1}}\right) \frac{\partial \vect{h}_{t+1}}{\partial \vect{h}_t} \\
        \frac{\partial L}{\partial \mat{W}} &\mathrel{+}= \delta_t \frac{\partial \vect{h}_t}{\partial \mat{W}} \quad (\text{or add contributions for all parameters at time } t)
    \end{align*}
    \item Apply gradient clipping if needed and update parameters.
\end{enumerate}

This view aligns with the computational-graph treatment in \cite{GoodfellowEtAl2016} and standard expositions \cite{WebDLBRNN,D2LChapterRNN}.

\subsection{Vanishing and Exploding Gradients}

Gradients can vanish or explode exponentially:

\begin{equation}
\frac{\partial \vect{h}_t}{\partial \vect{h}_k} = \prod_{i=k+1}^{t} \frac{\partial \vect{h}_i}{\partial \vect{h}_{i-1}} = \prod_{i=k+1}^{t} \mat{W}^\top \text{diag}(\sigma'(\vect{z}_i))
\end{equation}

If eigenvalues of $\mat{W}$ are:
\begin{itemize}
    \item $< 1$: gradients vanish
    \item $> 1$: gradients explode
\end{itemize}

\textbf{Solutions:}
\begin{itemize}
    \item Gradient clipping (for explosion) \glsadd{gradient-clipping}
    \item Careful initialization
    \item ReLU activation
    \item LSTM/GRU architectures
\end{itemize}

\subsection{Truncated BPTT}

For very long sequences, truncate gradient computation by limiting backpropagation to a sliding window of $k$ steps \cite{GoodfellowEtAl2016}:
\begin{itemize}
    \item Process inputs in segments of length $k$ (possibly overlapping with stride $s$).
    \item Backpropagate gradients only within each segment to reduce memory and time.
    \item Hidden state is carried forward between segments but treated as a constant during the truncated backward step.
\end{itemize}

\paragraph{Algorithm (Truncated BPTT).}
\begin{enumerate}[nosep]
    \item For $t=1,1+s,1+2s,\dots$, take the chunk $[t,\, t+k-1]$.
    \item Run forward pass over the chunk using the current hidden state as initialization.
    \item Backpropagate losses only within the chunk; accumulate gradients.
    \item Optionally detach the final hidden state from the graph before the next chunk to bound gradient length.
\end{enumerate}

\textbf{Trade-offs:} Lower memory and latency versus potentially missing very long-range dependencies. Increasing $k$ improves dependency length coverage but increases cost. Hybrids with dilated RNNs or attention can mitigate the trade-off.

