% Chapter 15, Section 3

\section{Self-Supervised Learning \difficultyInline{intermediate}}
\label{sec:self-supervised}

Self-supervised learning learns representations without manual labels by solving pretext tasks that can be automatically generated from the data itself, where the model learns useful representations by predicting parts of the input from other parts or by solving auxiliary tasks that don't require human annotation.

\subsection{Pretext Tasks}

Pretext tasks for images include rotation prediction where the model learns to predict the rotation angle of an image, jigsaw puzzle where the model arranges shuffled patches to learn spatial relationships, colorization where the model predicts colors from grayscale images to learn color semantics, and inpainting where the model fills masked regions to learn object structure and context. For text, pretext tasks include masked language modeling where the model predicts masked words as in BERT, next sentence prediction where the model determines if sentences are consecutive to learn discourse relationships, and autoregressive generation where the model predicts the next token as in GPT to learn language modeling capabilities.

\subsection{Benefits}

Self-supervised learning provides several key benefits including the ability to leverage unlabeled data that is often abundant and cheap to obtain, where the model can learn from vast amounts of data without requiring expensive human annotation. The learned representations are often general-purpose and transfer well to downstream tasks, where the model learns to capture the underlying structure of the data rather than task-specific patterns. Self-supervised learning often outperforms supervised pre-training on downstream tasks, where the learned representations are more robust and generalizable because they are not biased toward specific labeled examples.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (self-supervised)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Pretraining epochs}, ylabel={Linear probe accuracy}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(10,0.55) (20,0.62) (40,0.68) (80,0.72) (160,0.74)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Self-supervised pretraining improves downstream linear probe performance (illustrative).}
%   \label{fig:ssl-probe}
% \end{figure}

% \subsection{References}

% For contrastive and masked pretraining approaches, see \textcite{Chen2020,Devlin2018,Prince2023}.

