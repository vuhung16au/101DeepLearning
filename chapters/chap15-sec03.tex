% Chapter 15, Section 3

\section{Self-Supervised Learning \difficultyInline{intermediate}}
\label{sec:self-supervised}

Learn representations without manual labels by solving pretext tasks.

\subsection{Pretext Tasks}

\textbf{For images:}
\begin{itemize}
    \item \textbf{Rotation prediction:} Predict rotation angle
    \item \textbf{Jigsaw puzzle:} Arrange shuffled patches
    \item \textbf{Colorization:} Predict colors from grayscale
    \item \textbf{Inpainting:} Fill masked regions
\end{itemize}

\textbf{For text:}
\begin{itemize}
    \item \textbf{Masked language modeling:} Predict masked words (BERT)
    \item \textbf{Next sentence prediction:} Predict if sentences are consecutive
    \item \textbf{Autoregressive generation:} Predict next token (GPT)
\end{itemize}

\subsection{Benefits}

\begin{itemize}
    \item Leverage unlabeled data
    \item Learn general-purpose representations
    \item Often outperforms supervised pre-training
\end{itemize}

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (self-supervised)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={Pretraining epochs}, ylabel={Linear probe accuracy}, grid=both]
%       \addplot[bookpurple,very thick] coordinates{(10,0.55) (20,0.62) (40,0.68) (80,0.72) (160,0.74)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Self-supervised pretraining improves downstream linear probe performance (illustrative).}
%   \label{fig:ssl-probe}
% \end{figure}

% \subsection{References}

% For contrastive and masked pretraining approaches, see \textcite{Chen2020,Devlin2018,Prince2023}.

