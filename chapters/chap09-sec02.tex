% Chapter 9, Section 2

\section{Pooling\index{pooling}\index{max pooling}\index{average pooling}}
\label{sec:pooling}


\textbf{Pooling} reduces spatial dimensions and provides translation invariance.

\subsection*{Intuition}
Pooling summarizes nearby activations so that small translations of the input do not significantly change the summary. Max pooling keeps the strongest response, while average pooling smooths responses. It provides a degree of translation \emph{invariance} complementary to convolution's translation \emph{equivariance} \index{invariance}\index{equivariance}. Modern designs sometimes prefer strided convolutions to make downsampling learnable \cite{GoodfellowEtAl2016,He2016}.

\subsection{Max Pooling}

Takes maximum value in each pooling region:
\begin{equation}
\text{MaxPool}(i,j) = \max_{m,n \in \mathcal{R}_{ij}} I(m,n)
\end{equation}

Common: $2 \times 2$ max pooling with stride 2 (halves spatial dimensions).

\paragraph{Understanding Max Pooling}
Max pooling takes the maximum value within each pooling region, preserving only the strongest activation while discarding weaker responses. This operation provides translation invariance by keeping the strongest response in each region, making the output robust to small spatial shifts in the input. Max pooling is particularly effective for detecting the presence of features (like edges or textures) rather than their exact location, making it useful for building hierarchical representations in CNNs. The operation reduces spatial dimensions while maintaining the most important information, helping the network focus on the most salient features.

\begin{examplebox}{Max Pooling Example}
For input of size $H\times W=32\times32$ and a $2\times2$ window with stride $2$, the output is $16\times16$. Channels are pooled independently.
\end{examplebox}

\subsection{Average Pooling}

Computes average:
\begin{equation}
\text{AvgPool}(i,j) = \frac{1}{|\mathcal{R}_{ij}|} \sum_{m,n \in \mathcal{R}_{ij}} I(m,n)
\end{equation}

\paragraph{Understanding Average Pooling}
Average pooling computes the mean value across each pooling region, providing a smooth summary of local activations rather than preserving only the strongest response like max pooling. This operation reduces spatial dimensions while providing translation invariance by averaging nearby activations, making the output less sensitive to small spatial shifts in the input. Unlike max pooling which preserves the strongest features, average pooling creates a smoother representation that can help reduce noise and provide more stable feature maps. It's particularly useful when you want to preserve information about the overall activation level in a region rather than just the peak response.

\subsection{Global Pooling}

Global pooling operations extend the concept of local pooling to cover entire spatial dimensions, providing powerful mechanisms for reducing feature map complexity while preserving essential information. Global Average Pooling (GAP) computes the average value across all spatial locations for each channel, while Global Max Pooling identifies the maximum value across all spatial locations. These operations prove particularly useful for reducing the parameter count before fully connected layers and for connecting convolutional backbones to classification heads, such as using GAP before softmax layers.

The strategic use of global average pooling can replace large fully connected layers by averaging each feature map to a single scalar value, dramatically reducing overfitting and parameter count while maintaining classification performance. This approach has become particularly popular in modern architectures where the final feature maps are globally pooled before classification, eliminating the need for expensive fully connected layers and providing better generalization properties.\cite{GoodfellowEtAl2016}

\subsection{Alternative: Strided Convolutions\index{strided convolution}}
\label{subsec:strided-convs}

Instead of relying on non-learned pooling operators, strided convolutions with stride $s>1$ perform learned downsampling that can adapt to the specific requirements of the task. For kernel size $k$, stride $s$, and padding $p$, the output spatial dimensions are calculated as $H' = \left\lfloor \frac{H - k + 2p}{s} \right\rfloor + 1$ and $W' = \left\lfloor \frac{W - k + 2p}{s} \right\rfloor + 1$, providing precise control over the downsampling process.

The primary advantages of strided convolutions include their learnable nature and ability to combine feature extraction with downsampling in a single operation, making them particularly effective for stage transitions in modern architectures like ResNet. However, these operations may introduce aliasing artifacts if high-frequency content is not properly low-pass filtered before sub-sampling, leading to the development of anti-aliasing variants that apply blurring before strided operations to preserve signal quality.\index{ResNet}\cite{He2016}

\paragraph{Example.} A $3\times3$ convolution with stride $2$ and padding $1$ keeps spatial size roughly halved (e.g., $32\to16$) while learning filters.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=0.25cm,y=0.25cm]
        % input grid
        \foreach \i in {0,...,15} {\draw (0,\i) -- (16,\i);} 
        \foreach \j in {0,...,16} {\draw (\j,0) -- (\j,16);} 
        % stride-2 sampling dots
        \foreach \i in {1,3,5,7,9,11,13,15} {\foreach \j in {1,3,5,7,9,11,13,15} {\fill[bookpurple] (\j,\i) circle (0.12);} }
        % arrow to output
        \draw[->,thick] (8,-1) -- (24,-1) node[midway,below]{stride 2};
        % output grid 8x8
        \begin{scope}[shift={(24,-0)}]
            \foreach \i in {0,...,8} {\draw (0,\i) -- (8,\i);} 
            \foreach \j in {0,...,8} {\draw (\j,0) -- (\j,8);} 
        \end{scope}
    \end{tikzpicture}
    \caption{Downsampling via stride 2: fewer spatial samples after a strided convolution compared to pooling.}
    \label{fig:strided-conv}
\end{figure}

