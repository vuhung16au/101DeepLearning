% Chapter 9, Section 2

\section{Pooling\index{pooling}\index{max pooling}\index{average pooling}}
\label{sec:pooling}


\textbf{Pooling} reduces spatial dimensions and provides translation invariance.

\subsection*{Intuition}
Pooling summarizes nearby activations so that small translations of the input do not significantly change the summary. Max pooling keeps the strongest response, while average pooling smooths responses. It provides a degree of translation \emph{invariance} complementary to convolution's translation \emph{equivariance} \index{invariance}\index{equivariance}. Modern designs sometimes prefer strided convolutions to make downsampling learnable \cite{GoodfellowEtAl2016,He2016}.

\subsection{Max Pooling}

Takes maximum value in each pooling region:
\begin{equation}
\text{MaxPool}(i,j) = \max_{m,n \in \mathcal{R}_{ij}} I(m,n)
\end{equation}

Common: $2 \times 2$ max pooling with stride 2 (halves spatial dimensions).

\paragraph{Understanding Max Pooling}
Max pooling takes the maximum value within each pooling region, preserving only the strongest activation while discarding weaker responses. This operation provides translation invariance by keeping the strongest response in each region, making the output robust to small spatial shifts in the input. Max pooling is particularly effective for detecting the presence of features (like edges or textures) rather than their exact location, making it useful for building hierarchical representations in CNNs. The operation reduces spatial dimensions while maintaining the most important information, helping the network focus on the most salient features.

\paragraph{Example.} For input of size $H\times W=32\times32$ and a $2\times2$ window with stride $2$, the output is $16\times16$. Channels are pooled independently.

\subsection{Average Pooling}

Computes average:
\begin{equation}
\text{AvgPool}(i,j) = \frac{1}{|\mathcal{R}_{ij}|} \sum_{m,n \in \mathcal{R}_{ij}} I(m,n)
\end{equation}

\paragraph{Understanding Average Pooling}
Average pooling computes the mean value across each pooling region, providing a smooth summary of local activations rather than preserving only the strongest response like max pooling. This operation reduces spatial dimensions while providing translation invariance by averaging nearby activations, making the output less sensitive to small spatial shifts in the input. Unlike max pooling which preserves the strongest features, average pooling creates a smoother representation that can help reduce noise and provide more stable feature maps. It's particularly useful when you want to preserve information about the overall activation level in a region rather than just the peak response.

\subsection{Global Pooling}

Global pooling operates over entire spatial dimensions. \textbf{Global Average Pooling (GAP)} computes the average over all spatial locations, while \textbf{Global Max Pooling} takes the maximum over all spatial locations. These operations are useful for reducing parameters before fully connected layers and for connecting convolutional backbones to classification heads (e.g., GAP before softmax).

\paragraph{Note.} Global average pooling (GAP) can replace large fully connected layers by averaging each feature map to a single scalar, reducing overfitting and parameter count \cite{GoodfellowEtAl2016}.

\subsection{Alternative: Strided Convolutions\index{strided convolution}}
\label{subsec:strided-convs}

Instead of a non-learned pooling operator, a convolution with stride $s>1$ performs \emph{learned downsampling}. For kernel size $k$, stride $s$, and padding $p$, the output spatial dimension per axis is
\begin{equation}
H' = \left\lfloor \frac{H - k + 2p}{s} \right\rfloor + 1,\quad W' = \left\lfloor \frac{W - k + 2p}{s} \right\rfloor + 1.
\end{equation}
This approach offers several advantages: it is learnable and can combine feature extraction with downsampling in one step, as used in stage transitions of ResNet\index{ResNet} \cite{He2016}. However, it may introduce aliasing if high-frequency content is not low-pass filtered prior to sub-sampling. Anti-aliasing variants address this by applying blur before stride.

\paragraph{Example.} A $3\times3$ convolution with stride $2$ and padding $1$ keeps spatial size roughly halved (e.g., $32\to16$) while learning filters.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=0.25cm,y=0.25cm]
        % input grid
        \foreach \i in {0,...,15} {\draw (0,\i) -- (16,\i);} 
        \foreach \j in {0,...,16} {\draw (\j,0) -- (\j,16);} 
        % stride-2 sampling dots
        \foreach \i in {1,3,5,7,9,11,13,15} {\foreach \j in {1,3,5,7,9,11,13,15} {\fill[bookpurple] (\j,\i) circle (0.12);} }
        % arrow to output
        \draw[->,thick] (8,-1) -- (24,-1) node[midway,below]{stride 2};
        % output grid 8x8
        \begin{scope}[shift={(24,-0)}]
            \foreach \i in {0,...,8} {\draw (0,\i) -- (8,\i);} 
            \foreach \j in {0,...,8} {\draw (\j,0) -- (\j,8);} 
        \end{scope}
    \end{tikzpicture}
    \caption{Downsampling via stride 2: fewer spatial samples after a strided convolution compared to pooling.}
    \label{fig:strided-conv}
\end{figure}

