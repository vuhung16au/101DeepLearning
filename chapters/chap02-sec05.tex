% Chapter 2, Section 5: Norms

\section{Norms \difficultyInline{beginner}}
\label{sec:norms}

Norms are functions that measure the size or length of vectors. They are essential for regularization, optimization, and measuring distances in deep learning.

\subsection{Definition of a Norm}

A norm is a mathematical function that provides a consistent and intuitive way to measure the size or length of vectors, serving as the foundation for many important concepts in deep learning including regularization, optimization, and distance metrics. The non-negativity property $f(\vect{x}) \geq 0$ with equality if and only if $\vect{x} = \vect{0}$ ensures that norms always return non-negative values and that only the zero vector has zero norm, which makes intuitive sense as the zero vector has no magnitude. The homogeneity property $f(\alpha\vect{x}) = |\alpha|f(\vect{x})$ ensures that scaling a vector by a scalar $\alpha$ scales its norm by the absolute value of that scalar, which means that the norm scales proportionally with the vector's magnitude, preserving the relative size relationships between vectors. The triangle inequality $f(\vect{x} + \vect{y}) \leq f(\vect{x}) + f(\vect{y})$ ensures that the norm of the sum of two vectors is never greater than the sum of their individual norms, which captures the intuitive notion that the shortest path between two points is a straight line and prevents the norm from behaving in counterintuitive ways. We typically denote norms using the notation $\norm{\vect{x}}$, which provides a concise and standardized way to refer to the magnitude of vectors in mathematical expressions and algorithms.

\subsection{$L^p$ Norms}

The most common family of norms are the $L^p$ norms.

\begin{definition}[$L^p$ Norm]
For $p \geq 1$, the $L^p$ norm of a vector $\vect{x} \in \mathbb{R}^n$ is:
\begin{equation}
    \norm{\vect{x}}_p = \left(\sum_{i=1}^{n} |x_i|^p\right)^{1/p}
\end{equation}
\end{definition}

\subsection{Common Norms}

\subsubsection{$L^1$ Norm (Manhattan Distance)}

The $L^1$ norm is the sum of absolute values:
\begin{equation}
    \norm{\vect{x}}_1 = \sum_{i=1}^{n} |x_i|
\end{equation}

\begin{example}
For $\vect{x} = \begin{bmatrix} 3 \\ -4 \\ 2 \end{bmatrix}$, we have $\norm{\vect{x}}_1 = 3 + 4 + 2 = 9$.
\end{example}

The $L^1$ norm is particularly useful in applications where sparsity is desired, as it encourages solutions with many zero components, making it ideal for feature selection and model compression in deep learning applications.

\subsubsection{$L^2$ Norm (Euclidean Distance)}

The $L^2$ norm is the most common norm, corresponding to Euclidean distance:
\begin{equation}
    \norm{\vect{x}}_2 = \sqrt{\sum_{i=1}^{n} x_i^2} = \sqrt{\vect{x}\transpose\vect{x}}
\end{equation}

\begin{example}
For $\vect{x} = \begin{bmatrix} 3 \\ -4 \\ 2 \end{bmatrix}$, we have $\norm{\vect{x}}_2 = \sqrt{9 + 16 + 4} = \sqrt{29} \approx 5.39$.
\end{example}

The $L^2$ norm is the most widely used norm in deep learning, serving as the foundation for many important algorithms and techniques including ridge regularization, gradient descent optimization, and distance-based similarity measures.

The squared $L^2$ norm is often used in optimization because it has simpler derivatives:
\begin{equation}
    \norm{\vect{x}}_2^2 = \vect{x}\transpose\vect{x} = \sum_{i=1}^{n} x_i^2
\end{equation}

\subsubsection{$L^\infty$ Norm (Maximum Norm)}

The $L^\infty$ norm is defined as:
\begin{equation}
    \norm{\vect{x}}_\infty = \max_i |x_i|
\end{equation}

This can be viewed as the limit of $L^p$ norms as $p \rightarrow \infty$.

\begin{example}
For $\vect{x} = \begin{bmatrix} 3 \\ -4 \\ 2 \end{bmatrix}$, we have $\norm{\vect{x}}_\infty = \max(3, 4, 2) = 4$.
\end{example}

\subsection{Frobenius Norm}

For matrices, the Frobenius norm is analogous to the $L^2$ norm for vectors.

\begin{definition}[Frobenius Norm]
For a matrix $\mat{A} \in \mathbb{R}^{m \times n}$:
\begin{equation}
    \norm{\mat{A}}_F = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} A_{ij}^2} = \sqrt{\text{trace}(\mat{A}\transpose\mat{A})}
\end{equation}
\end{definition}

The Frobenius norm is used for regularizing weight matrices in neural networks.

\subsection{Unit Vectors and Normalization}

A vector with unit norm ($\norm{\vect{x}} = 1$) is called a \emph{unit vector}.

\begin{definition}[Normalization]
To normalize a vector $\vect{x}$, we divide by its norm:
\begin{equation}
    \hat{\vect{x}} = \frac{\vect{x}}{\norm{\vect{x}}}
\end{equation}
resulting in a unit vector pointing in the same direction.
\end{definition}

Normalization is commonly used in deep learning for improving training stability and convergence, including batch normalization for normalizing activations across mini-batches, layer normalization for normalizing within individual samples, input feature scaling for ensuring consistent input ranges, and weight normalization for controlling the magnitude of network parameters.

\subsection{Distance Metrics}

Norms induce distance metrics. The distance between vectors $\vect{x}$ and $\vect{y}$ is:
\begin{equation}
    d(\vect{x}, \vect{y}) = \norm{\vect{x} - \vect{y}}
\end{equation}

Different norms lead to different notions of distance that are useful in various contexts. The $L^1$ norm leads to Manhattan distance, which measures the sum of coordinate differences and is particularly useful in applications where movement is constrained to grid-like patterns, such as in image processing or when dealing with sparse data where the path between points must follow coordinate axes. The $L^2$ norm leads to Euclidean distance, which measures the straight-line distance between points and is the most intuitive distance metric for most applications, making it the default choice for similarity measures, clustering algorithms, and optimization problems in deep learning. The $L^\infty$ norm leads to Chebyshev distance, which measures the maximum coordinate difference and is useful in applications where the worst-case difference between coordinates is more important than the overall distance, such as in error analysis or when dealing with constraints that must be satisfied simultaneously.

\begin{remark}
While the Lebesgue measure is not covered in this book, it is an important concept in advanced deep learning. The Lebesgue measure provides a more general framework for measuring sets and integrating functions, which becomes crucial when dealing with high-dimensional spaces, probability measures, and advanced optimization theory in deep learning. Understanding Lebesgue integration is particularly important for rigorous analysis of convergence properties in optimization algorithms and for understanding the theoretical foundations of probability measures in machine learning.
\end{remark}

\subsection{Regularization in Deep Learning}

Norms are central to regularization techniques that help prevent overfitting and improve generalization in deep learning models. $L^1$ regularization adds $\lambda\norm{\vect{w}}_1$ to the loss function, promoting sparsity by encouraging many weights to become exactly zero, which helps with feature selection and model compression by automatically identifying and removing less important features. $L^2$ regularization adds $\lambda\norm{\vect{w}}_2^2$ to the loss function, preventing large weights by penalizing the squared magnitude of the weight vector, which helps prevent overfitting by keeping the model parameters small and well-behaved. Elastic Net regularization combines both $L^1$ and $L^2$ penalties using $\lambda_1\norm{\vect{w}}_1 + \lambda_2\norm{\vect{w}}_2^2$, providing a balanced approach that can achieve both sparsity and smoothness in the learned parameters, making it particularly useful in applications where both feature selection and parameter stability are important.

Understanding norms and their properties is essential for designing effective regularization strategies and analyzing model behavior.
