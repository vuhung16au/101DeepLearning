% Chapter 20, Section 1

\section{Variational Autoencoders (VAEs) \difficultyInline{advanced}}
\label{sec:vaes}

(See also Chapter 14 for detailed VAE coverage.)

\subsection{Recap}

VAE learns latent representation $\vect{z}$ and decoder $p_{\theta}(\vect{x}|\vect{z})$:
\begin{equation}
\max_{\theta, \phi} \mathbb{E}_{q_{\phi}(\vect{z}|\vect{x})}[\log p_{\theta}(\vect{x}|\vect{z})] - D_{KL}(q_{\phi}(\vect{z}|\vect{x}) \| p(\vect{z}))
\end{equation}

The VAE objective consists of two terms: the reconstruction loss $\mathbb{E}_{q_{\phi}(\vect{z}|\vect{x})}[\log p_{\theta}(\vect{x}|\vect{z})]$ encourages the decoder to accurately reconstruct input data from latent codes, while the KL divergence term $D_{KL}(q_{\phi}(\vect{z}|\vect{x}) \| p(\vect{z}))$ regularizes the encoder to produce latent distributions close to the prior $p(\vect{z})$. This balance ensures both faithful reconstruction and meaningful latent representations.

\subsection{Conditional VAEs}

Generate conditioned on class or attributes:
\begin{equation}
\max \mathbb{E}_{q(\vect{z}|\vect{x}, y)}[\log p(\vect{x}|\vect{z}, y)] - D_{KL}(q(\vect{z}|\vect{x}, y) \| p(\vect{z}))
\end{equation}

Conditional VAEs extend the standard VAE framework by incorporating conditioning information $y$ (such as class labels or attributes) into both the encoder and decoder. The encoder $q(\vect{z}|\vect{x}, y)$ learns to map input data and conditioning information to latent codes, while the decoder $p(\vect{x}|\vect{z}, y)$ generates samples conditioned on both the latent code and the conditioning variable. This enables controlled generation of specific types of content.

\subsection{Disentangled Representations}

\textbf{$\beta$-VAE:} Increase KL weight for disentanglement
\begin{equation}
\mathcal{L} = \mathbb{E}_{q}[\log p(\vect{x}|\vect{z})] - \beta D_{KL}(q(\vect{z}|\vect{x}) \| p(\vect{z}))
\end{equation}

The $\beta$-VAE introduces a hyperparameter $\beta$ that controls the strength of the KL regularization term. When $\beta > 1$, the model is encouraged to learn more independent latent factors, where each dimension of $\vect{z}$ corresponds to a distinct, interpretable attribute of the data. This disentanglement is achieved by penalizing the mutual information between latent dimensions, forcing the model to encode different aspects of variation in separate latent variables.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (VAE recap)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={$z_1$}, ylabel={$z_2$}, grid=both]
%       \addplot+[only marks,mark=*,mark size=0.9pt,bookpurple!70] coordinates{(-2,-2) (-1,0) (0,0) (1,1) (2,2)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Latent samples from a learned VAE prior/posterior (illustrative).}
%   \label{fig:vae20-latent}
% \end{figure}

% \subsection{Notes and references}

% See \textcite{Kingma2013,GoodfellowEtAl2016,Prince2023} for VAE objectives, conditional VAEs, and disentanglement.

