% Chapter 5, Section 03

\section{Support Vector Machines \difficultyInline{intermediate}}
\label{sec:svm}

\textbf{Support Vector Machines} (SVMs) are powerful classification algorithms that find the optimal hyperplane that maximally separates different classes. The key insight is to maximize the margin between classes, leading to better generalization performance.

\begin{remark}
The name "Support Vector Machine" comes from the critical role that support vectors play in defining the optimal hyperplane: these are the data points closest to the decision boundary that literally "support" the entire structure of the classifier. If you move any other data point, the hyperplane doesn't change, but if you move a support vector, the entire hyperplane shifts, demonstrating that the entire "machine" is defined and supported by these few critical vectors.
\end{remark}

\subsection{Intuition and Motivation}

Imagine you have two groups of points on a plane that you want to separate with a line, where there are many possible lines that could separate them, but SVM finds the line that maximizes the distance to the nearest points from each class. This "maximum margin" approach leads to better generalization because the decision boundary is as far as possible from both classes, making the model more robust to new data. The key concepts include support vectors which are the training examples closest to the decision boundary and determine its position, the margin which is the distance between the decision boundary and the nearest support vectors, and the maximum margin principle which chooses the hyperplane that maximizes this margin to achieve the best possible separation.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={Feature 1},
    ylabel={Feature 2},
    title={SVM: Maximum Margin Classification},
    grid=major,
    width=10cm,
    height=8cm
]
% Generate some sample data
\addplot[only marks, mark=*, mark size=3pt, color=bookpurple] coordinates {
    (1,1) (1.5,1.5) (2,1) (2.5,2) (3,1.5)
};
\addplot[only marks, mark=square*, mark size=3pt, color=bookred] coordinates {
    (2,3) (2.5,3.5) (3,3) (3.5,3.5) (4,3)
};
% Add decision boundary and margins
\addplot[thick, color=bookblack, domain=0:5] {3-x};
\addplot[dashed, color=bookpurple, domain=0:5] {3-x+1};
\addplot[dashed, color=bookred, domain=0:5] {3-x-1};
\node at (axis cs:2.5,2.5) [anchor=west] {Decision boundary};
\node at (axis cs:2.5,3.5) [anchor=west] {Margin};
\node at (axis cs:2.5,1.5) [anchor=west] {Margin};
\legend{Class -1, Class +1, Decision boundary, Margins}
\end{axis}
\end{tikzpicture}
\caption{SVM finds the hyperplane (line in 2D) that maximizes the margin between classes. The support vectors are the points closest to the decision boundary.}
\label{fig:svm-margin}
\end{figure}

\subsection{Linear SVM}

For binary classification with labels $y \in \{-1, +1\}$, the decision boundary is:

\begin{equation}
\vect{w}^\top \vect{x} + b = 0
\end{equation}

The \textbf{margin} is the distance between the decision boundary and the nearest support vectors. For a point $\vect{x}^{(i)}$, the distance to the hyperplane is:

\begin{equation}
\text{distance} = \frac{|y^{(i)}(\vect{w}^\top \vect{x}^{(i)} + b)|}{\|\vect{w}\|}
\end{equation}

Since we want to maximize the margin, we can set the margin to be $\frac{2}{\|\vect{w}\|}$ by requiring:

\begin{equation}
y^{(i)}(\vect{w}^\top \vect{x}^{(i)} + b) \geq 1 \quad \forall i
\end{equation}

\subsubsection{Optimization Problem}

Maximizing the margin is equivalent to minimizing $\|\vect{w}\|^2$ subject to the constraints:

\begin{equation}
\min_{\vect{w}, b} \frac{1}{2}\|\vect{w}\|^2
\end{equation}

subject to:
\begin{equation}
y^{(i)}(\vect{w}^\top \vect{x}^{(i)} + b) \geq 1 \quad \forall i
\end{equation}

This is a quadratic programming problem that can be solved using Lagrange multipliers.

\subsection{Soft Margin SVM}

In practice, data is rarely linearly separable, so the soft margin SVM allows some training examples to be misclassified by introducing slack variables $\xi_i$ that measure how much each point violates the margin constraint. The optimization problem becomes $\min_{\vect{w}, b, \boldsymbol{\xi}} \frac{1}{2}\|\vect{w}\|^2 + C \sum_{i=1}^{n} \xi_i$ subject to $y^{(i)}(\vect{w}^\top \vect{x}^{(i)} + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$, where the parameter $C$ controls the trade-off between large margin (small $C$ allows more slack and larger margin) and training accuracy (large $C$ penalizes misclassifications more heavily).

\subsection{Dual Formulation}

The SVM optimization problem can be reformulated in its dual form, which reveals the support vectors and enables the kernel trick:

\begin{equation}
\max_{\boldsymbol{\alpha}} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y^{(i)} y^{(j)} \vect{x}^{(i)} \cdot \vect{x}^{(j)}
\end{equation}

subject to:
\begin{equation}
\sum_{i=1}^{n} \alpha_i y^{(i)} = 0, \quad 0 \leq \alpha_i \leq C
\end{equation}

The decision function becomes:
\begin{equation}
f(\vect{x}) = \sum_{i=1}^{n} \alpha_i y^{(i)} \vect{x}^{(i)} \cdot \vect{x} + b
\end{equation}

Only examples with $\alpha_i > 0$ are support vectors.

\subsection{Kernel Trick}

For non-linear decision boundaries, we can map inputs to a higher-dimensional space using a kernel function $k(\vect{x}, \vect{x}')$ that computes the inner product in the transformed space without explicitly computing the transformation. The kernel trick is used in machine learning to handle non-linear relationships by implicitly mapping data to higher-dimensional spaces where linear separation becomes possible, enabling SVMs to learn complex decision boundaries while maintaining computational efficiency.

\subsubsection{Common Kernels}

\textbf{Linear kernel:}
\begin{equation}
k(\vect{x}, \vect{x}') = \vect{x}^\top \vect{x}'
\end{equation}

\textbf{Polynomial kernel:}
\begin{equation}
k(\vect{x}, \vect{x}') = (\vect{x}^\top \vect{x}' + c)^d
\end{equation}

\textbf{RBF (Gaussian) kernel:}
\begin{equation}
k(\vect{x}, \vect{x}') = \exp(-\gamma \|\vect{x} - \vect{x}'\|^2)
\end{equation}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={Feature 1},
    ylabel={Feature 2},
    title={SVM with RBF Kernel: Non-linear Decision Boundary},
    grid=major,
    width=10cm,
    height=8cm
]
% Generate some sample data in a circular pattern
\addplot[only marks, mark=*, mark size=3pt, color=bookpurple] coordinates {
    (1,1) (1.5,1.5) (2,1) (2.5,2) (3,1.5) (1,2) (1.5,2.5) (2,2) (2.5,3) (3,2.5)
};
\addplot[only marks, mark=square*, mark size=3pt, color=bookred] coordinates {
    (2,3) (2.5,3.5) (3,3) (3.5,3.5) (4,3) (2,4) (2.5,4.5) (3,4) (3.5,4.5) (4,4)
};
% Add a non-linear decision boundary (circle)
\addplot[thick, color=bookblack, domain=0:5] {sqrt(6.25-(x-2.5)^2)+2.5};
\addplot[thick, color=bookblack, domain=0:5] {-sqrt(6.25-(x-2.5)^2)+2.5};
\node at (axis cs:3.5,3.5) [anchor=west] {Non-linear boundary};
\legend{Class -1, Class +1, Decision boundary}
\end{axis}
\end{tikzpicture}
\caption{SVM with RBF kernel can learn non-linear decision boundaries. The decision boundary here is approximately circular.}
\label{fig:svm-rbf-kernel}
\end{figure}

\subsection{Kernel Properties}

A function $k(\vect{x}, \vect{x}')$ is a valid kernel if and only if it is symmetric where $k(\vect{x}, \vect{x}') = k(\vect{x}', \vect{x})$, and positive semi-definite where for any set of points $\{\vect{x}^{(1)}, \ldots, \vect{x}^{(n)}\}$, the kernel matrix $K_{ij} = k(\vect{x}^{(i)}, \vect{x}^{(j)})$ is positive semi-definite. These properties ensure that the kernel function represents a valid inner product in some feature space, making it suitable for use in kernel-based machine learning algorithms.

\subsection{Advantages and Limitations}

SVMs have several advantages including being effective in high-dimensional spaces where the curse of dimensionality affects other methods less, being memory efficient by only storing support vectors rather than all training data, being versatile with different kernel functions that can handle various data types, having a strong theoretical foundation with well-understood generalization bounds, and working well with small to medium datasets where they can achieve good performance. However, they also have limitations including poor performance on large datasets where training time becomes prohibitive, being sensitive to feature scaling which requires careful preprocessing, providing no probabilistic output which limits their use in applications requiring uncertainty quantification, having kernel selection that can be tricky and often requires domain expertise, and being computationally expensive for very large datasets where other methods might be more practical.

\subsection{SVM for Regression}

SVM can also be extended to regression problems (Support Vector Regression, SVR). Instead of finding a hyperplane that separates classes, SVR finds a hyperplane that fits the data within an $\epsilon$-tube:

\begin{equation}
\min_{\vect{w}, b, \boldsymbol{\xi}, \boldsymbol{\xi}^*} \frac{1}{2}\|\vect{w}\|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*)
\end{equation}

subject to:
\begin{align}
y^{(i)} - \vect{w}^\top \vect{x}^{(i)} - b &\leq \epsilon + \xi_i \\
\vect{w}^\top \vect{x}^{(i)} + b - y^{(i)} &\leq \epsilon + \xi_i^* \\
\xi_i, \xi_i^* &\geq 0
\end{align}

