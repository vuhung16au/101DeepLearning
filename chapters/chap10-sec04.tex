% Chapter 10, Section 4

\section{Gated Recurrent Units (GRU) \difficultyInline{intermediate}}
\label{sec:gru}

\subsection*{Intuition}

GRU simplifies LSTM by merging cell and hidden state and combining gates, often matching performance with fewer parameters—useful when data or compute is limited \cite{Cho2014,GoodfellowEtAl2016}.

\subsection*{Historical Context}

Proposed in the mid-2010s, GRU offered a practical alternative to LSTM with competitive empirical results and simpler implementation \cite{Cho2014}.

% Index and glossary
\index{gated recurrent unit}
\glsadd{recurrent-neural-network}

\subsection{Architecture}

GRU simplifies LSTM with fewer parameters and merges the cell and hidden state into a single vector, often yielding comparable performance with less computation \cite{Cho2014,GoodfellowEtAl2016}:

\begin{align}
\vect{z}_t &= \sigma(\mat{W}_z [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(update gate)} \\
\vect{r}_t &= \sigma(\mat{W}_r [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(reset gate)} \\
\tilde{\vect{h}}_t &= \tanh(\mat{W} [\vect{r}_t \odot \vect{h}_{t-1}, \vect{x}_t]) \quad \text{(candidate)} \\
\vect{h}_t &= (1 - \vect{z}_t) \odot \vect{h}_{t-1} + \vect{z}_t \odot \tilde{\vect{h}}_t
\end{align}

\subsection{Architecture (visual)}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=stealth, node distance=2.0cm]
        \tikzstyle{gate}=[draw, rounded corners, minimum width=1.5cm, minimum height=0.9cm]
        \node[gate] (zt) {$\vect{z}_t$};
        \node[gate, below of=zt] (rt) {$\vect{r}_t$};
        \node[right of=zt, xshift=2.2cm] (ht1) {$\vect{h}_{t-1}$};
        \node[right of=ht1, xshift=1.2cm] (cand) {$\tilde{\vect{h}}_t$};
        \node[right of=cand, xshift=1.2cm] (ht) {$\vect{h}_t$};
        \draw[->] (ht1) -- node[midway, above] {$\odot\,\vect{r}_t$} (cand);
        \draw[->] (cand) -- (ht);
        \draw[->] (ht1) |- ++(0.8,1.8) node[pos=0.55, right] {$1-\vect{z}_t$} -| (ht);
        \draw[->] (cand) |- ++(0.8,-1.8) node[pos=0.55, right] {$\vect{z}_t$} -| (ht);
    \end{tikzpicture}
    \caption{Illustrative GRU flow with update and reset gates.}
\end{figure}

\subsection{Comparison with LSTM}
\begin{table}[h]
    \centering
    \begin{tabular}{@{}p{3.2cm}p{5.2cm}p{5.2cm}@{}}
    \toprule
    & \textbf{GRU} & \textbf{LSTM} \\
    \midrule
    State & Single hidden state $\vect{h}_t$ & Hidden $\vect{h}_t$ and cell $\vect{c}_t$ \\
    Gates & Update, reset & Input, forget, output \\
    Parameters & Fewer (often faster) & More (more expressive) \\
    Long-range deps. & Good in practice & Often stronger on very long spans \\
    Simplicity & Simpler to implement & Slightly more complex \\
    Typical use & Smaller data/compute budgets & Longer sequences or when capacity helps \\
    \bottomrule
    \end{tabular}
    \caption{GRU vs. LSTM at a glance \cite{Cho2014,Hochreiter1997,GoodfellowEtAl2016}.}
\end{table}

The choice between GRU and LSTM often depends on the specific task requirements and computational constraints. GRU's simpler architecture with fewer parameters makes it computationally more efficient, typically requiring less memory and allowing for faster training, which can be particularly advantageous when working with limited computational resources or when rapid prototyping is needed. Empirically, GRU performance is often comparable to LSTM on many tasks, especially those with moderate sequence lengths, though LSTM tends to have a slight edge on problems requiring very long-term dependencies due to its separate cell state that can maintain information more independently. In practice, both architectures have proven highly effective, and the choice often comes down to experimentation—trying both on your specific dataset and choosing the one that yields better validation performance whilst considering training time and deployment constraints. Modern deep learning frameworks make it straightforward to swap between these architectures, so practitioners can easily evaluate both options during model development.

