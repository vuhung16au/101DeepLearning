% Chapter 10, Section 4

\section{Gated Recurrent Units (GRU) \difficultyInline{intermediate}}
\label{sec:gru}

\subsection*{Intuition}

GRU simplifies LSTM by merging cell and hidden state and combining gates, often matching performance with fewer parametersâ€”useful when data or compute is limited \cite{Cho2014,GoodfellowEtAl2016}.

\subsection*{Historical Context}

Proposed in the mid-2010s, GRU offered a practical alternative to LSTM with competitive empirical results and simpler implementation \cite{Cho2014}.

% Index and glossary
\index{gated recurrent unit}
\glsadd{recurrent-neural-network}

\subsection{Architecture}

GRU simplifies LSTM with fewer parameters and merges the cell and hidden state into a single vector, often yielding comparable performance with less computation \cite{Cho2014,GoodfellowEtAl2016}. The mathematical formulation involves two key gates: the update gate $\vect{z}_t$ controls how much of the previous hidden state to retain, while the reset gate $\vect{r}_t$ determines how much of the previous hidden state to forget when computing the candidate hidden state. The candidate hidden state $\tilde{\vect{h}}_t$ is computed using the reset gate to selectively incorporate information from the previous state, and the final hidden state $\vect{h}_t$ is a weighted combination of the previous state and the candidate, where the weights are determined by the update gate.

\begin{align}
\vect{z}_t &= \sigma(\mat{W}_z [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(update gate)} \\
\vect{r}_t &= \sigma(\mat{W}_r [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(reset gate)} \\
\tilde{\vect{h}}_t &= \tanh(\mat{W} [\vect{r}_t \odot \vect{h}_{t-1}, \vect{x}_t]) \quad \text{(candidate)} \\
\vect{h}_t &= (1 - \vect{z}_t) \odot \vect{h}_{t-1} + \vect{z}_t \odot \tilde{\vect{h}}_t
\end{align}

\subsection{Architecture (visual)}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=stealth, node distance=2.0cm]
        \tikzstyle{gate}=[draw, rounded corners, minimum width=1.5cm, minimum height=0.9cm]
        \node[gate] (zt) {$\vect{z}_t$};
        \node[gate, below of=zt] (rt) {$\vect{r}_t$};
        \node[right of=zt, xshift=2.2cm] (ht1) {$\vect{h}_{t-1}$};
        \node[right of=ht1, xshift=1.2cm] (cand) {$\tilde{\vect{h}}_t$};
        \node[right of=cand, xshift=1.2cm] (ht) {$\vect{h}_t$};
        \draw[->] (ht1) -- node[midway, above] {$\odot\,\vect{r}_t$} (cand);
        \draw[->] (cand) -- (ht);
        \draw[->] (ht1) |- ++(0.8,1.8) node[pos=0.55, right] {$1-\vect{z}_t$} -| (ht);
        \draw[->] (cand) |- ++(0.8,-1.8) node[pos=0.55, right] {$\vect{z}_t$} -| (ht);
    \end{tikzpicture}
    \caption{Illustrative GRU flow with update and reset gates.}
\end{figure}

\subsection{Comparison with LSTM}

GRU and LSTM represent two different approaches to solving the vanishing gradient problem in recurrent networks, each with distinct advantages depending on the application requirements. GRU uses a single hidden state $\vect{h}_t$ with two gates (update and reset), making it computationally more efficient and easier to implement, while LSTM maintains separate hidden $\vect{h}_t$ and cell $\vect{c}_t$ states with three gates (input, forget, and output), providing more expressive power for complex sequence modeling tasks. The choice between them often depends on the specific requirements: GRU is typically preferred for smaller datasets or when computational efficiency is crucial, while LSTM often performs better on very long sequences or when the additional model capacity is beneficial for capturing complex temporal dependencies.

