% Chapter 10, Section 4

\section{Gated Recurrent Units (GRU) \difficultyInline{intermediate}}
\label{sec:gru}

\subsection*{Intuition}

Gated Recurrent Unit (GRU): A simplification of the LSTM that uses only two gates (Reset and Update) and merges the hidden state and cell state, making it faster to train with fewer parameters while achieving comparable performance in many tasks. GRU simplifies LSTM by merging cell and hidden state and combining gates, often matching performance with fewer parametersâ€”useful when data or compute is limited \cite{Cho2014,GoodfellowEtAl2016}.

\begin{remark}[Historical Context of GRU]
Proposed in the mid-2010s, GRU offered a practical alternative to LSTM with competitive empirical results and simpler implementation \cite{Cho2014}. GRU became important in modern deep learning as it provided a more efficient alternative to LSTM for many sequence modeling tasks, particularly valuable for resource-constrained applications and when computational efficiency is crucial.
\end{remark}

% Index and glossary
\index{gated recurrent unit}
\glsadd{recurrent-neural-network}

\subsection{Architecture}

GRU simplifies LSTM with fewer parameters and merges the cell and hidden state into a single vector, often yielding comparable performance with less computation \cite{Cho2014,GoodfellowEtAl2016}. The mathematical formulation involves two key gates: the update gate $\vect{z}_t$ controls how much of the previous hidden state to retain, while the reset gate $\vect{r}_t$ determines how much of the previous hidden state to forget when computing the candidate hidden state. The candidate hidden state $\tilde{\vect{h}}_t$ is computed using the reset gate to selectively incorporate information from the previous state, and the final hidden state $\vect{h}_t$ is a weighted combination of the previous state and the candidate, where the weights are determined by the update gate.

\begin{algorithm}[h]
\caption{GRU Forward Pass}
\label{alg:gru}
\begin{algorithmic}[1]
\State \textbf{Input:} $\vect{x}_t$ (input at time $t$), $\vect{h}_{t-1}$ (previous hidden state)
\State \textbf{Output:} $\vect{h}_t$ (current hidden state)
\State
\State \Comment{Compute gates}
\State $\vect{z}_t \leftarrow \sigma(\mat{W}_z [\vect{h}_{t-1}, \vect{x}_t])$ \Comment{Update gate}
\State $\vect{r}_t \leftarrow \sigma(\mat{W}_r [\vect{h}_{t-1}, \vect{x}_t])$ \Comment{Reset gate}
\State
\State \Comment{Compute candidate hidden state}
\State $\tilde{\vect{h}}_t \leftarrow \tanh(\mat{W} [\vect{r}_t \odot \vect{h}_{t-1}, \vect{x}_t])$
\State
\State \Comment{Compute final hidden state}
\State $\vect{h}_t \leftarrow (1 - \vect{z}_t) \odot \vect{h}_{t-1} + \vect{z}_t \odot \tilde{\vect{h}}_t$
\State
\State \textbf{return} $\vect{h}_t$
\end{algorithmic}
\end{algorithm}

\begin{align}
\vect{z}_t &= \sigma(\mat{W}_z [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(update gate)} \\
\vect{r}_t &= \sigma(\mat{W}_r [\vect{h}_{t-1}, \vect{x}_t]) \quad \text{(reset gate)} \\
\tilde{\vect{h}}_t &= \tanh(\mat{W} [\vect{r}_t \odot \vect{h}_{t-1}, \vect{x}_t]) \quad \text{(candidate)} \\
\vect{h}_t &= (1 - \vect{z}_t) \odot \vect{h}_{t-1} + \vect{z}_t \odot \tilde{\vect{h}}_t
\end{align}

\subsection{Architecture (visual)}
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=stealth, node distance=2.0cm]
        \tikzstyle{gate}=[draw, rounded corners, minimum width=1.5cm, minimum height=0.9cm]
        \node[gate] (zt) {$\vect{z}_t$};
        \node[gate, below of=zt] (rt) {$\vect{r}_t$};
        \node[right of=zt, xshift=2.2cm] (ht1) {$\vect{h}_{t-1}$};
        \node[right of=ht1, xshift=1.2cm] (cand) {$\tilde{\vect{h}}_t$};
        \node[right of=cand, xshift=1.2cm] (ht) {$\vect{h}_t$};
        \draw[->] (ht1) -- node[midway, above] {$\odot\,\vect{r}_t$} (cand);
        \draw[->] (cand) -- (ht);
        \draw[->] (ht1) |- ++(0.8,1.8) node[pos=0.55, right] {$1-\vect{z}_t$} -| (ht);
        \draw[->] (cand) |- ++(0.8,-1.8) node[pos=0.55, right] {$\vect{z}_t$} -| (ht);
    \end{tikzpicture}
    \caption{Illustrative GRU flow with update and reset gates.}
\end{figure}

\subsection{Comparison with LSTM}

GRU and LSTM represent two different approaches to solving the vanishing gradient problem in recurrent networks, each with distinct advantages depending on the application requirements. GRU uses a single hidden state $\vect{h}_t$ with two gates (update and reset), making it computationally more efficient and easier to implement, while LSTM maintains separate hidden $\vect{h}_t$ and cell $\vect{c}_t$ states with three gates (input, forget, and output), providing more expressive power for complex sequence modeling tasks. The choice between them often depends on the specific requirements: GRU is typically preferred for smaller datasets or when computational efficiency is crucial, while LSTM often performs better on very long sequences or when the additional model capacity is beneficial for capturing complex temporal dependencies.

