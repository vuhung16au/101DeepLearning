% Chapter 6, Section 4

\section{Backpropagation \difficultyInline{intermediate}}
\label{sec:backpropagation}

Backpropagation is the fundamental algorithm for training neural networks that efficiently computes gradients by propagating errors backward through the network using the chain rule of calculus.

\subsection{Intuition: Learning from Mistakes}

Imagine you're learning to play basketball, where after each shot, you need to know how far off your shot was (the error), which part of your technique needs adjustment (which parameters to change), and how much you should adjust each part (how much to change each parameter). Backpropagation is like having a coach who watches your shot and tells you exactly what to adjust, such as "Your elbow was too high" (gradient for elbow angle), "You need to follow through more" (gradient for follow-through), and "Your timing was off" (gradient for release timing). The key insight is that we can efficiently compute how much each parameter contributed to the final error by working backwards through the network, where backpropagation efficiently computes gradients using the chain rule to determine the precise adjustments needed for each parameter.

\subsection{The Chain Rule}

The chain rule is fundamental for computing derivatives of composite functions. Let's start with simple examples to build intuition.

\subsubsection{Simple Chain Rule: $f(g(x))$}

For composition $f(g(x))$:
\begin{equation}
\frac{df}{dx} = \frac{df}{dg} \frac{dg}{dx}
\end{equation}

\begin{example}
Let $f(x) = (x^2 + 1)^3$. We can think of this as $f(g(x))$ where:
\begin{itemize}
    \item $g(x) = x^2 + 1$ (inner function)
    \item $f(g) = g^3$ (outer function)
\end{itemize}

Using the chain rule:
\begin{align}
\frac{df}{dx} &= \frac{df}{dg} \cdot \frac{dg}{dx} \\
&= 3g^2 \cdot (2x) \\
&= 3(x^2 + 1)^2 \cdot 2x \\
&= 6x(x^2 + 1)^2
\end{align}
\end{example}

\subsubsection{Multivariable Chain Rule: $f(g(x,y))$}

For functions of multiple variables, we use partial derivatives:
\begin{equation}
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \frac{\partial g}{\partial x}
\end{equation}

\begin{example}
Let $f(x,y) = (x^2 + y^2)^2$. We can think of this as $f(g(x,y))$ where:
\begin{itemize}
    \item $g(x,y) = x^2 + y^2$ (inner function)
    \item $f(g) = g^2$ (outer function)
\end{itemize}

Using the chain rule for partial derivatives:
\begin{align}
\frac{\partial f}{\partial x} &= \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial x} \\
&= 2g \cdot (2x) \\
&= 2(x^2 + y^2) \cdot 2x \\
&= 4x(x^2 + y^2)
\end{align}

Similarly:
\begin{align}
\frac{\partial f}{\partial y} &= \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial y} \\
&= 2g \cdot (2y) \\
&= 2(x^2 + y^2) \cdot 2y \\
&= 4y(x^2 + y^2)
\end{align}
\end{example}

\subsubsection{Vector Chain Rule}

For vectors, we use the Jacobian:
\begin{equation}
\frac{\partial \vect{y}}{\partial \vect{x}} = \frac{\partial \vect{y}}{\partial \vect{z}} \frac{\partial \vect{z}}{\partial \vect{x}}
\end{equation}

\subsection{Backward Pass}

Starting from the loss $L$, we compute gradients layer by layer:

\begin{align}
\delta^{(L)} &= \nabla_{\vect{h}^{(L)}} L \\
\delta^{(l)} &= (\mat{W}^{(l+1)})^\top \delta^{(l+1)} \odot \sigma'(\vect{z}^{(l)})
\end{align}

where $\odot$ denotes element-wise multiplication.

Parameter gradients:
\begin{align}
\frac{\partial L}{\partial \mat{W}^{(l)}} &= \delta^{(l)} (\vect{h}^{(l-1)})^\top \\
\frac{\partial L}{\partial \vect{b}^{(l)}} &= \delta^{(l)}
\end{align}

\subsection{Computational Graph}

Modern frameworks use automatic differentiation on computational graphs, where forward mode is efficient when outputs greatly exceed inputs, and reverse mode (backprop) is efficient when inputs greatly exceed outputs, allowing for optimal gradient computation based on the specific network architecture and task requirements.

% Index entries
\index{backpropagation!algorithm}
\index{chain rule!backpropagation}
\index{gradient!computation}
\index{computational graph}
\index{automatic differentiation}

