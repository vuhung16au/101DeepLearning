% Chapter 6, Section 4

\section{Backpropagation \difficultyInline{intermediate}}
\label{sec:backpropagation}

\subsection{Intuition: Learning from Mistakes}

Imagine you're learning to play basketball. After each shot, you need to know:
\begin{itemize}
    \item How far off was your shot? (the error)
    \item Which part of your technique needs adjustment? (which parameters to change)
    \item How much should you adjust each part? (how much to change each parameter)
\end{itemize}

Backpropagation is like having a coach who watches your shot and tells you exactly what to adjust:
\begin{itemize}
    \item "Your elbow was too high" (gradient for elbow angle)
    \item "You need to follow through more" (gradient for follow-through)
    \item "Your timing was off" (gradient for release timing)
\end{itemize}

The key insight is that we can efficiently compute how much each parameter contributed to the final error by working backwards through the network.

\textbf{Backpropagation} efficiently computes gradients using the chain rule.

\subsection{The Chain Rule}

For composition $f(g(x))$:
\begin{equation}
\frac{df}{dx} = \frac{df}{dg} \frac{dg}{dx}
\end{equation}

For vectors, we use the Jacobian:
\begin{equation}
\frac{\partial \vect{y}}{\partial \vect{x}} = \frac{\partial \vect{y}}{\partial \vect{z}} \frac{\partial \vect{z}}{\partial \vect{x}}
\end{equation}

\subsection{Backward Pass}

Starting from the loss $L$, we compute gradients layer by layer:

\begin{align}
\delta^{(L)} &= \nabla_{\vect{h}^{(L)}} L \\
\delta^{(l)} &= (\mat{W}^{(l+1)})^\top \delta^{(l+1)} \odot \sigma'(\vect{z}^{(l)})
\end{align}

where $\odot$ denotes element-wise multiplication.

Parameter gradients:
\begin{align}
\frac{\partial L}{\partial \mat{W}^{(l)}} &= \delta^{(l)} (\vect{h}^{(l-1)})^\top \\
\frac{\partial L}{\partial \vect{b}^{(l)}} &= \delta^{(l)}
\end{align}

\subsection{Computational Graph}

Modern frameworks use automatic differentiation on computational graphs:
\begin{itemize}
    \item \textbf{Forward mode:} efficient when outputs $\gg$ inputs
    \item \textbf{Reverse mode (backprop):} efficient when inputs $\gg$ outputs
\end{itemize}

% Index entries
\index{backpropagation!algorithm}
\index{chain rule!backpropagation}
\index{gradient!computation}
\index{computational graph}
\index{automatic differentiation}

