% Chapter 16, Section 1

\section{Graphical Models \difficultyInline{advanced}}
\label{sec:graphical-models}

Graphical models provide a powerful framework for representing complex probability distributions using graph structures, where nodes represent random variables and edges encode probabilistic dependencies, enabling efficient representation and inference in high-dimensional spaces.

\subsection{Motivation}

Graphical models represent complex probability distributions using graphs, where nodes represent random variables and edges encode probabilistic dependencies, enabling the efficient representation of high-dimensional joint distributions through factorization. This approach is particularly valuable in deep learning because it allows us to encode domain knowledge and structural assumptions directly into the model, where the graph structure provides inductive biases that guide the learning process and improve generalization. The graphical representation makes the model more interpretable by explicitly showing which variables interact and how, where this transparency is crucial for understanding model behavior and debugging complex systems. Furthermore, graphical models enable efficient inference by exploiting the conditional independence structure encoded in the graph, where this computational efficiency is essential for practical applications with large-scale data.

\subsection{Bayesian Networks}

Bayesian networks use directed acyclic graphs (DAGs) to represent conditional dependencies, where the joint probability distribution factorizes as $p(\vect{x}) = \prod_{i=1}^{n} p(x_i | \text{Pa}(x_i))$ with $\text{Pa}(x_i)$ being the parents of $x_i$. In deep learning, Bayesian networks are particularly useful for modeling causal relationships and incorporating domain knowledge, where the directed structure allows for natural representation of cause-effect relationships that can guide the learning process. The Naive Bayes classifier exemplifies this approach with $p(y, \vect{x}) = p(y) \prod_{i=1}^{d} p(x_i|y)$, where the assumption of feature independence given the class enables efficient learning and inference, making it a fundamental building block for more complex graphical models in deep learning applications.

\subsection{Markov Random Fields}

Markov Random Fields use undirected graphs with potential functions, where the joint probability distribution is given by $p(\vect{x}) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\vect{x}_c)$ with $\mathcal{C}$ being cliques and $Z$ the partition function. In deep learning, Markov Random Fields are particularly valuable for modeling symmetric relationships and spatial dependencies, where the undirected structure allows for natural representation of mutual influences between variables without imposing causal directionality. Examples like the Ising model and Conditional Random Fields (CRFs) demonstrate how MRFs can capture complex spatial and temporal dependencies in data, where CRFs are widely used in sequence labeling tasks like named entity recognition and part-of-speech tagging, enabling the model to consider the context of neighboring labels when making predictions.


% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (graphical models)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}[>=stealth]
%     % Simple DAG: X1 -> X3 <- X2
%     \node[circle,draw] (x1) at (0,0) {$x_1$};
%     \node[circle,draw] (x2) at (2,0) {$x_2$};
%     \node[circle,draw] (x3) at (1,-1.2) {$x_3$};
%     \draw[->] (x1) -- (x3);
%     \draw[->] (x2) -- (x3);
%   \end{tikzpicture}
%   \caption{A simple Bayesian network (DAG) encoding conditional dependencies.}
%   \label{fig:bn-dag}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     % 3x3 MRF grid (undirected)
%     \foreach \i in {0,1,2} {
%       \foreach \j in {0,1,2} {
%         \node[circle,draw,inner sep=1.5pt] (n\i\j) at (\i*0.9,-\j*0.9) {};
%       }
%     }
%     \foreach \i in {0,1,2} {
%       \foreach \j in {0,1} {
%         \draw (n\i\j) -- (n\i\the\numexpr\j+1\relax);
%       }
%     }
%     \foreach \i in {0,1} {
%       \foreach \j in {0,1,2} {
%         \draw (n\i\j) -- (n\the\numexpr\i+1\relax\j);
%       }
%     }
%   \end{tikzpicture}
%   \caption{Undirected MRF grid with pairwise potentials between neighbors.}
%   \label{fig:mrf-grid}
% \end{figure}

% \subsection{Notes and references}

% Foundations and factorization properties are covered extensively in \textcite{Bishop2006,GoodfellowEtAl2016,Prince2023}.
