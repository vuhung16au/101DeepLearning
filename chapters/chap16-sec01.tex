% Chapter 16, Section 1

\section{Graphical Models \difficultyInline{advanced}}
\label{sec:graphical-models}

Graphical models provide a powerful framework for representing complex probability distributions using graph structures, where nodes represent random variables and edges encode probabilistic dependencies, enabling efficient representation and inference in high-dimensional spaces.

\subsection{Motivation}

Graphical models represent complex probability distributions using graphs, where nodes represent random variables and edges encode probabilistic dependencies, enabling the efficient representation of high-dimensional joint distributions through factorization. This approach is particularly valuable in deep learning because it allows us to encode domain knowledge and structural assumptions directly into the model, where the graph structure provides inductive biases that guide the learning process and improve generalization. The graphical representation makes the model more interpretable by explicitly showing which variables interact and how, where this transparency is crucial for understanding model behavior and debugging complex systems. Furthermore, graphical models enable efficient inference by exploiting the conditional independence structure encoded in the graph, where this computational efficiency is essential for practical applications with large-scale data.

\subsection{Bayesian Networks}

Bayesian networks use directed acyclic graphs (DAGs) to represent conditional dependencies, where the joint probability distribution factorizes as $p(\vect{x}) = \prod_{i=1}^{n} p(x_i | \text{Pa}(x_i))$ with $\text{Pa}(x_i)$ being the parents of $x_i$. In deep learning, Bayesian networks are particularly useful for modeling causal relationships and incorporating domain knowledge. The Naive Bayes classifier exemplifies this approach with
\begin{equation}
p(y, \vect{x}) = p(y) \prod_{i=1}^{d} p(x_i\,|\,y).
\end{equation}
\noindent\textbf{Variables.} $y$: class label; $\vect{x}=(x_1,\dots,x_d)$: features; $p(y)$: class prior; $p(x_i|y)$: class-conditional likelihoods.

\subsection{Markov Random Fields}

Markov Random Fields use undirected graphs with potential functions. The joint distribution factorizes as
\begin{equation}
p(\vect{x}) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\vect{x}_c),
\end{equation}
where $\mathcal{C}$ are cliques, $\psi_c$ are nonnegative potentials over variables in clique $c$, and $Z$ is the partition function ensuring normalization.


% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (graphical models)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}[>=stealth]
%     % Simple DAG: X1 -> X3 <- X2
%     \node[circle,draw] (x1) at (0,0) {$x_1$};
%     \node[circle,draw] (x2) at (2,0) {$x_2$};
%     \node[circle,draw] (x3) at (1,-1.2) {$x_3$};
%     \draw[->] (x1) -- (x3);
%     \draw[->] (x2) -- (x3);
%   \end{tikzpicture}
%   \caption{A simple Bayesian network (DAG) encoding conditional dependencies.}
%   \label{fig:bn-dag}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     % 3x3 MRF grid (undirected)
%     \foreach \i in {0,1,2} {
%       \foreach \j in {0,1,2} {
%         \node[circle,draw,inner sep=1.5pt] (n\i\j) at (\i*0.9,-\j*0.9) {};
%       }
%     }
%     \foreach \i in {0,1,2} {
%       \foreach \j in {0,1} {
%         \draw (n\i\j) -- (n\i\the\numexpr\j+1\relax);
%       }
%     }
%     \foreach \i in {0,1} {
%       \foreach \j in {0,1,2} {
%         \draw (n\i\j) -- (n\the\numexpr\i+1\relax\j);
%       }
%     }
%   \end{tikzpicture}
%   \caption{Undirected MRF grid with pairwise potentials between neighbors.}
%   \label{fig:mrf-grid}
% \end{figure}

% \subsection{Notes and references}

% Foundations and factorization properties are covered extensively in \textcite{Bishop2006,GoodfellowEtAl2016,Prince2023}.
