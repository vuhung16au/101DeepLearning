% Problems (Exercises) for Chapter 18

\section*{Problems}
\addcontentsline{toc}{section}{Problems}

\subsection*{Easy}

\begin{problem}[MDP Definition]
Define the components of a Markov Decision Process.

\textbf{Hint:} States, actions, rewards, transition dynamics, discount factor.
\end{problem}

\begin{problem}[Value Function]
Explain the difference between $V(s)$ and $Q(s,a)$.

\textbf{Hint:} State value vs. action-value.
\end{problem}

\begin{problem}[Policy Types]
Contrast deterministic and stochastic policies.

\textbf{Hint:} Mapping vs. distribution over actions.
\end{problem}

\begin{problem}[Exploration vs. Exploitation]
Give two exploration strategies in RL.

\textbf{Hint:} $\epsilon$-greedy; UCB; entropy regularisation.
\end{problem}

\subsection*{Medium}

\begin{problem}[Bellman Equation]
Derive the Bellman equation for $Q(s,a)$.

\textbf{Hint:} Recursive relationship with successor states.
\end{problem}

\begin{problem}[Policy Gradient]
Explain why policy gradient methods are useful for continuous action spaces.

\textbf{Hint:} Direct parameterisation; differentiability.
\end{problem}

\subsection*{Hard}

\begin{problem}[Actor-Critic Derivation]
Derive the advantage actor-critic update rule.

\textbf{Hint:} Baseline subtraction; variance reduction.
\end{problem}

\begin{problem}[Off-Policy Correction]
Analyse importance sampling for off-policy learning and its variance.

\textbf{Hint:} Likelihood ratio; distribution mismatch.
\end{problem}

