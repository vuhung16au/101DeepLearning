% Chapter 4, Section 3: Constrained Optimization

\section{Constrained Optimization \difficultyInline{beginner}}
\label{sec:constrained-optimization}

Constrained optimization extends traditional optimization by incorporating additional requirements or limitations that must be satisfied while finding optimal solutions, providing essential tools for implementing regularization, fairness constraints, and other practical considerations in deep learning applications.

\subsection{Intuition: Optimization with Rules}

Imagine you're trying to find the best location for a new store, but you have constraints such as being within 10 km of the city centre, having parking for at least 50 cars, and a budget that cannot exceed \$1 million. You can't just pick any location - you must follow these rules while still optimizing your objective like maximizing customer traffic. In deep learning, we often have similar constraints including weight constraints to keep weights small and prevent overfitting, probability constraints where outputs must sum to 1 like in softmax functions, and fairness constraints where the model must treat different groups equally. Many problems require optimizing a function subject to constraints, making constrained optimization an essential tool for practical machine learning applications.

\subsection{Lagrange Multipliers}

For equality constraint $g(\vect{x}) = 0$, the \textbf{Lagrangian} is:

\begin{equation}
\mathcal{L}(\vect{x}, \lambda) = f(\vect{x}) + \lambda g(\vect{x})
\end{equation}

At the optimum, both:
\begin{equation}
\nabla_{\vect{x}} \mathcal{L} = \boldsymbol{0} \quad \text{and} \quad \frac{\partial \mathcal{L}}{\partial \lambda} = 0
\end{equation}

\begin{remark}
Lagrange multipliers are crucial in deep learning for implementing regularization techniques like weight decay and dropout, where we optimize the loss function subject to constraints on parameter norms or activation patterns. They also enable the development of constrained optimization algorithms for training neural networks with fairness constraints, ensuring models treat different demographic groups equally while maintaining high performance.
\end{remark}

\subsection{Inequality Constraints}

For inequality constraint $g(\vect{x}) \leq 0$, we use the \textbf{Karush-Kuhn-Tucker (KKT)} conditions:

\begin{align}
\nabla_{\vect{x}} \mathcal{L} &= \boldsymbol{0} \\
\lambda &\geq 0 \\
\lambda g(\vect{x}) &= 0 \quad \text{(complementary slackness)} \\
g(\vect{x}) &\leq 0
\end{align}

\begin{remark}
KKT conditions are essential in deep learning for handling inequality constraints such as bounding neural network weights to prevent exploding gradients, ensuring activation functions stay within valid ranges, and implementing robust optimization with adversarial training where perturbations must remain within specified bounds. They also enable the development of constrained optimization algorithms for training neural networks with fairness constraints, ensuring models treat different demographic groups equally while maintaining high performance.
\end{remark}

\subsection{Projected Gradient Descent}

For constraints defining a set $\mathcal{C}$, \textbf{projected gradient descent} applies:

\begin{equation}
\vect{x}_{t+1} = \text{Proj}_{\mathcal{C}}\left(\vect{x}_t - \alpha \nabla f(\vect{x}_t)\right)
\end{equation}

where $\text{Proj}_{\mathcal{C}}$ projects onto the feasible set.

The mathematical operation works as follows: first, we compute the standard gradient descent step $\vect{x}_t - \alpha \nabla f(\vect{x}_t)$, which may move us outside the feasible set $\mathcal{C}$. Then, the projection operator $\text{Proj}_{\mathcal{C}}$ finds the closest point in $\mathcal{C}$ to this intermediate result, ensuring that $\vect{x}_{t+1} \in \mathcal{C}$. The projection is defined as:
\begin{equation}
\text{Proj}_{\mathcal{C}}(\vect{y}) = \arg\min_{\vect{x} \in \mathcal{C}} \|\vect{x} - \vect{y}\|_2
\end{equation}

This ensures that each update step remains within the constraint set while still moving in the direction that reduces the objective function.

\begin{remark}
Projected gradient descent is essential in deep learning for maintaining constraints during training, such as keeping neural network weights within specified bounds to prevent exploding gradients, ensuring probability outputs sum to 1 in softmax layers, and implementing robust optimization with adversarial training where perturbations must stay within $\ell_\infty$ balls around input examples.
\end{remark}

\subsection{Applications in Deep Learning}

Constrained optimization appears in several important applications in deep learning, including weight constraints such as unit norm constraints that help prevent overfitting and improve generalization, projection to valid probability distributions that ensures model outputs are mathematically valid, adversarial training with bounded perturbations that creates robust models by training against carefully crafted adversarial examples, and fairness constraints that ensure models treat different groups equally and avoid discriminatory behavior.
