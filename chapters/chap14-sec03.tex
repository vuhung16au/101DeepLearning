% Chapter 14, Section 3

\section{Variational Autoencoders \difficultyInline{intermediate}}
\label{sec:vae}

Variational Autoencoders (VAEs) are probabilistic generative models that learn to encode data into a latent space and generate new samples by combining the encoder-decoder architecture with variational inference techniques.

\subsection{Probabilistic Framework}

VAE is a generative model with the marginal likelihood $p(\vect{x}) = \int p(\vect{x}|\vect{z}) p(\vect{z}) d\vect{z}$, where the prior $p(\vect{z}) = \mathcal{N}(\boldsymbol{0}, \mat{I})$ is a standard normal distribution and the likelihood $p(\vect{x}|\vect{z}) = \mathcal{N}(\vect{x}; \boldsymbol{\mu}_{\theta}(\vect{z}), \boldsymbol{\sigma}^2_{\theta}(\vect{z})\mat{I})$ is parameterized by neural networks that output the mean and variance of the reconstruction distribution.

\subsection{Evidence Lower Bound (ELBO)}

Since we cannot directly maximize $\log p(\vect{x})$, we instead maximize the ELBO $\mathcal{L} = \mathbb{E}_{q(\vect{z}|\vect{x})}[\log p(\vect{x}|\vect{z})] - D_{KL}(q(\vect{z}|\vect{x}) \| p(\vect{z}))$, where the first term encourages accurate reconstruction and the second term regularizes the encoder distribution $q(\vect{z}|\vect{x}) = \mathcal{N}(\vect{z}; \boldsymbol{\mu}_{\phi}(\vect{x}), \boldsymbol{\sigma}^2_{\phi}(\vect{x})\mat{I})$ to match the prior.

\subsection{Reparameterization Trick}

The reparameterization trick enables backpropagation through stochastic nodes by writing sampling as a deterministic function of parameters and an auxiliary noise variable. For a Gaussian encoder
\begin{equation}
q_{\phi}(\vect{z}|\vect{x}) = \mathcal{N}\big(\vect{z};\, \boldsymbol{\mu}_{\phi}(\vect{x}),\; \mathrm{diag}(\boldsymbol{\sigma}^2_{\phi}(\vect{x}))\big),
\end{equation}
sample via
\begin{equation}
\vect{z} = \boldsymbol{\mu}_{\phi}(\vect{x}) + \boldsymbol{\sigma}_{\phi}(\vect{x}) \odot \boldsymbol{\epsilon},\qquad \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \mat{I}).
\end{equation}
This yields a low-variance gradient estimator of the ELBO because the expectation over $q_{\phi}(\vect{z}|\vect{x})$ becomes an expectation over $\boldsymbol{\epsilon}$ independent of $\phi$:
\begin{align}
\nabla_{\phi} \, \mathbb{E}_{q_{\phi}(\vect{z}|\vect{x})}[\,f(\vect{z})\,] &= \nabla_{\phi} \, \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(0,\mat{I})}\big[ f\big(\boldsymbol{\mu}_{\phi}(\vect{x}) + \boldsymbol{\sigma}_{\phi}(\vect{x}) \odot \boldsymbol{\epsilon}\big) \big] \\
&= \mathbb{E}_{\boldsymbol{\epsilon}}\big[ \nabla_{\phi} f\big(\boldsymbol{\mu}_{\phi}(\vect{x}) + \boldsymbol{\sigma}_{\phi}(\vect{x}) \odot \boldsymbol{\epsilon}\big) \big].
\end{align}
Thus gradients flow through $\boldsymbol{\mu}_{\phi}$ and $\boldsymbol{\sigma}_{\phi}$ via the deterministic mapping while preserving stochasticity through $\boldsymbol{\epsilon}$.

\subsection{Generation}

Generation in VAEs involves sampling from the prior $\vect{z} \sim \mathcal{N}(\boldsymbol{0}, \mat{I})$ and decoding to generate new data, where the decoder network learns to map latent samples to realistic data samples by training on the reconstruction task. This process enables the generation of new samples that follow the learned data distribution, where the quality of generated samples depends on how well the model has learned to capture the underlying data structure and the effectiveness of the latent space representation. The generation process is particularly powerful because it allows for controlled sampling and interpolation in the latent space, enabling the creation of new data points that maintain the statistical properties of the training data while potentially exploring new combinations of learned features.

% \subsection{Visual aids}
% \addcontentsline{toc}{subsubsection}{Visual aids (VAE)}

% \begin{figure}[h]
%   \centering
%   \begin{tikzpicture}
%     \begin{axis}[
%       width=0.48\textwidth,height=0.36\textwidth,
%       xlabel={$z_1$}, ylabel={$z_2$}, grid=both]
%       \addplot+[only marks,mark=*,mark size=0.9pt,bookpurple!70] coordinates{(-1,-1) (-1,0) (-1,1) (0,-1) (0,0) (0,1) (1,-1) (1,0) (1,1)};
%     \end{axis}
%   \end{tikzpicture}
%   \caption{Samples from a 2D latent Gaussian prior (illustrative).}
%   \label{fig:vae-latent}
% \end{figure}

\begin{algorithm}[htbp]
\caption{Variational Autoencoder Training Algorithm}
\label{alg:vae-training}
\begin{algorithmic}[1]
\State \textbf{Input:} Dataset $\mathcal{D} = \{\vect{x}^{(i)}\}_{i=1}^{N}$, learning rate $\alpha$
\State Initialize encoder parameters $\phi$ and decoder parameters $\theta$
\For{epoch = 1 to max\_epochs}
    \For{batch $\mathcal{B} \subset \mathcal{D}$}
        \State Compute encoder outputs: $\boldsymbol{\mu}_{\phi}(\vect{x})$, $\boldsymbol{\sigma}_{\phi}(\vect{x})$
        \State Sample $\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \mat{I})$
        \State Reparameterize: $\vect{z} = \boldsymbol{\mu}_{\phi}(\vect{x}) + \boldsymbol{\sigma}_{\phi}(\vect{x}) \odot \boldsymbol{\epsilon}$
        \State Compute decoder outputs: $\boldsymbol{\mu}_{\theta}(\vect{z})$, $\boldsymbol{\sigma}_{\theta}(\vect{z})$
        \State Compute reconstruction loss: $\mathcal{L}_{rec} = \mathbb{E}_{q(\vect{z}|\vect{x})}[\log p(\vect{x}|\vect{z})]$
        \State Compute KL divergence: $\mathcal{L}_{KL} = D_{KL}(q(\vect{z}|\vect{x}) \| p(\vect{z}))$
        \State Total loss: $\mathcal{L} = \mathcal{L}_{rec} - \mathcal{L}_{KL}$
        \State Update parameters: $\phi \leftarrow \phi - \alpha \nabla_{\phi} \mathcal{L}$, $\theta \leftarrow \theta - \alpha \nabla_{\theta} \mathcal{L}$
    \EndFor
\EndFor
\State \textbf{Return} trained encoder $q(\vect{z}|\vect{x})$ and decoder $p(\vect{x}|\vect{z})$
\end{algorithmic}
\end{algorithm}

\subsection{Notes and references}

VAEs provide a principled probabilistic framework for representation learning and generation, representing a significant milestone in generative modeling that combines the power of neural networks with variational inference techniques. The work by Kingma and Welling in 2013 introduced the reparameterization trick that made VAEs trainable, while subsequent research has extended VAEs to various domains including image generation, text modeling, and molecular design. These models have achieved remarkable success in applications ranging from image compression and denoising to drug discovery and creative applications, demonstrating their versatility and practical impact in modern machine learning systems.
