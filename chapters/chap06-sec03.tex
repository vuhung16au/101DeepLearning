% Chapter 6, Section 3

\section{Output Units and Loss Functions \difficultyInline{intermediate}}
\label{sec:output-loss}

Output units and loss functions are the final components of neural networks that determine how predictions are formatted and how the network learns from errors, with different choices required for different types of machine learning tasks.

\subsection{Intuition: Matching Output to Task}

Think of the output layer as the "final decision maker" in your network, where just like different jobs require different tools, different machine learning tasks require different output formats. For regression tasks like predicting prices, you want a real number such as "This house costs \$250,000", for binary classification tasks like spam detection, you want a probability such as "This email is 95\% likely to be spam", and for multiclass classification tasks like image recognition, you want probabilities for each class such as "This image is 80\% cat, 15\% dog, 5\% bird". The loss function is like a "teacher" that tells the network how wrong it is, where a good teacher gives clear, helpful feedback that guides learning in the right direction, and the choice of output layer and loss function depends on the specific task requirements.

\subsection{Linear Output for Regression}

For regression, use linear output:
\begin{equation}
\hat{y} = \mat{W}^\top \vect{h} + b
\end{equation}

with MSE loss:
\begin{equation}
L = \frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2
\end{equation}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=0.7\textwidth,
    height=0.4\textwidth,
    axis lines=left,
    xlabel={$\hat y$}, ylabel={$L(\hat y)$},
    xmin=-2, xmax=2, ymin=-0.1, ymax=4,
    grid=both, grid style={draw=bookpurple!10},
    major grid style={draw=bookpurple!40}
  ]
    % Visualize MSE as (y - yhat)^2 with y=1
    \addplot[domain=-2:2, samples=300, very thick, bookpurple] {(1 - x)^2};
    \addlegendentry{$(y-\hat y)^2$ for $y=1$}
    \addplot[only marks, mark=*, bookred] coordinates {(1,0)};
    \addlegendentry{Minimum at $\hat y{=}y$}
  \end{axis}
\end{tikzpicture}
\caption{MSE loss in 1D: a convex parabola centered at the target $y$. The minimum occurs at $\hat y{=}y$.}
\label{fig:mse-1d}
\end{figure}

\subsection{Sigmoid Output for Binary Classification}

For binary classification:
\begin{equation}
\hat{y} = \sigma(\mat{W}^\top \vect{h} + b)
\end{equation}

with binary cross-entropy loss:
\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})]
\end{equation}

\subsection{Softmax Output for Multiclass Classification}

For $K$ classes:
\begin{equation}
\hat{y}_k = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)}
\end{equation}

with categorical cross-entropy loss:
\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log \hat{y}_k^{(i)}
\end{equation}

% Index entries
\index{output layer!regression}
\index{output layer!classification}
\index{loss function!MSE}
\index{loss function!cross-entropy}
\index{softmax!multiclass classification}
\index{sigmoid!binary classification}

