% Chapter 6, Section 3

\section{Output Units and Loss Functions \difficultyInline{intermediate}}
\label{sec:output-loss}

\subsection{Intuition: Matching Output to Task}

Think of the output layer as the "final decision maker" in your network. Just like different jobs require different tools, different machine learning tasks require different output formats:

\begin{itemize}
    \item \textbf{Regression (predicting prices):} You want a real number. "This house costs \$250,000"
    \item \textbf{Binary Classification (spam detection):} You want a probability. "This email is 95\% likely to be spam"
    \item \textbf{Multiclass Classification (image recognition):} You want probabilities for each class. "This image is 80\% cat, 15\% dog, 5\% bird"
\end{itemize}

The loss function is like a "teacher" that tells the network how wrong it is. A good teacher gives clear, helpful feedback that guides learning in the right direction.

The choice of output layer and loss function depends on the task.

\subsection{Linear Output for Regression}

For regression, use linear output:
\begin{equation}
\hat{y} = \mat{W}^\top \vect{h} + b
\end{equation}

with MSE loss:
\begin{equation}
L = \frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2
\end{equation}

\subsection{Sigmoid Output for Binary Classification}

For binary classification:
\begin{equation}
\hat{y} = \sigma(\mat{W}^\top \vect{h} + b)
\end{equation}

with binary cross-entropy loss:
\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log(1-\hat{y}^{(i)})]
\end{equation}

\subsection{Softmax Output for Multiclass Classification}

For $K$ classes:
\begin{equation}
\hat{y}_k = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)}
\end{equation}

with categorical cross-entropy loss:
\begin{equation}
L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} y_k^{(i)} \log \hat{y}_k^{(i)}
\end{equation}

% Index entries
\index{output layer!regression}
\index{output layer!classification}
\index{loss function!MSE}
\index{loss function!cross-entropy}
\index{softmax!multiclass classification}
\index{sigmoid!binary classification}

